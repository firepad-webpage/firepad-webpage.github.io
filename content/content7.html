<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>The capacity to generate and manipulate digital 3D objects lies at the core of a multitude of applications related to the entertainment and media industries, virtual and augmented reality and healthcare.</p>

<p>However, achieving fine control in mesh manipulation necessitates the learning of a disentangled representation of 3D shapes which is still an open research problem.</p>

<p>Recently proposed methods based on Graph Convolutional Network (GCN)-based Auto-Encoders (AEs) have demonstrated impressive performance in dimensionality reduction but typically learn a highly entangled latent space making them unsuitable for detailed shape manipulation.</p>

<p>Additionally, despite having a low parameter count, these methods struggle to handle high-resolution meshes, limiting their applicability.</p>

<p>Few works have dealt with the disentanglement of local identity attributes, however these methods still rely on GCNs and opt for controlling manipulations through the state of the latent code which is partitioned and assigned to predefined object regions.</p>

<p>Using the latent code to drive shape manipulation requires the use of explicit optimization objectives to learn a disentangled latent space.</p>

<p>Moreover, partitioning its state is critically suboptimal for learning compressed representations of 3D objects.</p>

<p>We propose a different paradigm which does not involve partitioning the latent code or relying on its state to drive changes in shape, resulting in state-of-the-art (SOTA) disentanglement and reconstruction capabilities in a unified architecture.</p>

<p>Instead, we use a global latent code for 3D object unconditional generation and utilise additional inputs to jointly train our generative model to locally overwrite the latent encoded geometry.</p>

<p>We present the Locally Adaptive Morphable Model (LAMM), a general framework for manipulating the geometry of registered meshes.</p>

<p>To the best of our knowledge, this is the first method that allows direct shape control with a single forward pass.</p>

<p>Applied on human 3D heads, LAMM exhibits SOTA disentanglement properties and allows for very fine geometric control over both facial identities and expressions.</p>

<p>Our models, trained for manipulation, concurrently exhibit SOTA performance in mesh dimensionality reduction compared against methods trained exclusively on this task.</p>

<p>As a result, a single model can be used to generate entirely new shapes and apply both localized and global modifications to their geometry.</p>

<p>We show how our framework can leverage direct control as a primitive to achieve higher level editing operations such as region swapping and sampling.</p>

<p>By deviating from GCN-based AE design, LAMM can scale to much larger meshes, needs only a fraction of GPU memory to train and can be significantly faster during inference compared to competing methods.</p>

<p>For example, trained on 72k vertex meshes with batch size 32, our model requires 7.5Gb of GPU memory and runs at 0.045s on a single CPU thread.</p>

<p>This model outperforms SpiralNet++ which equivalently requires more than 40Gb of memory and runs 13 times slower at 0.58s.</p>

<p>Blanz and Vetter pioneered an innovative approach for synthesis and reconstruction of 3D human faces through their seminal 3D Morphable Model (3DMM).</p>

<p>First, they establish a fixed mesh topology, ensuring that each face vertex is semantically significant and could be uniquely identified.</p>

<p>Subsequently, Principal Component Analysis (PCA) is employed to map 3D shapes and textures into a lower dimensional latent code.</p>

<p>However, the linear formulation of PCA limits its expressivity and capacity to capture high frequency details.</p>

<p>The deep GCN-based family of models discussed above have been consistently shown to outperform PCA at high levels of compression.</p>

<p>However, when increasing the size of the latent space to values that are practical for applications, e.g. monocular 3D face reconstruction, PCA is shown to match and quickly surpass GCN architectures.</p>

<p>We argue that the translation equivariant property of GCNs is not optimal for modelling registered meshes as it does not account for the distinct semantics of each vertex.</p>

<p>In contrast, by employing fully connected layers to encode input shapes into tokens and a global receptive field for downstream processing, our framework respects this attribute of the data, having the capacity to treat similar features differently depending on their location.</p>

<p>All statistical models of 3D meshes discussed above can be used to manipulate mesh geometry, e.g. the optimal fit for a given shape can be discerned through latent code optimization.</p>

<p>The main issue with this approach is that latent spaces learnt for generative modelling are typically highly entangled.</p>

<p>This attribute discards the possibility for finely-tuned control of local geometry as varying any one dimension of the latent code can concurrently impact multiple properties of the reconstructed data.</p>

<p>Notably, works by Foti et al. adeptly deform 3D face geometry to manipulate an individualâ€™s identity.</p>

<p>Both works harness the backbone architecture from SpiralNet++, along with various face regions, each governed by non-overlapping parts of the latent code.</p>

<p>Importantly, we do not partition our latent codes into region-specific segments, which is beneficial for retaining strong performance in mesh reconstruction.</p>

<p>Furthermore, we do not enforce spatial disentanglement through explicitly defined loss components but rather reach SOTA disentanglement performance solely as an emerging property of our architecture.</p>

<p>We leverage the AE framework for mesh representation learning, with targeted modifications in the decoder architecture to facilitate direct manipulation of mesh geometry.</p>

<p>At each step we sample B source and target elements Vs,Vt from the training set and construct our source and target batches respectively as {Vs,Vs} and {Vs,Vt}.</p>

<p>Training with direct supervision from real target data has the benefit of providing realistic examples for the target geometry space.</p>

<p>In practice, interactive vertex control is an iterative process.</p>

<p>We believe this attribute to be an intrinsic property of our architecture.</p>

<p>Our method is shown to significantly outperform others applied in head and hand data with improvements being more pronounced on UHM12k.</p>

<p>We show it is possible to generate new dense region shapes by sampling valid displacements for the control vertices.</p>

<p>We evaluate the capacity of our model to have only localized effects in manipulated geometry compared to SOTA method SD-VAE.</p>

<p>Consistently, we find that our method leads to more localized effects for all face regions and improved disentanglement over generated geometries.</p>

<p>Importantly, we note that in contrast to SD-VAE, our framework is trained without any loss component aimed towards disentanglement which indicates this to be an intrinsic property of our method.</p>

<p>We have introduced LAMM, a comprehensive framework for learning 3D mesh representations and achieving spatially disentangled shape manipulation.</p>

<p>To our knowledge, LAMM is the first end-to-end trainable method that is capable of directly manipulating mesh shape, by using desired displacements as inputs, in a single forward pass.</p>

<p>Our extensive experiments with 3D human head and hand mesh data demonstrate that LAMM simultaneously attains state-of-the-art performance in both disentanglement and reconstruction within a unified architecture.</p>

<p>Notably, it scales more effectively to high-resolution data, requires less memory for training, and offers faster inference speeds compared to existing methods.</p>

<p>Fast CPU inference makes LAMM an energy efficient and economically viable option, democratizing access to advanced 3D modeling.</p>

<h2>Original Abstract</h2>
<p>The generation and manipulation of 3D meshes are pivotal for applications across entertainment, virtual and augmented reality, and healthcare. However, existing methods struggle to achieve fine-grained, spatially disentangled control of 3D shapes, often relying on Graph Convolutional Network (GCN)-based auto-encoders (AEs) that produce highly entangled latent spaces and face scalability issues with high-resolution meshes. We propose the Locally Adaptive Morphable Model (LAMM), a novel framework that circumvents these limitations by enabling direct, localized mesh manipulation without partitioning the latent code or introducing explicit disentanglement objectives. LAMM employs fully connected layers and a global receptive field to respect the distinct semantics of registered mesh vertices, allowing for both unconditional shape generation and fine-grained editing via direct control inputs. Extensive experiments on 3D human head and hand meshes demonstrate that LAMM achieves state-of-the-art performance in both disentanglement and reconstruction while scaling efficiently to high-resolution data. Our model surpasses existing methods in terms of memory efficiency and inference speed, offering practical benefits such as faster CPU-based execution and reduced resource requirements. LAMM's capabilities democratize access to advanced 3D modeling techniques, enabling applications ranging from detailed facial identity editing to region swapping and geometric sampling.</p>
</body>
</html>
