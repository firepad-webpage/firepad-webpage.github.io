<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Excerpt</title>
</head>
<body>
<h2>Excerpt</h2>
<p>The emergence of diffusion models has significantly boosted generation quality across a wide range of media content. This generation paradigm has shown promise for video generation, despite the challenges of working with high-dimensional data.</p>
<p>While diffusion models are one factor driving progress, the scaling of training datasets has also played a key role. However, despite recent progress, the visual quality of generated videos still leaves room for improvement.</p>
<p>A significant contributing factor to this issue is the varying quality of web-scale data employed during pre-training, which can yield models capable of generating content that is visually unappealing, toxic and misaligned with the prompt.</p>
<p>While aligning model outputs with human preferences has proven highly effective for control, text generation and image generation, it remains a notion unexplored in video diffusion models.</p>
<p>Two major challenges arise when seeking to align video generation models with human preferences: The optimization process for optimizing human preferences is computationally demanding, often requiring video generation from textual inputs. The curation of a large annotated dataset to capture human preferences of videos is labor-intensive, while the computation- and memory-intensive demands of utilizing ViT-H or ViT-L-based computational alternatives to evaluate the entire video are high.</p>
<p>To surmount these mentioned challenges, we propose InstructVideo, a model that efficiently instructs text-to-video diffusion models to follow human feedback.</p>
<p>We recast the problem of reward fine-tuning as an editing procedure. This reformulation requires only partial inference of the DDIM sampling chain, thereby reducing computational demands while improving fine-tuning efficiency.</p>
<p>Our method focuses on refining coarse and structural videos into more detailed and nuanced outputs. During generation, the optimized model retains the capability to produce videos directly from textual inputs.</p>
<p>In conjunction with back-propagation truncation of the sampling chain, we make reward fine-tuning on text-to-video diffusion models computationally attainable and effective.</p>
<p>We postulate that the visual excellence of a video is tied to both the quality of its individual frames and the fluidity of motion across consecutive frames.</p>
<p>To this end, we resort to off-the-shelf image reward models to ascertain frame quality. We propose Segmental Video Reward (SegVR), which strategically evaluates video quality based on a subset of sparely sampled frames.</p>
<p>By providing sparse reward signals, SegVR offers dual benefits: it not only ameliorates computational burden but also mitigates temporal modeling collapse.</p>
<p>To mitigate visual artifacts such as structure twitching and color jittering, we propose Temporally Attenuated Reward (TAR), which operates under the hypothesis that central frames should be assigned paramount importance, with emphasis tapering off towards peripheral frames.</p>
<p>This strategic allocation of importance across frames ensures a more stable and visually coherent video generation process.</p>
<p>As part of our pioneering effort to align video diffusion models with human preferences, we conduct extensive experiments to assess the practicality and efficacy of integrating image reward models within InstructVideo.</p>
<p>Our findings reveal that InstructVideo markedly enhances the visual quality of generated videos without sacrificing the modelâ€™s generalization capabilities, setting a new precedent for future research in video generation.</p>
<p>We compare it with ModelScopeT2V utilizing 20 and even 50 DDIM steps. Examining the examples, we observe that the quality of videos generated by InstructVideo consistently outperforms the base model by a margin.</p>
<p>Specifically, notable enhancements include clearer and more coherent structures and scenes even if the animal is moving; more appealing coloration; an enhanced delineation of scene details; and improved video-text alignment.</p>
<p>Remarkably, these advancements are achieved without compromising motion fluidity and the resultant videos can often surpass the video quality of the WebVid10M dataset.</p>
<p>These qualitative leaps, consistently favored by human annotators, are attributed to the reward fine-tuning process, which effectively refines the video diffusion model.</p>
<p>We compare with other representative reward fine-tuning methods, including policy gradient algorithm, DDPO, reward-weighted regression, RWR and direct reward back-propagation method, DRaFT.</p>
<p>Both RWR and DDPO exhibit a performance plateau after about 11 hours of fine-tuning, with further optimization failing to enhance or even deteriorating performance.</p>
<p>Direct reward back-propagation methods, including InstructVideo and DRaFT, initially lag during the first 11 hours but subsequently demonstrate fine-tuning efficiency, especially InstructVideo.</p>
<p>InstructVideo outperforms other alternatives, affirming its superior generalization capabilities.</p>
<p>We observe that our method consistently outperforms other methods. Specifically, improvements in video quality, a noted shortcoming of the base model, are more pronounced than improvements in video-text alignment.</p>
<p>An increase in noise level correlates with a progressive enhancement in the highest reward scores achieved by InstructVideo.</p>
<p>However, excessively prolonged fine-tuning precipitates a sharp decline in generative performance.</p>
<p>Optimally, a noise level of 0.6 strikes a balance, providing a feasible starting point for editing that still allows for a substantial exploration of the edited space.</p>
<p>A relatively high value for the temporal attenuation rate results in faster decay towards border frames, thus providing diminished reward signals.</p>
<p>A relatively low value leads to more gentle decay towards those border frames, thus strengthening the reward signals.</p>
<p>Thus, an appropriate coefficient to ensure stable fine-tuning is imperative and we finalize on a temporal attenuation rate of 1.0.</p>
<p>Removing either SegVR or TAR results in a noticeable reduction in temporal modeling capabilities. These observations underscore the critical roles of SegVR and TAR in maintaining fine-tuning stability.</p>
<p>The employment of higher-quality data, i.e., WebVid10M, yields superior average reward scores compared to that obtained using the lower-quality counterpart.</p>
<p>This suggests that superior fine-tuning data can facilitate reward fine-tuning.</p>
<p>The quality of the fine-tuning data does not impose a ceiling on the potential quality of the fine-tuned results.</p>
<p>Our fine-tuning pipeline has the propensity to surpass the initial data quality, thus facilitating the generation of videos with substantially enhanced reward scores.</p>
<h2>Original Abstract</h2>
<p>Diffusion models have emerged as the de facto paradigm for video generation. However, their reliance on web-scale data of varied quality often yields results that are visually unappealing and misaligned with the textual prompts. To tackle this problem, we propose InstructVideo to instruct text-to-video diffusion models with human feedback by reward fine-tuning. InstructVideo has two key ingredients: 1) To ameliorate the cost of reward fine-tuning induced by generating through the full DDIM sampling chain, we recast reward fine-tuning as editing. By leveraging the diffusion process to corrupt a sampled video, InstructVideo requires only partial inference of the DDIM sampling chain, reducing fine-tuning cost while improving fine-tuning efficiency. 2) To mitigate the absence of a dedicated video reward model for human preferences, we repurpose established image reward models, e.g., HPSv2. To this end, we propose Segmental Video Reward, a mechanism to provide reward signals based on segmental sparse sampling, and Temporally Attenuated Reward, a method that mitigates temporal modeling degradation during fine-tuning. Extensive experiments, both qualitative and quantitative, validate the practicality and efficacy of using image reward models in InstructVideo, significantly enhancing the visual quality of generated videos without compromising generalization capabilities. Code and models will be made publicly available.</p>
</body>
</html>
