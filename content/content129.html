<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>Single-image super-resolution (SISR) involves generating high-resolution (HR) images from low-resolution (LR) counterparts, a process crucial in various applications including video surveillance, medical diagnosis, and photography.</p><p>SISR is challenging due to the diverse real-world degradation patterns and the inherent ill-posed nature of the task, where different HR images can correspond to the same LR image.</p><p>SISR methods are generally categorized into regression-based and generation-based approaches.</p><p>Regression-based methods focus on minimizing pixel-level discrepancies, i.e., distortion, between SR predictions and HR references.</p><p>However, this approach often fails to capture the perceptual quality of images.</p><p>To address this, generation-based methods employ deep generative models, including autoregressive models, variational autoencoders (VAEs), normalizing flows (NFs), and generative adversarial networks (GANs), aiming to improve the perceptual aspects of SR images.</p><p>Recently, Diffusion Probabilistic Models (DPMs), a novel class of generative models, have attracted increased interest for their impressive generative abilities, especially in the SISR task.</p><p>Nonetheless, DPM-based methods face challenges due to their dependence on a long sampling chain, which can lead to error accumulation and reduce training and sampling efficiency.</p><p>A further issue is the discrepancy between training and sampling: training typically involves denoising noisy images conditioned on ground truth samples, whereas testing (or sampling) conditions on previously self-generated results.</p><p>To bridge the gap between training and sampling in diffusion models, we introduce DREAM, an end-to-end training framework denoting Diffusion Rectification and Estimation-Adaptive Models.</p><p>DREAM consists of two key elements: diffusion rectification and estimation adaptation.</p><p>Diffusion rectification extends traditional diffusion training with an extra forward pass, enabling the model to utilize its own predictions.</p><p>However, solely relying on this self-alignment can compromise perceptual quality for the sake of reducing distortion.</p><p>To counter this, our estimation adaptation strategy balances standard diffusion and diffusion rectification by adaptively incorporating ground-truth information.</p><p>The DREAM framework excels in its simplicity, easily integrating into existing diffusion-based models with only three lines of code and requiring no alterations to the network architecture or sampling process.</p><p>When applied to the SR task, DREAM has notably improved generation quality across various diffusion-based SR methods and datasets.</p><p>DREAM accelerates training convergence by 2 to 3 times and improves sampling efficiency, requiring 10 to 20 times fewer steps for comparable or superior results.</p><p>It also demonstrates enhanced out-of-distribution (OOD) SR results compared to baseline methods.</p><p>We introduce DREAM, a simple yet effective framework to alleviate the training-sampling discrepancy in standard diffusion models, requiring minimal code modifications.</p><p>We demonstrate the application of DREAM to various diffusion-based SR methods, resulting in significant improvements in distortion and perception metrics.</p><p>The proposed DREAM also notably speeds up training convergence, enhances sampling efficiency, and delivers superior out-of-distribution (OOD) results.</p><p>Our approach, distinct from previous unconditional methods, addresses discrepancies based on predictions relative to the conditional input data, ensuring a tailored and accurate solution for complex visual prediction tasks like SISR.</p><p>Our method also draws inspiration from step-unrolling techniques in depth estimation and text generation, leveraging the model’s own predictions for error estimation.</p><p>However, we uniquely integrate self-estimation with adaptive incorporation of ground-truth data.</p><p>This integration, guided by the pattern of estimation errors, effectively balances perceptual quality and distortion, enhancing generated image qualities.</p><p>Training diffusion models for SR presents a critical challenge, stemming from a discrepancy between the training and inference phases, which we term as training-sampling discrepancy.</p><p>During the training phase, the model operates on actual data, wherein the noisy image at diffusion step is derived from the ground-truth HR image.</p><p>However, during the inference phase, the ground truth is unavailable.</p><p>The model now operates on predicted data, where the noisy image is obtained from the preceding sampling step.</p><p>Due to the estimation error, the noisy image constructed in these two processes usually differs, giving rise to the training-sampling discrepancy.</p><p>This underscores the efficacy of our approach in bridging the training-sampling discrepancy and thereby facilitating more accurate predictions.</p><p>The goal of diffusion rectification is to modify the behavior of the diffusion training to account for the training-sampling discrepancy.</p><p>We extend the diffusion training framework to align more closely with the sampling process, enabling the model to utilize its own output for prediction.</p><p>This DRM approach strives not only to eliminate the sampled noise but also to address the error term arising from the discrepancy between the imperfect estimation and the ground-truth.</p><p>While DRM incorporates additional rectification supervision to account for the sampling process, its naive application to the SR task might not deliver satisfactory results.</p><p>A distortion-perception tradeoff is observed in the generated SR images.</p><p>Despite achieving a state-of-the-art PSNR (less distortion), the images produced by DRM tend to be smoother and lack fine details, reflecting a high FID score (poor perception).</p><p>To address the issue, and inspired by the powerful generative capability of the standard diffusion model, we propose an estimation adaptation strategy.</p><p>This aims to harness both the superior quality of standard diffusion and the reduced distortion offered by diffusion rectification.</p><p>We adaptively inject ground-truth information by blending it with the self-estimation.</p><p>DREAM reaches convergence at around 100k to 150k iterations, a significant improvement over the standard diffusion-based SR3’s 400k iterations.</p><p>DREAM not only converges faster but also surpasses SR3’s final results before its own convergence.</p><p>DREAM achieves a PSNR of 28.07 and FID of 14.72 at just 470k iterations, while SR3 with standard diffusion reaches PSNR 27.02 and FID 16.72 after 980k iterations, indicating a 2× speedup in training.</p><p>DREAM achieves improved distortion metrics and comparable perceptual quality with only 100 steps, marking a 20× speedup in sampling.</p><p>DREAM training approach significantly enhances model robustness, producing more realistic and clearer images across different scales.</p><p>DREAM captures finer details such as the beard of cats, the structural integrity of a tower, and the intricate wrinkles on a bed.</p><p>DREAM training framework consistently improves baseline model across diverse datasets and scales.</p><p>This paper introduces DREAM, a novel training framework designed to address the training-sampling discrepancy in conditional diffusion models with minimal code modifications.</p><p>DREAM comprises two key components: diffusion rectification and estimation adaptation.</p><p>Extensive experiments demonstrate that DREAM enhances distortion and perception metrics across various diffusion-based SR baselines.</p><p>It also speeds up training, improves sampling efficiency, and achieves robust OOD performance across diverse datasets and scales.</p>
    <h2>Original Abstract</h2> 
    <p>We present DREAM, a novel training framework representing Diffusion Rectification and Estimation-Adaptive Models, requiring minimal code changes (just three lines) yet significantly enhancing the alignment of training with sampling in diffusion models. DREAM features two components: diffusion rectification, which adjusts training to reflect the sampling process, and estimation adaptation, which balances perception against distortion. When applied to image super-resolution (SR), DREAM adeptly navigates the tradeoff between minimizing distortion and preserving high image quality. Experiments demonstrate DREAM's superiority over standard diffusion-based SR methods, showing a 2 to 3× faster training convergence and a 10 to 20× reduction in sampling steps to achieve comparable results. We hope DREAM will inspire a rethinking of diffusion model training paradigms. Our source code is available at link.</p>
</body>
</html>
