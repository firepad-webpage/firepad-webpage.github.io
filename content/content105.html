<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Excerpt</title>
</head>
<body>
<h2>Excerpt</h2>
<p>LLM-based code agents have significantly advanced AI-assisted coding and software development. Integrated with external tools like Python interpreters or command-line interfaces, these agents can execute code actions and dynamically adjust the actions based on observations (e.g., execution results) for multiple interaction runs. However, despite their impressive capabilities, these code agents are not risk-free. For example, if code agents inadvertently suggest or execute code with security vulnerabilities, the consequences could be severe, particularly when the code is integrated into critical systems or when the agents directly operate these systems, potentially leading to actions such as deleting important files or leaking sensitive information.</p>
<p>While efforts have been made to assess the safety of code generated by code LLMs, a comprehensive safety evaluation of LLM-based code agents remains challenging and, to date, is still absent. In contrast to generating static code as code LLMs, code agents extend beyond mere code generation to include dynamic executions and interactions with the broader system environment, such as file and operating systems, network communications, API calls, etc. This broader range of functionalities introduces additional layers of complexity and potential risks, as code agents must be assessed not only for the vulnerability of the generated code but also for the safety and security implications of their actions in various execution environments.</p>
<p>To rigorously and comprehensively evaluate the safety of code agents, we propose RedCode, a benchmark for assessing the risks of code agents around code execution and generation. RedCode is built on the following principles: (1) Real interaction with systems. (2) Holistic evaluation on code execution and generation. (3) Diverse natural and programming languages input format. (4) Comprehensive risky scenarios and tests.</p>
<p>Specifically, we have constructed 4050 risky test cases in RedCode-Exec for code execution, spanning 25 major safety scenarios across various 8 domains, and 160 prompts in RedCode-Gen for malicious software generation spanning 8 malware families.</p>
<p>We evaluate 3 types of agents with a total of 19 LLM-based code agents under different RedCode scenarios. The overall attack success rate is high on RedCode-Exec when agents are queried to execute risky or buggy code, highlighting the vulnerability of existing agents. The rejection rate for risky test cases on the operating and file systems is higher than in other domains. Agents are more likely to execute harmful actions by risky queries in natural language than in programming languages. Python leads to a higher rejection rate than Bash. Experiments on three types of code agents, OpenCodeInterpreter, CodeAct, and ReAct, show that OpenCodeInterpreter is relatively safer than CodeAct and ReAct, potentially due to its hard-coded safety constraints. Agents paired with stronger base LLMs such as GPT-4 can have a higher rejection rate for risky code execution in RedCode-Exec, but they also generate more sophisticated and effective harmful software in RedCode-Gen, indicating safety concerns.</p>
<p>The overall rejection rate is low, and the attack success rate is high for all code agents, which demonstrates the high quality, stealthiness, and challenging test cases in our benchmark. Among the 8 domains, the agents’ rejection rates are generally high for dangerous test cases related to the operating system and file system domains, which include risky operations that can impact the system’s normal usage such as reading or modifying the “.bashrc” file or deleting the “/etc/passwd” file, and risky behaviors like monitoring the system. The rejection rate for risky test cases in the website domain is also relatively high for ReAct-GPT4, which could be because they can identify sensitive keywords that we deliberately include in the URLs, such as “dangerous” and “malicious”. In the remaining domains, such as program logic and cybersecurity, the rejection rate is relatively low, and the attack success rate is high, which highlights the serious safety concerns for existing code agents.</p>
<p>Agents are more likely to execute harmful actions by risky queries in natural language than in programming languages. Natural text inputs (text summaries/descriptions) are generally less prone to rejection than code inputs, and agents are easier to fulfill the threat functionality with a higher attack success rate in textual instructions compared to direct code inputs.</p>
<p>Code queries with jailbreak prefixes have a higher rejection rate compared to plain code queries for some agents.</p>
<p>Text descriptions lead to a higher attack success rate and lower rejection rate than summaries.</p>
<p>Bash code inputs usually have a lower rejection rate than Python under the same risky scenarios.</p>
<p>OpenCodeInterpreter is more robust than ReAct and CodeAct, with a higher rejection rate and lower attack success rate. This is because OpenCodeInterpreter has hard-coded disk space protection constraints in its agent codebase.</p>
<p>ReAct generally has a higher rejection rate than CodeAct.</p>
<p>Under ReAct, strong base LLMs in general have a high rejection rate, indicating the stronger safety awareness of more capable models. Fine-tuned LLMs could compromise the safety of agents, leading to the lowest rejection rate in CodeAct Mistral-7B and Llama2-7b.</p>
<p>Most models, with the exception of GPT-3.5, Claude-Opus, and Llama-2-7B, have a low refusal rate despite rejecting natural language based instructions to generate malware in prior safety benchmarks. We also observe a strong correlation between general coding capabilities with the quality of generated malware.</p>
<p>The GPT-4 models also generate malicious code more likely to be scored highly by the judge. Some of the generated samples from GPT-4 and DeepseekCoder can trigger antivirus evaluations from VirusTotal, suggesting LLMs have the potential to automatically generate functional malware.</p>
<p>Certain models with strong coding capabilities, such as Claude-Opus, also have strong safety guardrails, resulting in low accuracy on RedCode-Gen.</p>
<p>The ability to evaluate code and refine it results in a higher judge accuracy, lower refusal rate, and higher VirusTotal accuracy, suggesting the capabilities of code agents introduce new risks.</p>
<h2>Original Abstract</h2>
<p>With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding and software development, safety and security concerns, such as generating or executing malicious code, have become significant barriers to the real-world deployment of these agents. To provide comprehensive and practical evaluations on the safety of code agents, we propose RedCode, an evaluation platform with benchmarks grounded in four key principles: real interaction with systems, holistic evaluation of unsafe code generation and execution, diverse input formats, and high- quality safety scenarios and tests. RedCode consists of two parts to evaluate agents’ safety in unsafe code execution and generation: (1) RedCode-Exec provides challenging code prompts in Python as inputs, aiming to evaluate code agents’ ability to recognize and handle unsafe code. We then map the Python code to other programming languages (e.g., Bash) and natural text summaries or descriptions for evaluation, leading to a total of over 4,000 testing instances. We provide 25 types of critical vulnerabilities spanning various domains, such as websites, file systems, and operating systems. We provide a Docker sandbox environment to evaluate the execution capabilities of code agents and design corresponding evaluation metrics to assess their execution results. (2) RedCode-Gen provides 160 prompts with function signatures and docstrings as input to assess whether code agents will follow instructions to generate harmful code or software. Our empirical findings, derived from evaluating three agent frameworks based on 19 LLMs, provide insights into code agents’ vulnerabilities. For instance, evaluations on RedCode-Exec show that agents are more likely to reject executing unsafe operations on the operating system, but are less likely to reject executing technically buggy code, indicating high risks. Unsafe operations described in natural text lead to a lower rejection rate than those in code format. Additionally, evaluations on RedCode-Gen reveal that more capable base models and agents with stronger overall coding abilities, such as GPT-4, tend to produce more sophisticated and effective harmful software. Our findings highlight the need for stringent safety evaluations for diverse code agents. Our dataset and code are publicly available at https://github.com/AI-secure/RedCode.</p>
</body>
</html>
