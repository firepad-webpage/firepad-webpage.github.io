<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>Bridging the gap between the continuous nature of deep learning and the discrete intrinsic property of natural languages has been one of the most fundamental and essential questions since the very beginning.</p>
<p>Continuous representation makes the training of neural networks effective and efficient.</p>
<p>Nowadays, representing discrete natural languages in continuous format is the first and foremost step to leveraging the capabilities of deep learning.</p>
<p>One could even argue that the exhilarating advancements in natural language processing in the past decade can largely be attributed to the word embedding technique, as it is the first successful attempt.</p>

<p>However, the essence of this operation is still one-hot encoding, even the following subword tokenization techniques attempt to mitigate this issue by decomposing words into subword units, these approaches still require the building of vocabularies and embedding matrices that consist of tens of thousands of tokens.</p>
<p>In the era of large language models, these embedding matrices typically account for a considerable number of parameters, especially in cross-lingual models.</p>
<p>Moreover, imposing structural constraints on continuous representations to model relations among tokens is considered difficult, whereas it is easy and common in discrete representations.</p>
<p>Therefore, further bridging the gap has become increasingly important nowadays.</p>

<p>Recently, Wang and colleagues introduced a novel binary representation that lies between continuous and discrete representations.</p>
<p>They proposed a contrastive hashing method to compress continuous hidden states into binary codes.</p>
<p>These codes contain all the necessary task-relevant information, and using them as the only inputs can reproduce the performance of the original models.</p>
<p>Their binary representation breaks tokens down into combinations of semantic subspaces.</p>
<p>In this paper, we explore the possibility of further introducing this representation to output layers.</p>
<p>Therefore, we attempt to endow models with this capability by extending previous contrastive hashing to structured contrastive hashing.</p>

<p>We begin by upgrading the CKY, which parses sentences and returns spans with discrete labels, to support binary format labels.</p>
<p>Subsequently, we define a new similarity function by using span marginal probabilities obtained from this bit-level CKY to jointly learn label and structural information.</p>
<p>Furthermore, we conduct a detailed analysis of several widely used contrastive learning losses, identifying the geometric center issue, and introduce a novel contrastive learning loss to remedy it through carefully selecting instances.</p>
<p>By doing so, we show that it is feasible to introduce binary representation to output layers and have them output binary labels on trees.</p>

<p>For a given sentence, constituency parsing aims at detecting its hierarchical syntactic structures.</p>
<p>Previous work decomposes tree score as the sum of its constituent scores.</p>
<p>Constituent score reflects the joint score of selecting the specified span and assigning it the specified label.</p>
<p>Under the framework of graphical probabilistic models, they can efficiently compute the conditional probability by applying the CKY algorithm.</p>
<p>Marginal probability is also frequently mentioned.</p>
<p>Intuitively, marginal probability indicates the joint probability of selecting a specified span with a specified label.</p>
<p>Therefore, it is easy to notice that merely summing the marginal probabilities for all labels of a given span does not always yield one.</p>

<p>Contrastive learning is an effective yet simple representation learning method, which involves pulling together positive pairs and pushing apart negative pairs in a metric space.</p>
<p>Recently, Wang and colleagues extended this approach as contrastive hashing.</p>
<p>They append an untrained transformer to the end of a pre-trained language model and use its attention scores for both task learning and hashing.</p>
<p>Specifically, its entire attention probabilities are used to compute hidden states for downstream tasks as usual, and its diagonal entries of the attention scores are employed for hashing.</p>

<p>To leverage the multi-head mechanism, they allow each head to represent one and only one bit.</p>
<p>During the inference stage, codes are generated by binarizing these scores.</p>
<p>Apart from this similarity function, they also propose a novel loss by carefully selecting instances and eliminating potential positives and negatives.</p>
<p>Experiments show that they can reproduce the original performance on an extremely tiny model using only these twenty-four-bit codes as inputs.</p>
<p>Therefore, they claim that these codes preserve all the necessary task-relevant information.</p>

<p>Our model attempts to learn parsing and hashing simultaneously with a single structured contrastive hashing loss.</p>
<p>We decompose tree scores as the sum of constituent scores as well, but with discrete labels replaced with binary codes.</p>
<p>We assume that the bits are independent of each other, so we simply add their scores together to obtain the span score.</p>
<p>We extend the CKY module to the bit-level and calculate the conditional probability and partition function as usual.</p>
<p>Additionally, we define bit-level marginal probability.</p>

<p>Since we can straightforwardly obtain the span marginal probabilities, we then binarize scores into codes towards the sides with the higher span marginal probabilities.</p>
<p>We define the similarity function between two spans as the marginal probability of selecting the first span while assigning the second as its code.</p>
<p>Therefore, the model learns structural and label information simultaneously.</p>

<p>To keep the training of contrastive hashing stable, we collect sentences until the total number of tokens in each batch reaches one thousand and twenty-four.</p>
<p>We employ Adam optimizer, and the total number of training steps of constituency parsing and nested named entity recognition are fifty thousand and twenty thousand, respectively.</p>
<p>Experiments are all conducted on a single NVIDIA Tesla V100 graphics card, the total training wall time is around three hours and one hour, respectively.</p>
<p>Our model consistently achieves competitive performance on various structured prediction tasks and datasets.</p>

<p>Our models outperform almost all previous graph-based methods that rely on maximizing the log-likelihoods of target trees.</p>
<p>Therefore, we claim that leveraging contrastive learning is beneficial to representation learning.</p>
<p>Our methods outperform the previous sequential labeling methods and graph-based methods.</p>
<p>Our model can also achieve comparable results to them.</p>

<p>We note that selecting negatives union self is always superior to selecting negatives union positives in the negative term, but using self is inferior to using positives in the positive term.</p>
<p>Therefore, we claim for the setting that aims at straightforwardly hashing both the label and structural information using a single loss, our max-based strategy is the best choice.</p>

    <h2>Original Abstract</h2>
    <p>Bridging the gap between continuous neural representations and the inherently discrete nature of natural language remains a critical challenge in natural language processing (NLP). While embedding techniques have enabled significant advances, they introduce large vocabularies and embedding matrices, particularly in cross-lingual models, and make it difficult to model explicit token relations. To address this, we build upon recent work on contrastive hashing, which compresses continuous representations into binary codes that retain task-relevant information. We introduce structured contrastive hashing, a novel framework that integrates binary representations into output layers, enabling models to generate binary labels for syntactic trees. Specifically, we extend the CKY parsing algorithm to the bit level, define a new similarity function based on span marginal probabilities, and propose a tailored contrastive loss to stabilize training and address issues with negative instance selection. Our approach jointly learns syntactic structures and label representations in binary format. Experiments across constituency parsing and nested named entity recognition tasks demonstrate that our model achieves competitive or superior performance compared to existing graph-based and sequential labeling methods. These results highlight the effectiveness of contrastive learning in enhancing representation learning for structured prediction tasks.</p>
</body>
</html>
