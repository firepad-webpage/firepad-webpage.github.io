<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>Pathological examination serves as the gold standard for cancer diagnosis.</p><p>With the fast development of digital scanning devices, traditional glass slides can be rapidly digitized into the whole slide image.</p><p>WSI exhibits a hierarchical structure and huge size, which typically has about one billion pixels at its highest resolution.</p><p>Therefore, obtaining a substantial dataset with accurate pixel-level annotations can often be challenging and unattainable.</p><p>To alleviate this problem, various weakly supervised learning methods have been proposed.</p><p>Among them, multiple instance learning has gained significant popularity for tackling WSI classification tasks.</p><p>MIL utilizes small patches (i.e., instances) to generate the slide-level (i.e., bag-level) representation for analysis.</p><p>MIL-based methods usually follow a three-step pipeline: patch cropping, feature extraction using a pretrained encoder, and slide-level feature aggregation for WSI classification.</p><p>They have achieved significant success in various pathological diagnostic tasks like cancer subtyping, staging, and tissue segmentation.</p><p>However, training these MIL-based models still heavily relies on a large number of slides with bag-level labels which are often unreachable for rare diseases.</p><p>Moreover, these models only learn from the original slide, making them vulnerable to variations in data distribution and leading to sub-optimal generalization performance.</p><p>Recently, a series of emerging works based on visual language model such as CLIP and BLIP introduced the language information, bringing new advancements to the fields of natural image classification, segmentation, object detection, etc.</p><p>In the field of pathology, several VLM-based methods like MI-Zero, BiomedCLIP, PLIP, and QUILT have also been proposed and achieved promising results in diverse pathological diagnostic tasks and datasets.</p><p>These methods adopt the two-tower encoders (i.e., image and text encoders), which are pre-trained on a large number of patch-text pairs collected from the Internet.</p><p>Firstly, the text prompt does not provide effective guidance for identifying ambiguous categories, as it lacks the consideration of pathological prior knowledge.</p><p>Introducing visual descriptive texts can help the model focus on diagnosis-related features and enhance its discriminative ability in classes with subtle differences.</p><p>Secondly, transferring the VLM-based model to the field of pathology in a parameter-efficient way is challenging.</p><p>To address the above limitations, in this work, we propose a dual-scale vision-language multiple instance learning framework for whole slide image classification.</p><p>We construct our dual-scale visual descriptive text prompt, which corresponds to WSIs at different resolutions, based on the frozen large language model.</p><p>The low-scale visual descriptive text prompt mainly focuses on the global tumor structure presented in a WSI of low magnification.</p><p>The high-scale visual descriptive text prompt pays more attention to local finer details on a high-resolution WSI.</p><p>We propose a prototype-guided patch decoder to progressively guide the fusion process of patch features by grouping similar patch features into the same prototype.</p><p>We propose a context-guided text decoder to enhance the text features by incorporating the multi-granular image contexts.</p><p>ViLa-MIL achieves the best results compared with the current state-of-the-art MIL-based methods under the few-shot settings.</p><p>Multiple Instance Learning in WSI.</p><p>The whole process mainly includes three steps: 1) a series of patches are cropped from the original WSI; 2) a pre-trained encoder is utilized to extract the patch features; 3) the patch features are aggregated to generate the final slide features.</p><p>CLAM utilizes a pre-trained image encoder for patch feature extraction and proposes a multi-branch pooling operator trained for weakly-supervised WSI classification tasks.</p><p>Based on CLAM, a series of methods have been proposed to explore how to aggregate the patch features effectively.</p><p>Although these methods have achieved great success in many pathological diagnostic tasks, they rely solely on bag-level labels for training, thus requiring the model to learn discriminative patterns from a large quantity of WSIs, without fully utilizing pathological prior knowledge as the guideline.</p><p>In this work, our ViLa-MIL introduces the dual-scale visual descriptive text prompt as the language prior to guide the training of the model effectively.</p><p>The vision language models, like CLIP and FLIP, have shown remarkable performance in a wide variety of visual recognition tasks.</p><p>Although these works have demonstrated significant classification performance and transferability in a new dataset, collecting a large number of image-text pairs is extremely time-consuming and labor-intensive.</p><p>In this work, we utilize the frozen large language model to generate the dual-scale visual descriptive text prompt, which aids the model in transferring to the target dataset with the guidance of limited labeled data.</p><p>Moreover, two lightweight and trainable decoders are proposed, with one for the image branch and the other for text, to transfer the VLM model to the field of pathology efficiently.</p><p>Combining this diagnostic prior with the multi-scale characteristics of WSI, we propose our dual-scale visual descriptive text prompt to guide the CLIP model for WSI classification.</p><p>These dual-scale visual descriptive texts are consistent with the daily practice of pathologists, which can help the model distinguish subtle and fine-grained pathological morphological features and improve the model’s classification performance.</p><p>To apply the CLIP model to process WSI efficiently, another important question is how to aggregate a large number of patch features.</p><p>Following the current embedding-based MIL methods, a non-overlapping sliding window method is utilized to crop patches I from the WSI.</p><p>By introducing the guidance of learnable prototypes, the patches with high semantic similarity will be grouped into the same prototype.</p><p>Compared with the local patches with the limited receptive field, each prototype captures more global context information.</p><p>By bridging the gap between the two modalities, the model can achieve better alignment of images and texts.</p><p>Finally, the whole model is trained end-to-end, and the cross-entropy loss is formally defined.</p><p>We evaluate our ViLa-MIL on three real-world WSI subtyping datasets, namely TIHD-RCC, TCGA-RCC, and TCGA-Lung, under the few-shot setting.</p><p>GPT-3.5 is taken as the frozen large language model.</p><p>ViLa-MIL achieves significant performance improvement across all metrics on all three datasets.</p><p>Specifically, with the guidance of the visual descriptive text prompt, the single-scale ViLa-MILs already demonstrate promising performance, while the performance is boosted further with the dual-scale text prompt and image features.</p><p>ViLa-MIL learns an embedding space that exhibits a higher level of intra-class compactness and inter-class separability.</p><p>ViLa-MIL still achieves an improvement of 5.5% in the AUC metric compared with the best baseline, indicating its superior cross-domain adaptability and robustness.</p><p>Our dual-scale visual descriptive text prompt contains comprehensive pathological diagnostic information, ensuring ViLa-MIL to have a better generalization ability across multiple centers.</p><p>This indicates that our dual-scale visual descriptive text prompt can boost the WSI classification performance by effectively utilizing the complementary image features at multiple magnifications.</p><p>The model’s performance also shows a certain amount of improvement, which denotes that the multi-granular patch and prototype features can refine the text features further.</p><p>Our decoder achieves the best results under all three metrics on the TIHD-RCC dataset.</p><p>Our ViLa-MIL significantly outperforms all the other methods, indicating its capability to learn more discriminative visual features and improve the model’s classification ability for challenging and ambiguous samples.</p><p>All variants significantly outperform other baselines and superior performances have been achieved with different LLMs, indicating high robustness of ViLa-MIL to different kinds of frozen LLMs.</p><p>In this work, we proposed a dual-scale vision-language multiple instance learning framework for whole slide image classification.</p><p>To transfer the VLM to process the WSI efficiently, a prototype-guided patch decoder was proposed to progressively aggregate the patch features.</p><p>A context-guided text decoder was also proposed to refine the text prompt features further by utilizing the multi-granular image contexts.</p><p>Extensive comparative and ablation experiments on three cancer subtyping datasets demonstrated that ViLa-MIL achieved state-of-the-art results for whole slide image classification.</p>
    <h2>Original Abstract</h2> 
    <p>Multiple instance learning (MIL)-based framework has become the mainstream for processing the whole slide image (WSI) with giga-pixel size and hierarchical image context in digital pathology. However, these methods heavily depend on a substantial number of bag-level labels and solely learn from the original slides, which are easily affected by variations in data distribution. Recently, vision language model (VLM)-based methods introduced the language prior by pre-training on large-scale pathological image-text pairs. However, the previous text prompt lacks the consideration of pathological prior knowledge, therefore does not substantially boost the model’s performance. Moreover, the collection of such pairs and the pre-training process are very time-consuming and source-intensive. To solve the above problems, we propose a dual-scale vision-language multiple instance learning (ViLa-MIL) framework for whole slide image classification. Specifically, we propose a dual-scale visual descriptive text prompt based on the frozen large language model (LLM) to boost the performance of VLM effectively. To transfer the VLM to process WSI efficiently, for the image branch, we propose a prototype-guided patch decoder to aggregate the patch features progressively by grouping similar patches into the same prototype; for the text branch, we introduce a context-guided text decoder to enhance the text features by incorporating the multi-granular image contexts. Extensive studies on three multi-cancer and multi-center subtyping datasets demonstrate the superiority of ViLa-MIL.</p>
</body>
</html>
