<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>Large Language Models (LLMs) have evolved to become impressively powerful in recent developments, with Generative Pretrained Transformer (GPT) models showing increasingly effective capabilities.</p><p>The generative nature of these models has led to their widespread adoption in numerous application fields.</p><p>Despite their advanced capabilities, LLMs are capable of generating incorrect results, an issue that is particularly problematic in applications where precision and dependability are critical, like the biomedical and healthcare fields.</p><p>While existing methods show varying degrees of improvement on different tasks, in general there lacks a systematic way to efficiently quantify the likelihood of errors in LLM outputs.</p><p>In many LLM applications, including disease prediction, medical diagnosis, and question answering (QA), a concrete and precise answer is desired.</p><p>For these mission critical tasks, a quantitative error estimation or confidence level for the response is equally important as giving a correct answer itself.</p><p>However, the text-in text-out nature of the generative language models makes it challenging to estimate the error probability of the answer quantitatively.</p><p>Although some LLMs provide internal probability scores for the generated tokens, they are poorly calibrated to the true error rate, particularly after applying reinforcement learning with human feedback.</p><p>Our goal in this paper is to address this issue by establishing a systematic way to quantitatively estimate the likelihood of error in LLM answer.</p><p>We approach this through training an estimator model h via multi-objective optimization, leveraging extensive research in Pareto optimization.</p><p>Given the optimized model h and any LLM response, we can then directly estimate the LLM response error rate which we refer to as the Pareto optimal learning assessed risk (POLAR) score.</p><p>We introduce a novel approach that trains a Pareto-optimal probabilistic model h to simultaneously optimize on LLM and align with the external information sources.</p><p>We propose a novel framework using Pareto optimization aligning to the LLM and multiple external information sources.</p><p>The POLAR score from our framework is shown experimentally to be effective in estimating LLM error rate.</p><p>We demonstrate that POLAR scores can be leveraged to boost an LLM’s performance by easily combining with other popular strategies such as self-verification and retrieval augmented generation.</p><p>In this work we present a framework to systematically estimate error of an LLM output by simultaneously aligning to multiple information sources while circumventing the weighting dilemma through Pareto optimization.</p><p>Our error estimation framework for LLM responses is a two-step process.</p><p>In the first step, we iterate through a corpus of input instances to collect the corresponding LLM responses, while dynamically retrieving heuristic answers from other information sources.</p><p>In this process, a probabilistic function h is learned that fits the multiple sources in a Pareto optimal manner.</p><p>In the second step, the optimized h model is used to estimate the error rate of the LLM response on any new input instance, which we refer to as the POLAR score.</p><p>After the error estimation step, we also provide an optional third step that strategically re-prompt the LLM based on the POLAR score, and leverage the information retrieved from the information sources in an RAG manner.</p><p>The primary challenge here is to design a framework to resolve conflicts among different sources and with the LLM.</p><p>In order for h to align with the multiple information sources, mathematically, we want to solve the following multi-objective problem.</p><p>As the objectives may conflict, we seek an h∗ that is Pareto optimal, following multi-objective learning theory and Pareto optimization.</p><p>The Pareto optimization framework effectively manages dependencies between information sources.</p><p>However, finding Pareto optimal solutions remains challenging in the multi-objective optimization literature.</p><p>We propose finding an optimal solution h∗ by solving a scalarized version of the multi-objective problem, which can be done via standard stochastic gradient descent algorithms such as Adam.</p><p>Once a optimal solution h∗ is found, we can estimate the error rate for any new input, and compute a risk score referred to as the Pareto optimal learning assessed risk (POLAR) score.</p><p>Identifying LLM responses with a higher risk of error presents an opportunity to efficiently correct the errors and improve the final accuracy.</p><p>We provide an optional Step 3, which easily connects the POLAR score to other prompting strategies to correct the error automatically.</p><p>We propose two dynamic prompting strategies to illustrate correcting LLM errors using the POLAR score.</p><p>This setting allows information sources to serve as additional input to the LLM to enhance its answer.</p><p>Dynamic self-verification: if the POLAR score is above a threshold, simply ask the LLM to self-verify its previous answer.</p><p>POLAR-assisted RAG: if the POLAR score is high, retrieve information from all triggered sources and feed it back to the LLM to generate a revised answer.</p><p>We leverage the publicly available datasets collected by Zhang et al. and evaluate on four different NLP tasks: CDR, ChemProt, SemEval, and SMS.</p><p>We do not use any of the ground truth in the training sets, and only use the ground truth labels on test set for evaluation of the LLM error rate and error correction performance.</p><p>To maximize LLM capabilities, we carefully design prompts for each problem, clarifying the problem setting, knowledge background, input/output structure, and instructions for stating “unsure”.</p><p>The information sources in these datasets were created by human experts that utilize knowledge bases, textual patterns, and a combination of the two.</p><p>We choose the quadratic aggregator for G and the BERT model for h.</p><p>For the biomedical CDR and ChemProt, we choose BiomedBERT for h.</p><p>The POLAR score reliably estimates the true probability of LLM error rates.</p><p>The POLAR scores are highly correlated with the true error rate.</p><p>Responses with the highest POLAR scores are most prone to errors, with the top scores indicating nearly a 100 percent error rate.</p><p>The proposed POLAR score consistently outperforms other methods.</p><p>Snorkel, WeaSEL, and LLM distilled model can achieve top or close-to-top performance in some cases under specific metric, but lack the consistency to deliver stable calibration for different LLMs on different tasks.</p><p>The POLAR-assisted dynamic prompting increases the GPT-4 performance.</p><p>GPT-4 with POLAR-assisted RAG outperforms state-of-the-art supervised task-specific model.</p><p>Removing all external sources from the model leads to inconsistent error estimation capabilities.</p><p>External information sources are essential to prevent h from overfitting to LLM responses, especially for the biomedical domain tasks.</p><p>The nonlinear quadratic loss aggregator, when combined with BERT finetuning, delivers superior error estimation performance.</p><p>The Chebyshev aggregator consistently underperforms across various scenarios.</p><p>We presented a novel framework for LLM error estimation using Pareto optimal learning.</p><p>The error estimator learned in our framework aligns with the LLM and other information sources Pareto optimally.</p><p>We showed experimentally that the proposed POLAR score is well calibrated with the LLM error rate evaluated on ground truth, ensuring reliable error estimation.</p><p>We proposed two POLAR-assisted dynamic prompting strategies, and showed that POLAR-assisted RAG enhances GPT-4’s performance, surpassing state-of-the-art task-specific model.</p><p>This development marks a substantial advancement in the application of LLM, providing an effective method to both estimate and reduce LLM errors.</p>
    <h2>Original Abstract</h2> 
    <p>Large Language Models (LLMs) have demonstrated impressive generative capabilities across various domains, yet they remain prone to generating erroneous outputs—particularly concerning in high-stakes fields like biomedicine. This paper addresses the critical need for systematic and reliable error estimation in LLM responses. The authors introduce POLAR (Pareto Optimal Learning Assessed Risk), a novel framework that leverages multi-objective Pareto optimization to align an error estimator model with both the LLM and external information sources. The approach involves training a probabilistic function in two stages: collecting LLM responses alongside heuristic source-derived answers, and optimizing the model to estimate the error probability for new inputs. Experimental evaluations across four biomedical and NLP datasets demonstrate that the POLAR score is highly correlated with true error rates and consistently outperforms existing methods in calibration stability. Furthermore, the framework supports dynamic prompting strategies—self-verification and retrieval-augmented generation—which significantly enhance GPT-4 performance, surpassing state-of-the-art supervised baselines. This work offers a scalable and effective solution for error detection and correction in LLMs, with strong implications for improving model reliability in critical applications.</p>
</body>
</html>
