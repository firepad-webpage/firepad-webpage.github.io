<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset without additional interactions with the environment.</p><p>This characteristic makes it particularly promising for critical applications such as healthcare decision-making, human-AI coordination and autonomous driving.</p><p>Given that the offline data is limited, fine-tuning the policy through interactions with the environment is still necessary to achieve favorable performance.</p><p>Consequently, offline-to-online (O2O) RL tends to achieve faster performance improvements based on better initializations.</p><p>To effectively fine-tune offline policies, O2O methods are typically designed based on specific offline RL algorithms.</p><p>Unfortunately, these methods often suffer from inefficient performance improvement due to restricted action exploration caused by policy constraints.</p><p>However, these methods often face high computational costs due to the need to train multiple Q-networks.</p><p>To establish a general O2O framework, it is essential to address the core issues associated with transitioning from offline to online environments.</p><p>We identify two mismatches in general O2O RL: evaluation mismatches and improvement mismatches.</p><p>Evaluation mismatches primarily occur in value regularization methods.</p><p>Improvement mismatches, on the other hand, are prevalent in policy constraint methods.</p><p>We bridge these two types of mismatches within a unified framework for general RL-based offline algorithms.</p><p>To address the evaluation mismatch in value regularization methods, we propose re-evaluating the offline policy in an optimistic manner using an off-policy evaluation method.</p><p>To handle the improvement mismatch in the re-evaluated critics and policy constraint methods, we introduce value alignment to calibrate the critic so that it aligns with the probabilities predicted by the policy.</p><p>Finally, we propose a constrained fine-tuning framework to guide the policy update by adding a regularization term, with the target of mitigating the negative impact of data shift.</p><p>Extensive experimental results on multiple benchmark environments validate that the proposed methods can achieve better or comparable performance when compared to state-of-the-art methods.</p><p>We show that resolving these two types of mismatches is essential for achieving general O2O RL.</p><p>Policy re-evaluation aims to achieve optimistic Q-value estimates, preventing instability in Q-value estimation.</p><p>Value alignment calibrates the critic to align with the policy, ensuring consistency between action probabilities and their corresponding Q-values.</p><p>We introduce a constrained fine-tuning framework that incorporates a regularization term into the policy objective, combating the inevitable distribution shift and ensuring stable and optimal performance when fine-tuning the policy in online environments.</p><p>Most offline RL methods suffer from one or both of these issues, which underscores the importance of addressing these mismatches to achieve stable and effective online fine-tuning.</p><p>Evaluation mismatch often occurs in the value regularization methods.</p><p>This shift frequently results in a sharp increase in Q-values at the beginning of online fine-tuning, which can hinder stable performance improvements.</p><p>Improvement mismatch is commonly found in policy constraint methods.</p><p>This divergence often misguides the update of policy at the beginning stage of online fine-tuning, resulting in unfavourable performance.</p><p>Offline actor and critic trained by behavior-regularized MDP for initialization in online fine-tuning introduces both types of mismatches, resulting in unstable and inefficient updates.</p><p>We develop a policy re-evaluation technique to optimistically re-evaluate the well-trained offline policy using an off-policy evaluation method.</p><p>We propose value alignment, which aims to align the critic’s estimates with the policy’s action probabilities, effectively tackling the improvement mismatch in both types of methods.</p><p>We develop a constrained fine-tuning framework.</p><p>The goal is to enable the critic to have optimistic estimates of Q-values that more closely approximate the true values.</p><p>With a large training step K, the error will be bounded by an acceptable value.</p><p>This implies that, given sufficient data, one can achieve a critic with optimistic property and minor extrapolation error through policy re-evaluation.</p><p>The misalignment means that the action with the highest Q-value does not necessarily have the highest probability, often leading to misleading updates of the policy.</p><p>The performance of re-evaluated critics sharply declines at the initial stage and does not recover in the subsequent training; while our aligned critic achieves stable and favorable performance.</p><p>These results indicate that the misalignment between the re-evaluated critic and the offline policy can make it difficult for the policy to optimize in a correct direction.</p><p>Given that the well-trained offline policy is reliable, the desirable critic should not only have optimistic Q-value estimates but also maintain alignment with the offline policy.</p><p>We derive Proposition 4.3 to show that the aligned state values can be maintained within an appropriate range, meaning that the estimates remain optimistic while avoiding overestimation.</p><p>This flexibility allows us to implement the transition to SAC from different offline algorithms.</p><p>This enables us to assume that normalized Q-values around the optimal action follow a Gaussian distribution.</p><p>This ensures that the output action of the policy maintains a smoothing and optimistic property of Q-values.</p><p>To mitigate this problem, we propose an auxiliary advantage function to correct erroneous updates.</p><p>The auxiliary advantage enables the policy to update in a reliable region.</p><p>To deal with the inevitable distribution shift between the offline dataset and the online environment, we develop a constrained fine-tuning framework.</p><p>We impose a constraint term on the policy objective to ensure that it updates within the credible region of the reference policy.</p><p>We provide a theoretical guarantee for the proposed CFT framework.</p><p>Experiments validate the effectiveness of the proposed method on D4RL MuJoCo and AntMaze tasks.</p><p>Our method can converge more stably and rapidly than other methods and achieve the optimal performance in most cases.</p><p>Although PPO is an on-policy method with low efficiency, it shows significant superiority on sparse reward tasks.</p><p>One of the advantages of our method is that it imposes no requirements on offline algorithms.</p><p>Our methods achieve the strong transferability from various offline methods.</p><p>To address these two mismatches in O2O RL, we proposed optimistic critic reconstruction to re-evaluate an optimistic critic and align it with the offline actor before online fine-tuning.</p><p>We introduce constrained fine-tuning to constrain the divergence of current policy and the best foregoing policy to maintain the stability of online fine-tuning.</p><p>These two components form a versatile O2O framework, allowing the transition from any offline algorithms to three state-of-the-art online algorithms.</p><p>Experiments show our framework can converge to optimal performance without affecting the aligned critic at the beginning of online fine-tuning and achieve strong empirical performance.</p>
    <h2>Original Abstract</h2> 
    <p>Offline-to-online (O2O) reinforcement learning aims to fine-tune a policy learned from fixed offline data through limited online interactions. While this approach offers promise for high-stakes domains like healthcare and autonomous driving, existing methods often suffer from inefficiencies due to evaluation and improvement mismatches between the offline and online settings. This paper introduces a general O2O framework that addresses these mismatches through three key components: optimistic policy re-evaluation, value alignment between the critic and policy, and a constrained fine-tuning (CFT) framework to mitigate distribution shift. The optimistic re-evaluation produces reliable Q-value estimates, while value alignment ensures consistency between action probabilities and Q-values. The CFT approach incorporates a regularization term into the policy objective to limit updates to credible regions. Extensive experiments on benchmark environments demonstrate that the proposed method achieves more stable and efficient performance than prior approaches and offers strong transferability across various offline algorithms. The results confirm the effectiveness of addressing mismatches to enable general and robust O2O RL.</p>
</body>
</html>
