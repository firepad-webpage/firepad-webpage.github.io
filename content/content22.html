<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>Documents with rich layouts, including invoices, receipts, contracts, orders, and forms, constitute a significant portion of enterprise corpora.</p><p>The automatic interpretation and analysis of these documents offer considerable advantages, which has spurred the development of AI-driven solutions.</p><p>These visually rich documents feature complex layouts, bespoke type-setting, and often exhibit variations in templates, formats and quality.</p><p>Although Document AI (DocAI) has made tremendous progress in various tasks including extraction, classification and question answering, there remains a significant performance gap in real-world applications.</p><p>In particular, accuracy, reliability, contextual understanding and generalization to previously unseen domains continues to be a challenge.</p><p>Document intelligence is inherently a multi-modal problem with both the text content and visual layout cues being critical to understanding the documents.</p><p>It requires solutions distinct from conventional large language models such as GPT-3.5, Llama, Falcon or PaLM that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents.</p><p>Numerous vision-language frameworks that can process documents as images and capture the interactions between textual and visual modalities are available.</p><p>However, these frameworks necessitate the use of complex vision backbone architectures to encode image information, and they often make use of spatial information as an auxiliary contextual signal.</p><p>In this paper we present DocLLM, a light-weight extension to standard LLMs that excels in several visually rich form understanding tasks.</p><p>Unlike traditional LLMs, it models both spatial layouts and text semantics, and therefore is intrinsically multi-modal.</p><p>The spatial layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component.</p><p>Consequently, our solution preserves the causal decoder architecture, introduces only a marginal increase in the model size, and has reduced processing times, as it does not rely on a complex vision encoder.</p><p>We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering.</p><p>Existing efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings or summing the two.</p><p>In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner.</p><p>We extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships.</p><p>This is motivated by the observation that there is often a correlation between the content, position and size of the fields in a form.</p><p>Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.</p><p>When working with such documents, employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive.</p><p>In particular, the preceding tokens may not always be relevant due to the diverse arrangements of text, which can be positioned horizontally, vertically, or even in a staggered manner.</p><p>To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens.</p><p>Due to these modifications, the model is better equipped to address misaligned text, contextual completions, intricate layouts, and mixed data types.</p><p>We adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets.</p><p>These tasks encompass key information extraction, natural language inference, visual question-answering and document classification.</p><p>We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 15% to 61% for the Llama2-7B model in four out of five previously unseen datasets.</p><p>DocLLM is constructed upon the foundation of an auto-regressive transformer language model following a causal decoder structure.</p><p>It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network.</p><p>DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained using OCR.</p><p>We treat the spatial information about the text tokens as a distinct modality.</p><p>We use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner.</p><p>Instead of the traditional left-to-right next token prediction during self-supervised training, we employ a text infilling objective that better leverages contextual information.</p><p>The new attention mechanism is calculated by combining text-to-text, text-to-spatial, spatial-to-text, and spatial-to-spatial components.</p><p>The disentangled representation of these modalities in the attention scores enables selective focus when appropriate, thereby providing an optimal balance between model size and effectiveness.</p><p>DocLLM is first pre-trained in a self-supervised fashion on a large number of unlabeled documents.</p><p>Visual documents are often sparse and irregular, featuring isolated and disconnected text fragments.</p><p>It is preferable to consider coarse segments of related tokens during pre-training rather than focusing on individual tokens.</p><p>Furthermore, learning to infill text, where the prediction is conditioned on both prefix and suffix tokens rather than only preceding tokens, can be beneficial.</p><p>The infilling objectives enable contextually relevant completions, provide robustness to OCR noise or misaligned tokens, and can better handle relationships between various document fields.</p><p>We follow an autoregressive block infilling objective, where text blocks are randomly masked, and the masked blocks are shuffled and reconstructed in a sequential left-to-right fashion.</p><p>Following recent work in the field of VRDU and prior work in NLP, we instruction-tune DocLLM on a variety of instructions derived from DocAI datasets using various templates.</p><p>We employ a total of 16 datasets with their corresponding OCRs, spanning four DocAI tasks: visual question answering (VQA), natural language inference (NLI), key information extraction (KIE), and document classification (CLS).</p><p>DocLLM-7B excels in 12 out of 16 datasets, inclusively compared to ZS results of GPT4 and Llama2, and SDDS results of mPLUG-DocOwl and UReader.</p><p>DocLLM demonstrates superior performance in layout-intensive tasks such as KIE and CLS.</p><p>DocLLM also outperforms mPLUG-DocOwl on DocVQA and both mPLUG-DocOwl and UReader on KLC, despite both baselines having been instruction-tuned on these datasets.</p><p>The vanilla text-only self-attention mechanism yields the lowest NTP accuracy, underlining the importance of incorporating spatial features for understanding documents with rich layouts.</p><p>Autoregressive block infilling exhibits the best performance.</p>
    <h2>Original Abstract</h2> 
    <p>Visually rich documents such as invoices, forms, and contracts pose significant challenges for existing language models due to their complex layouts and spatially dispersed content. While vision-language models address these challenges by incorporating visual encoders, they often rely on heavyweight architectures. This paper introduces DocLLM, a lightweight, multi-modal extension to standard language models that integrates spatial layout information without requiring vision encoders. By treating bounding box coordinates of OCR-extracted text as a separate modality, DocLLM disentangles spatial and textual interactions via an extended self-attention mechanism. It replaces traditional next-token prediction with a novel block infilling objective conditioned on both preceding and succeeding tokens, better capturing context in irregular layouts. Pre-trained on unlabeled documents and fine-tuned using instruction data from 16 datasets spanning four Document AI tasks (VQA, NLI, KIE, and CLS), DocLLM demonstrates strong generalization. It achieves performance gains of 15-61% on unseen datasets and outperforms larger vision-language models like mPLUG-DocOwl and UReader, particularly in layout-sensitive tasks. By preserving a causal decoder architecture with minimal overhead, DocLLM offers a scalable and effective solution for document understanding without sacrificing speed or simplicity.</p>
</body>
</html>
