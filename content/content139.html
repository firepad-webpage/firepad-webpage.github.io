<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>We are in an era of long-running cyber attack campaigns involving “Advanced and Persistent Threats” (APTs).</p><p>Carried out by sophisticated actors that prioritize stealth over other goals, these campaigns often remain undetected for many months.</p><p>During this period, attackers remain hidden in a victim’s network, while moving across hosts, installing malware, and gathering data.</p><p>Because of the stealthy nature of APTs, the primary recourse against them is after-the-fact detection, followed by forensic analysis to understand their full impact.</p><p>Such an analysis requires logs that faithfully capture all important system activity across the hosts in an enterprise.</p><p>Hence it is necessary to collect system-wide logs that cover all applications.</p><p>Most recent research works on APT investigation rely on system audit logs that operate roughly at the level of system calls.</p><p>Coarse-grained provenance tracking enabled by these logs ensures the completeness of forensic analysis, i.e., the analysis won’t miss any of the effects of an attack.</p><p>For server systems that tend to be based on Linux, the Linux auditing daemon auditd is an obvious choice for audit data collection.</p><p>Unfortunately, auditd incurs very high overheads for system-call granularity data collection, slowing down programs by 5× or more.</p><p>However, these systems require OS kernel modifications, making them a challenge from a deployment perspective.</p><p>In addition to these portability and deployability concerns, we show that existing audit data collection systems suffer from serious data loss and performance problems.</p><p>Under moderate to heavy loads, most existing systems drop a significant fraction of the events.</p><p>Even when operating at loads they can cope with, today’s log collection systems incur high overheads, slowing down workloads by 2× to 8×.</p><p>We show that existing systems can buffer hundreds of thousands of log records in memory before the data is output.</p><p>A successful attack can wipe these buffers, thus removing all attack evidence from the logs.</p><p>Existing systems produce logs ranging from several GBs to hundreds of GBs per host per day.</p><p>In this paper, we present a new audit log collection approach that overcomes these challenges.</p><p>It has been implemented into a light-weight and easily deployable system called eAudit.</p><p>eAudit is based on the Extended Berkeley Packet Filter (eBPF) framework built into recent Linux versions.</p><p>Consequently, eAudit can be readily deployed on these kernels without loading kernel modules, or changing the kernel code.</p><p>eAudit works “as is” on most recent Linux distributions.</p><p>We show that all these systems: incur high performance overheads, slowing down systems by 2x to 8x; can drop a large fraction of events; and store events in memory for substantial periods, making it easier for attackers to wipe out suspicious activities before they are logged permanently.</p><p>In Sec. 3, we describe eAudit design, focusing specifically on features for avoiding data loss, reducing runtime overhead, and minimizing opportunities for log tampering.</p><p>We present: a compact data encoding scheme that results in log files that are 10× smaller than those of other systems; a two-level buffer design that reduces contention and avoids data loss; a simple analytical model that underpins an optimal balancing of system throughput and latency; and a granular and tunable event prioritization scheme that further reduces log tampering opportunities.</p><p>eAudit avoids data loss even on peak loads that cause the best previous systems to lose over 90% of the data.</p><p>Our two-level buffer design and parameter tuning optimizations are very effective, reducing overheads by an average of 18.4× across our benchmarks.</p><p>With the benchmarks and metrics used in our motivational study, eAudit’s overhead is just 4.5%.</p><p>Our design and optimizations reduce the log tamper window by about 100× over previous systems.</p><p>Our event prioritization scheme shrinks this window by another 100× for the most important system calls.</p><p>The main goals of our design are: Reduce data volume so as to speed up every component involved in the data pipeline; Eliminate data loss even at peak system loads; Reduce overhead so as to minimize degradation of peak workloads that can be sustained; and Reduce latency of data capture so that records are logged to safe storage quickly, minimizing the opportunities for an adversary to tamper with these records.</p><p>The primary goal of this paper is to develop techniques for efficient provenance collection — techniques that do not degrade the workloads that can be sustained on a target system.</p><p>Our last goal is to degrade the ability of attackers to hide their activities through log tampering.</p><p>The eBPF framework enables small code snippets, called ebpf probes, to be safely deployed at well-defined hooks within the Linux kernel.</p><p>Each probe is a function with the signature specified in the sysfs pseudo file system at /sys/kernel/debug/tracing/events/syscalls/⟨scevent⟩/format.</p><p>System call information, including argument and return values, is first serialized and stored in per-CPU buffers that we call as message caches.</p><p>A side benefit of compact encoding is that we can record the entry and exit events separately, without being overly concerned about log size.</p><p>Due to this encoding, our system call records are 17 bytes on average (postmark benchmark), as compared with 175 bytes for sysdig and 850 bytes for auditd.</p><p>We therefore developed a two level buffering scheme to reduce the number of accesses to the ring buffer.</p><p>Our optimized two-level buffer design incurs tq once every p system calls, and ts every p · w system calls.</p><p>The overhead per system calls to drop to 0.8/100 + 2.9/800 ≈ 0.01µs, which means eAudit can potentially keep up with syscall rates of tens of millions per second.</p><p>The techniques described in the previous two sections helps to reduce the log tampering window significantly from several tens of thousands of records for previous systems (Fig. 4) to the range of hundreds.</p><p>We classify system calls into several categories, and associate a weight with each category.</p><p>Our design improves over theirs in two important ways.</p><p>Our evaluation shows that it comes close, achieving a window of just a few system calls even on very intensive workloads.</p><p>None of the benchmarks resulted in any data loss in the case of eAudit.</p><p>Across these benchmarks, our optimized design decreases the agent overhead by 18.4×.</p><p>The median among these averages is 48 nanoseconds.</p><p>Note that eAudit incurs between 4% and 5% overhead on these benchmarks, while all other systems have overheads that are much higher.</p><p>The average overhead across all the points in Fig. 21 is about 18%.</p><p>We found that it significantly underestimates eAudit’s log tamper window.</p><p>eAudit’s tamper window is several hundreds, as opposed to 10K to 100K records for sysdig.</p><p>eAudit achieves a 100-fold reduction, which significantly raises the bar for log tampering attacks.</p><p>In terms of log volume, eAudit produces 10×–13× smaller logs than sysdig, and 5.8× smaller even after compression.</p><p>eAudit scales to processors with a dozen cores, and can sustain peak workloads on such machines.</p><p>The practical significance of this ability for APT detection has not been established.</p><p>The research presented in this paper identifies and analyzes critical bottlenecks in existing audit collection systems, including high performance overheads, the dropping of a large fraction of events under sustained workloads, and large windows for log tampering.</p><p>We presented several new techniques to overcome these challenges, including a compact data encoding technique that significantly cuts down data volumes; a two-level buffering scheme that minimizes contention and avoids data loss even on intense multi-core workloads; an analytical model for optimally tuning latency and throughput; and an event prioritization scheme that reduces opportunities for log tampering.</p>
    <h2>Original Abstract</h2> 
    <p>Today’s advanced cyber attack campaigns can often bypass all existing protections. The primary defense against them is after-the-fact detection, followed by a forensic analysis to understand their impact. Such an analysis requires audit logs (also called provenance logs) that faithfully capture all activities and data flows on each host. While the Linux auditing daemon (auditd) and sysdig are the most popular tools for audit data collection, a number of other systems, authored by researchers and practitioners, are also available. Through a motivating experimental study, we show that these systems impose high overheads, slowing workloads by 2×to 8×; lose a majority of events under sustained workloads; and are vulnerable to log tampering that erases log entries before they are committed to persistent storage. We present a new approach that overcomes these challenges. By relying on the extended Berkeley Packet Filter (eBPF) framework built into recent Linux versions, we avoid changes to the kernel code, and hence our data collector works out of the box on most Linux distributions. We present new design, tuning and optimization techniques that enables our system to sustain workloads that are an order of magnitude more intense than those causing major data loss with existing systems. Moreover, our system incurs only a fraction of the overhead of previous systems, while considerably reducing data volumes, and shrinking the log tampering window by∼100×.</p>
</body>
</html>
