<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>With the advancement of artificial intelligence techniques and supercomputer performance, numerical software with extensive use of floating-point (FP) arithmetic has become increasingly prevalent, accompanied by a rapid escalation in power consumption.</p><p>Unfortunately, designing compute-intensive applications that are both reliable and energy-efficient remains a significant challenge in recent years.</p><p>Although high precision guarantees program accuracy and reliability, it may also compromise efficiency and result in unnecessary energy consumption.</p><p>A trade-off between accuracy and performance is often achieved by mixed precision, i.e., performing different operations in different precisions.</p><p>Automated precision tuning is regarded as a promising direction for finding mixed-precision programs that achieve the best trade-off between performance and accuracy.</p><p>Precision tuning entails replacing the original precision assigned to FP variables in numerical programs with lower precision in a manner that ensures accuracy standards are maintained.</p><p>However, it is non-trivial to reason about mixed precision due to the higher potential for numerical errors arising from minor changes in the precision of FP variables.</p><p>Existing automated precision tuners mainly use either static analysis or dynamic search-based approaches.</p><p>Although static approaches are generally sound and do not require executing programs with input data, they are restricted to FP expressions or small programs and unable to tune large codes with conditionals and loops.</p><p>On the other hand, dynamic search-based approaches have been applied to larger-scale numerical programs but require running numerous mixed-precision program versions.</p><p>Dynamic approaches are time-intensive and face the challenge of an exponential search space of mixed-precision programs.</p><p>As far as we are aware, all search-based precision tuners suffer from scalability issues when applied to large HPC programs.</p><p>In this paper, we present FPLearner, a Machine Learning (ML) based approach to learn the representation of floating-point mixed-precision programs for predicting their performance and computation accuracy.</p><p>Our insight is straightforward: reducing the number of program runs required during the search by automatically predicting “promising” mixed-precision programs.</p><p>We propose a novel GNN-based approach to learn features from a customized graph representation, named Precision Interaction Graph (PIG), which is designed to represent mixed-precision programs by modeling interactions of precision among FP variables across the program.</p><p>To overcome the challenge, we innovatively deploy a Gated Graph Neural Network (GGNN) architecture to capture long dependencies among FP operations in such programs.</p><p>We build a dataset with 1228 mixed-precision programs from five representative HPC applications.</p><p>Our models are effective at accurately predicting both execution performance (96.34% F1 score) and computation accuracy (97.03% F1 score), outperforming other baseline methods.</p><p>The results show that our models improve the efficiency of precision tuners by an average of 25.54% and up to 61.07% in time cost while generating a mixed-precision program of comparable or better quality.</p><p>Dynamic Precision Tuning. Given a target FP program, the dynamic FP precision tuning process seeks to find a lower-precision variant of the program, often a mixed-precision program, that improves performance while adhering to specified computation accuracy constraints.</p><p>The majority of existing precision tuners rely on a search-based approach with a trial-and-fail paradigm.</p><p>Despite their potential benefits, dynamic precision tuners face significant scalability challenges.</p><p>An Example of Precision Tuning. We present a motivating example of precision tuning on LULESH version 2.0, a proxy application developed at Lawrence Livermore National Laboratory.</p><p>If we assume each mixed-precision program version of LULESH also takes around 18 seconds, then evaluating all possible mixed-precision programs would take approximately 3.76 × 10¹⁰⁷ hours.</p><p>This is a large amount of time compared to the running time of the target program, and limits the scalability of existing search-based precision tuners.</p><p>This motivates the need for predicting the performance and accuracy of mixed-precision programs.</p><p>Our goal is to train models that predict if a mixed-precision version of a given initial FP program (i) achieves performance speedup with respect to the initial program, and (ii) produces a result within a predefined error threshold.</p><p>FPLearner analyzes a mixed-precision program and extracts the necessary information to build a graph representation, named Precision Interaction Graph (PIG).</p><p>Representing programs is challenging due to the abundance of structural information contained within them, which cannot be effectively captured by conventional text-based representations.</p><p>FPLearner utilizes the Abstract Syntax Tree (AST) as the backbone of a PIG and extracts FP arithmetic related features of the nodes in the AST to obtain their initial representation.</p><p>The final PIG serves as input to the second stage of FPLearner for graph-level prediction tasks.</p><p>We create the initial node representation using three node features that are most relevant to FP characteristics: the node’s type, its precision (if applicable), and the name of the operator (if applicable).</p><p>We use word2vec to encode each feature and concatenate the three encodings together into a fixed-length vector to initialize the node representation.</p><p>FPLearner creates four additional types of edges using the graph backbone: TypeCasting, AssignedFrom, Control Flow, and Program Dependence.</p><p>FPLearner constructs the TypeCasting edges based on our observation that excessive type castings can have adverse effects on program performance.</p><p>The use of AssignedFrom edges is motivated by a prior study that leverages variable dependence in FP arithmetic assignments to model programs.</p><p>To build the PIG across a wider range of contexts, we employ two classic program analysis techniques: control flow analysis and program dependence analysis.</p><p>Data dependence (DD) edges are a type of program dependence edge that is created by calculating reaching definitions for each statement and predicate.</p><p>The second stage of FPLearner uses a GNN architecture to learn features on the input graph PIG.</p><p>We train separate models for the performance and accuracy prediction respectively.</p><p>We formulate the PIG as a multi-relational graph, which is a type of information network.</p><p>We set the training epochs as 500, and use the early stopping manner with the patience set to 30 epochs.</p><p>Our approach achieves 96.34% F1 score on the performance prediction task and 97.03% on the accuracy prediction task.</p><p>Compared to using all types of edges, excluding any one type of edge decreases the accuracy score by 5.46%–12.55% for performance prediction, and 4.69%–8.60% for accuracy prediction.</p><p>Each type of edge provides a distinct context for learning the FP precision interactions.</p><p>We evaluate our models on two dynamic precision tuners: Precimonious and HiFPTuner.</p><p>Across all case studies, FPLearner achieves a 35.14%–62.32% reduction in program runs compared to the total number of programs in the search.</p><p>When compared to Vanilla Precision Tuners, using FPLearner successfully reduces program runs by 57.34%–65.98%</p><p>Total time cost reductions are observed in all cases (17.18%–61.07%) except for MG.</p><p>Using FPLearner yields comparable or slightly superior results in terms of final program speedup.</p><p>Fine-tuning our pre-trained models on all target programs yields a substantial improvement in model performance of up to 31.9% compared to training from scratch.</p><p>FPLearner w/o FT requires 3.83× training cost on average than our fine-tuned FPLearner.</p><p>Our work is the first to utilize a DL-based approach to replace program execution with a model prediction to improve the efficiency of precision tuning.</p><p>FPLearner is the first to predict both performance and accuracy of numerical software that uses mixed precision.</p><p>We proposed a distinct graph representation, PIG, specialized for such programs by modeling precision interactions among FP variables across the program.</p><p>We presented FPLearner, a novel approach for predicting the performance and accuracy of mixed-precision programs.</p><p>FPLearner substantially improved time efficiency in two dynamic precision tuners, Precimonious and HiFPTuner, boasting an average enhancement of 25.54% and reaching up to 61.07%, all while maintaining precision tuning results of comparable or superior quality.</p>
    <h2>Original Abstract</h2> 
    <p>A mixed-precision program is a floating-point program that utilizes different precisions for different operations, providing the opportunity of balancing the trade-off between accuracy and performance. Precision tuning aims to find a mixed-precision version of a program that improves its performance while maintaining a given accuracy. Unfortunately, existing precision tuning approaches are either limited to small-scale programs, or suffer from efficiency issues. In this paper, we propose FPLearner, a novel approach that addresses these limitations. Our insight is to leverage a Machine Learning based technique, Graph Neural Networks, to learn the representation of mixed-precision programs to predict their performance and accuracy. Such prediction models can then be used to accelerate the process of dynamic precision tuning by reducing the number of program runs. We create a dataset of mixed-precision programs from five diverse HPC applications for training our models, which achieve 96.34% F1 score in performance prediction and 97.03% F1 score in accuracy prediction. FPLearner improves the time efficiency of two dynamic precision tuners, Precimonious and HiFPTuner, by an average of 25.54% and up to 61.07% while achieving precision tuning results of comparable or better quality.</p>
</body>
</html>
