<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>While the use of large-scale language models (LMs) and vision-language models (VLMs), pretrained on a massive amount of data, is becoming a dominant paradigm in machine learning.</p><p>Prompt tuning has emerged as a promising low-cost solution, discovering input prompts that effectively guide the pre-trained models to generate the desired outputs, while keeping the model parameters frozen.</p><p>Prompt tuning is generally categorized into two approaches, soft and hard prompting methods, based on their representation of prompts.</p><p>Soft prompt methods primarily focus on learning continuous embedding vectors at the token level, which are called as soft prompts.</p><p>However, the prompts learned through soft tuning are opaque to human interpretation and are not compatible with other pre-trained models that do not share the same embedding spaces.</p><p>These limitations necessitate an alternative approach: discovery of the prompts composed of human-readable discrete tokens, referred to as hard prompts.</p><p>Hard prompts offer numerous advantages over soft prompts: they are transferable from one pre-trained model to another since they are agnostic to the embedding.</p><p>Despite the benefits, they require large-scale discrete optimization in principle.</p><p>Reinforcement learning (RL) serves as a powerful alternative tool for optimization, as exemplified by RLPrompt that uses soft Q-learning.</p><p>One of the key ideas behind RLPrompt is an efficient parameterization leveraging a frozen pretrained LM.</p><p>We address this limitation in a principled manner.</p><p>Our approach leverages sparse Tsallis entropy regularization for RL to ignore very unlikely tokens from consideration.</p><p>We demonstrate the effectiveness of our algorithm across various tasks, including few-shot text classification, unsupervised text style transfer, and textual inversion from images, comparing it against various baselines.</p><p>Unlike hard prompts learned by baselines which are often referred to as the ‘secret language’ of models due to their opacity for human interpretation, our learned prompts are more natural and straightforward.</p><p>We propose a principled solution to the problem using sparse Tsallis entropy, referred to as PIN (Prompts made INterpretable).</p><p>Pioneering work by Brown et al. highlighted the efficacy of using prompts for task adaptation in pretrained language models, a technique now commonly referred to as instruction tuning.</p><p>Despite its success, the automated generation of effective text prompts, particularly hard prompts, remains a challenge.</p><p>AutoPrompt is an initial framework for discrete prompt optimization in transformer-based language models, inspiring a range of diverse methods.</p><p>RLPrompt introduces an efficiently parameterized network that maps continuous embedding vectors to adaptive vectors within the same space.</p><p>Despite its simplicity, RLPrompt struggles with accurately representing Q-values across all tokens, potentially leading to sub-optimal prompts.</p><p>Hard prompt tuning is the process of discovering an optimal prompt within the token space V, to efficiently tackle specific downstream tasks.</p><p>The reward function measures the appropriateness of the model output when prompted with z for input x.</p><p>To cope with the exponentially large action space, we can treat the optimization as a sequential decision-making process, as in RLPrompt.</p><p>RLPrompt employs SQL, an RL algorithm that incorporates entropy regularization.</p><p>An intrinsic characteristic of a softmax policy is its distribution of nonzero probability mass across all actions.</p><p>Employing sparse Tsallis entropy as regularization leads to a sparse optimal policy that concentrates probability mass on a subset set of actions.</p><p>The sparse policy assigns non-zero probabilities for only top-K actions, where smaller α makes the policy sparser.</p><p>Training the Q-network is essentially solving for an extremely overdetermined linear system, where approximation error is inevitable.</p><p>The estimated action value could become unreasonably high for low-probability tokens, promoting the RL algorithm to excessively try out these improbable tokens.</p><p>This results in an RL approach that is overly biased towards exploration, specifically favoring the selection of insignificant low-probability tokens.</p><p>We introduce the ignorable token set comprised of tokens deemed improbable from a general language model.</p><p>We empirically chose k = 10000 to ensure a sufficiently diverse set of tokens, disregarding about 80% of the vocabulary tokens.</p><p>We instead employ the sparse Tsallis entropy regularized Q-learning which yields a sparse policy that naturally suppresses the probability of choosing many unimportant tokens.</p><p>Our algorithm, PIN (Prompts made INterpretable), employs operator FI that systematically filters out the action values of ignorable tokens.</p><p>The goal of these tasks is to find the optimal prompt that assigns input text x to the class label, given a few examples in the context.</p><p>Our approach demonstrates competitive or superior performance compared to RLPrompt across all datasets.</p><p>A notable aspect of our PIN method is its efficiency: it achieves these strong results while necessitating fewer trials than required by RLPrompt.</p><p>Our algorithm relies on a weaker feedback and reward, as opposed to the direct back-propagated gradients used in Soft Prompt Tuning, AutoPrompt, and PEZ.</p><p>However, PIN underperforms relative to Soft Prompt Tuning on Yahoo dataset.</p><p>The text style transfer task aims to rephrase an input text to match a desired style.</p><p>We focus on unsupervised text style transfer task, where there are no input-output pair examples for training.</p><p>Our experiments, conducted with various sizes of task models under the same policy LM, consistently demonstrate the superiority of our algorithm.</p><p>There is no guarantee that the fluency will be improved by optimizing the reward, and it may even be possible to achieve higher rewards at the expense of fluency score.</p><p>The task of textual inversion entails identifying the caption that describe target images using VLMs, such as CLIP.</p><p>We use a range of image datasets for textual inversion, including MS COCO, LAION, and Lexica.art.</p><p>PIN is most effective in discovering high-quality prompts with fewer interactions, particularly compared to RLPrompt.</p><p>The sparse Tsallis entropy regularization employed by PIN enabled to handle a larger token search space.</p><p>PIN and PIN-no-fluency do not suffer from the aforementioned problem related to replay buffers.</p><p>The prompts learned by PIN are more interpretable to humans and more accurately capture the content of the target images.</p><p>Our empirical investigations across various tasks indicate that maintaining the number of tokens that are not to be ignored within the range of 10000 to 20000 yields the most favorable results.</p><p>PIN can discover prompts that are more interpretable compared to baselines.</p><p>The algorithm exhibits a relatively higher time consumption for training when compared to gradient-based methods.</p><p>Despite its advanced capabilities in discovering interpretable prompts, our algorithm does not guarantee the consistent discovery of grammatically perfect sentences.</p><p>We propose PIN algorithm, which uses sparse Tsallis entropy regularization to systematically exclude ignorable tokens from constraints.</p><p>Prompts learned by PIN exhibit better performance in various tasks.</p>
    <h2>Original Abstract</h2> 
    <p>Large-scale pretrained language and vision-language models have demonstrated strong adaptability to downstream tasks through prompt tuning, yet existing methods face key limitations. Soft prompt tuning learns continuous embeddings but produces opaque, non-transferable prompts, while hard prompt tuning offers interpretability and transferability but requires expensive discrete optimization. To address this challenge, we introduce PIN (Prompts made INterpretable), a reinforcement learning–based framework that incorporates sparse Tsallis entropy regularization to ignore unlikely tokens and focus on a reduced, informative action space. By filtering ignorable tokens and encouraging sparse policies, PIN mitigates exploration bias toward low-probability tokens and generates interpretable, human-readable prompts. We evaluate PIN across diverse tasks, including few-shot text classification, unsupervised text style transfer, and textual inversion with vision-language models. Experiments on datasets such as Yahoo, MS COCO, LAION, and Lexica.art show that PIN achieves competitive or superior performance compared to RLPrompt, requiring fewer interactions and producing more interpretable prompts. While computationally more demanding than gradient-based methods, PIN offers a principled and effective approach for hard prompt discovery.</p>
</body>
</html>
