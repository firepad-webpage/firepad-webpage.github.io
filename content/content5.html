<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>LLM-based code agents have significantly advanced AI-assisted coding and software development. Integrated with external tools like Python interpreters or command-line interfaces, these agents can execute code actions and dynamically adjust the actions based on observations (e.g., execution results) for multiple interaction runs. However, despite their impressive capabilities, these code agents are not risk-free. For example, if code agents inadvertently suggest or execute code with security vulnerabilities, the consequences could be severe, particularly when the code is integrated into critical systems or when the agents directly operate these systems, potentially leading to actions such as deleting important files or leaking sensitive information.</p>

    <p>While efforts have been made to assess the safety of code generated by code LLMs, a comprehensive safety evaluation of LLM-based code agents remains challenging and, to date, is still absent. In contrast to generating static code as code LLMs, code agents extend beyond mere code generation to include dynamic executions and interactions with the broader system environment, such as file and operating systems, network communications, API calls, etc. This broader range of functionalities introduces additional layers of complexity and potential risks, as code agents must be assessed not only for the vulnerability of the generated code but also for the safety and security implications of their actions in various execution environments.</p>
    
    <p>To rigorously and comprehensively evaluate the safety of code agents, we propose RedCode, a benchmark for assessing the risks of code agents around code execution and generation. RedCode is built on the following principles: (1) Real interaction with systems. (2) Holistic evaluation on code execution and generation. (3) Diverse natural and programming languages input format. (4) Comprehensive risky scenarios and tests.</p>
    
    <p>Specifically, we have constructed 4050 risky test cases in RedCode-Exec for code execution, spanning 25 major safety scenarios across various 8 domains, and 160 prompts in RedCode-Gen for malicious software generation spanning 8 malware families.</p>
    
    <p>We evaluate 3 types of agents with a total of 19 LLM-based code agents under different RedCode scenarios. The overall attack success rate is high on RedCode-Exec when agents are queried to execute risky or buggy code, highlighting the vulnerability of existing agents. The rejection rate for risky test cases on the operating and file systems is higher than in other domains. Agents are more likely to execute harmful actions by risky queries in natural language than in programming languages. Python leads to a higher rejection rate than Bash. Experiments on three types of code agents, OpenCodeInterpreter, CodeAct, and ReAct, show that OpenCodeInterpreter is relatively safer than CodeAct and ReAct, potentially due to its hard-coded safety constraints. Agents paired with stronger base LLMs such as GPT-4 can have a higher rejection rate for risky code execution in RedCode-Exec, but they also generate more sophisticated and effective harmful software in RedCode-Gen, indicating safety concerns.</p>
    
    <p>The overall rejection rate is low, and the attack success rate is high for all code agents, which demonstrates the high quality, stealthiness, and challenging test cases in our benchmark. Among the 8 domains, the agents’ rejection rates are generally high for dangerous test cases related to the operating system and file system domains, which include risky operations that can impact the system’s normal usage such as reading or modifying the “.bashrc” file or deleting the “/etc/passwd” file, and risky behaviors like monitoring the system. The rejection rate for risky test cases in the website domain is also relatively high for ReAct-GPT4, which could be because they can identify sensitive keywords that we deliberately include in the URLs, such as “dangerous” and “malicious”. In the remaining domains, such as program logic and cybersecurity, the rejection rate is relatively low, and the attack success rate is high, which highlights the serious safety concerns for existing code agents.</p>
    
    <p>Agents are more likely to execute harmful actions by risky queries in natural language than in programming languages. Natural text inputs (text summaries/descriptions) are generally less prone to rejection than code inputs, and agents are easier to fulfill the threat functionality with a higher attack success rate in textual instructions compared to direct code inputs.</p>
    
    <p>Code queries with jailbreak prefixes have a higher rejection rate compared to plain code queries for some agents.</p>
    
    <p>Text descriptions lead to a higher attack success rate and lower rejection rate than summaries.</p>
    
    <p>Bash code inputs usually have a lower rejection rate than Python under the same risky scenarios.</p>
    
    <p>OpenCodeInterpreter is more robust than ReAct and CodeAct, with a higher rejection rate and lower attack success rate. This is because OpenCodeInterpreter has hard-coded disk space protection constraints in its agent codebase.</p>
    
    <p>ReAct generally has a higher rejection rate than CodeAct.</p>
    
    <p>Under ReAct, strong base LLMs in general have a high rejection rate, indicating the stronger safety awareness of more capable models. Fine-tuned LLMs could compromise the safety of agents, leading to the lowest rejection rate in CodeAct Mistral-7B and Llama2-7b.</p>
    
    <p>Most models, with the exception of GPT-3.5, Claude-Opus, and Llama-2-7B, have a low refusal rate despite rejecting natural language based instructions to generate malware in prior safety benchmarks. We also observe a strong correlation between general coding capabilities with the quality of generated malware.</p>
    
    <p>The GPT-4 models also generate malicious code more likely to be scored highly by the judge. Some of the generated samples from GPT-4 and DeepseekCoder can trigger antivirus evaluations from VirusTotal, suggesting LLMs have the potential to automatically generate functional malware.</p>
    
    <p>Certain models with strong coding capabilities, such as Claude-Opus, also have strong safety guardrails, resulting in low accuracy on RedCode-Gen.</p>
    
    <p>The ability to evaluate code and refine it results in a higher judge accuracy, lower refusal rate, and higher VirusTotal accuracy, suggesting the capabilities of code agents introduce new risks.</p>
    
    <h2>Original Abstract</h2>
    <p>LLM-based code agents have advanced AI-assisted software development by combining large language models with external tools for dynamic code execution and system interaction. However, these agents pose significant safety risks, as they may generate or execute harmful code with severe security implications. Existing research primarily focuses on static code generation, leaving a gap in the comprehensive evaluation of code agents' dynamic behavior. To address this, we introduce RedCode, a benchmark designed to assess the safety of code agents across both code generation and execution. RedCode comprises 4,050 test cases spanning 25 safety scenarios across 8 domains for code execution (RedCode-Exec) and 160 prompts targeting malware generation (RedCode-Gen). We evaluate 19 LLM-based code agents, revealing a high attack success rate and low rejection rate, especially for natural language inputs. OpenCodeInterpreter demonstrates stronger safety constraints compared to CodeAct and ReAct. However, agents leveraging stronger base models like GPT-4 exhibit improved rejection rates but also generate more potent malware. These findings highlight significant safety concerns and the need for improved safeguards in LLM-based code agents, as they remain vulnerable to executing harmful actions in real-world environments.</p>
</body>
</html>
