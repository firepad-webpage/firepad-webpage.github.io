<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>Managed data transfer is an enabler, an unsung hero of large-scale, globally-distributed systems.</p><p>The goal of data transfer management systems is to transfer when it is optimal to do so — in contrast to a last-minute transfer at the moment data needs to be consumed.</p><p>In this paper we describe Effingo, a data transfer management system we designed and built at Google.</p><p>Effingo has requirements and features uncommon in reported large-scale data transfer systems.</p><p>Effingo optimizes for smooth bandwidth usage and thus network cost.</p><p>At the same time, Effingo can operate in a deadline-aware, yet cost-efficient way.</p><p>Effingo operates on a higher layer, providing SLOs on a transfer of multiple files, including to/from multiple destinations/sources.</p><p>Effingo uses decentralized heuristics that scale well while maintaining cluster-level failure domains.</p><p>Effingo is used in production for years and widely adopted at Google, transferring daily over an exabyte in billions of files.</p><p>This scale is orders of magnitude larger than anything previously reported.</p><p>This magnitude of deployment enables economies of scale: development of specific, niche optimizations.</p><p>Moreover, operating at this scale makes many reliability investments justified to keep pace with the growth of adoption and meet the service SLO.</p><p>Effingo continuously (1) optimizes the copy tree to reduce the usage of expensive WAN links (e.g., subsea cables); (2) adjusts the transfer speed to react to changing effective bandwidth while avoiding spikes in usage according to the user latency hint; (3) maintains fairness between competing, same-priority transfers; (4) alerts about bottlenecks caused by insufficient quotas on various resources (WAN bandwidth, disk IOPS and compute).</p><p>We identify the throughput-oriented data transfer management problem in the context of a large-scale, heterogeneous organization dealing with many multi-petabyte transfers over a globally-distributed, multi-layer infrastructure.</p><p>We describe how to efficiently and at scale respond to these requirements.</p><p>Effingo is a standard service providing fairness, detecting bottlenecks, optimizing the copy tree and reducing bandwidth bursts.</p><p>We measure a production system operating at a scale of orders of magnitude larger than previously reported.</p><p>Effingo schedules transfers over space and time to minimize cost while ensuring fairness (between other Effingo transfers).</p><p>Effingo uses the store-and-forward approach originally proposed by NetStitcher, but limits it only to the locations permitted by the user, and integrates it with throughput- and fairness isolation.</p><p>Effingo parallelizes the file transfers as much as possible, given other constraints.</p><p>Effingo is optimized for a highly scalable, cluster-level filesystem Colossus.</p><p>Effingo is used by human users, single-tenant services, and multi-tenant services.</p><p>Effingo quota is measured in ingress and egress bandwidth from a particular cluster.</p><p>Effingo was designed with the following requirements: Client isolation, Isolated failure domains, Data residency, Data integrity, Resiliency, Cost/Performance tradeoff.</p><p>To isolate failure domains, Effingo stack is deployed in each cluster.</p><p>Effingo stores all transfers over a 14-day window in the copy tracker (CT) database.</p><p>For each set of files being copied from the same OSF, Effingo builds a Minimum Spanning Tree (MST) over the graph of planned transfers for this file, using values from the cost matrix as edge weights.</p><p>Effingo allows users to smooth the throughput by specifying an (optional) discrete latency hint: the maximum expected copy time.</p><p>Effingo adaptively controls the throughput of each copy operation.</p><p>Effingo uses a layered system, consisting of quotas, queues and priorities to fulfill these requirements.</p><p>Effingo’s enhanced debugging is a significant reason, besides efficiency, that teams switch to Effingo from their own solutions.</p><p>Effingo is a widely-deployed, throughput-oriented file copy system, integrated with resource management and authorization systems.</p><p>Two key design decisions are (1) per-cluster deployments, limiting failure domains to individual clusters; (2) separation from BwE, the bandwidth management layer, resulting in a modular design that reduces dependencies.</p><p>The lack of a central coordinator does not impede achieving efficiency and fairness.</p>
    <h2>Original Abstract</h2> 
    <p>Efficient data transfer is critical to the operation of globally distributed systems. This paper presents Effingo, a large-scale data transfer management system developed and deployed at Google. Unlike traditional systems, Effingo emphasizes proactive transfers that optimize for smooth bandwidth usage and network cost while meeting transfer deadlines. Effingo supports service-level objectives (SLOs) over collections of files and destinations, leveraging decentralized heuristics that scale without central coordination. It employs adaptive techniques to optimize copy paths, manage fairness across competing transfers, and respond to resource bottlenecks. Operating at a scale of over an exabyte per day across billions of files, Effingo enables niche optimizations and justified reliability investments. Its architecture—featuring per-cluster deployments, layered quotas, and failure domain isolation—supports a wide variety of users and services. The system’s design integrates cost-aware scheduling, throughput control, and fairness isolation, while maintaining modularity from lower-level bandwidth managers. Through extensive production deployment, Effingo demonstrates how large-scale, deadline-aware, and cost-efficient data transfers can be managed with minimal dependencies and high resiliency.</p>
</body>
</html>
