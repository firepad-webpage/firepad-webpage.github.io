<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>Depth is crucial for 3D perception in various downstream applications, such as autonomous driving, augmented and virtual reality, and robotics.</p><p>However, sensor-based depth measurement is far from perfect.</p><p>Such measurements often exhibit sparsity, low resolution, noise interference, and incompleteness.</p><p>Various factors, including environmental conditions, motion, sensor power constraints, and the presence of specular, transparent, wet, or non-reflective surfaces, contribute to these limitations.</p><p>Consequently, the task of depth completion, aimed at generating dense and accurate depth maps from sparse measurements alongside aligned camera images, has emerged as a pivotal research area.</p><p>Thanks to the advances in deep learning, there has been significant progress in depth completion.</p><p>Earlier papers leverage convolutional neural networks to perform depth completion with image guidance and achieve promising results.</p><p>In order to improve accuracy, researchers have studied various spatial propagation methods, which performs further iterative processing on top of depth maps and features computed by an initial network.</p><p>These propagation algorithms, however, focus on 2D feature processing and do not fully exploit the 3D nature of the problem.</p><p>A few recent papers utilize transformers for depth completion.</p><p>However, they apply transformer operations mainly to improve feature learning on the 2D image plane and fail to achieve acceptable accuracy without employing spatial propagation.</p><p>Several studies have looked into harnessing 3D representation more comprehensively.</p><p>For instance, construct a point cloud from the input sparse depth, yet coping with extreme sparsity poses challenges in effective feature learning.</p><p>Another approach, as seen in, uplifts 2D features to 3D by using the initial dense depth predicted by a simple convolutional network, but it is impeded by the poor accuracy of the initial network and requires dynamic propagations to attain acceptable accuracy.</p><p>Very recently, researchers have proposed employing transformers for 3D feature learning in depth completion; however, this work applies transformer layers to extremely sparse points, which is ineffective for learning informative 3D features.</p><p>Here, we introduce DeCoTR to perform feature learning in full 3D.</p><p>It accomplishes this by constructing a dense feature point cloud derived from completed depth values obtained from an initial network and subsequently applying transformer processing to these 3D points.</p><p>To do this properly, it is essential to have reasonably accurate initial depths.</p><p>As such, we first enhance a commonly used convolution-based initial depth network, S2D, by integrating transformer layers on bottleneck and skip connection features.</p><p>This upgraded model, termed S2D-TR, achieves significantly improved depth accuracy, on par with state-of-the-art models, without requiring any iterative spatial propagation.</p><p>Given the initial depth map, we uplift 2D features to 3D to form a point cloud, which is subsequently processed by transformer layers, to which we refer as 3D-TR layers.</p><p>Prior to feeding the points to transformer layers, we normalize them, which regularizes the 3D feature learning and leads to better accuracy.</p><p>To facilitate long-range contextual understanding, we additionally incorporate global attention on lower-scale versions of the point cloud.</p><p>Finally, 3D features are projected back to the 2D image plane and consumed by a decoder to produce the final depth prediction.</p><p>We present DeCoTR, a novel transformer-based approach to perform full 3D feature learning for depth completion.</p><p>In order to properly do this, we upgrade the commonly used initial network S2D, by enhancing its bottleneck and skip connection features using transformers.</p><p>We devise useful techniques to normalize the uplifted 3D feature point cloud, which improves the model learning.</p><p>Through extensive evaluations on standard benchmarks, NYU Depth v2 and KITTI, we demonstrate the efficacy of DeCoTR and show that it sets the new SOTA.</p><p>Early depth completion approaches rely solely on the sparse depth measurements to estimate the dense depth.</p><p>Since these methods do not utilize the image, they usually suffer from artifacts like blurriness, especially at object boundaries.</p><p>Later, image-guided depth completion alleviates these issues by incorporating the image.</p><p>S2D, one of the first papers on this, leverages a convolutional network to consume both the image and sparse depth map.</p><p>Subsequent papers design more sophisticated convolutional models for depth completion.</p><p>In order to enhance depth quality, researchers have studied various spatial propagation algorithms.</p><p>These solutions utilize depth values and features given by an initial network, and performs iterative steps to mix and aggregate features on the 2D image plane.</p><p>While existing solutions predominately propose architectures to process features on 2D, several works explore 3D representations.</p><p>One of these works, GraphCSPN, employs S2D as an initial network to generate the full depth map, before creating a denser point cloud and performing feature learning on it.</p><p>However, this is limited by the insufficient accuracy of the initial depths by S2D and still needs iterative processing to achieve good accuracy.</p><p>More related to our paper are those that leverage vision transformers for depth completion, such as CompletionFormer and GuideFormer.</p><p>While they demonstrate the effectiveness of using vision transformers for depth completion, their feature learning is only performed on the 2D image plane.</p><p>A very recent paper, PointDC, proposes to apply transformer to 3D point cloud in the depth completion pipeline.</p><p>However, PointDC operates on very sparse points, which makes it challenging for learning 3D features.</p><p>Given aligned sparse depth map and an RGB image, the goal of image-guided depth completion is to recover a dense depth map based on the sparse input and with semantic guidance from the image.</p><p>It is a common approach to employ early fusion between the depth and RGB modalities.</p><p>The early-fusion architecture of S2D has been commonly used by researchers as a base network to predict an initial completed depth map.</p><p>This architecture, however, has limited accuracy and may provide erroneous depth values for subsequent operations.</p><p>As such, we propose to leverage self-attentions to enhance the S2D features.</p><p>Our upgraded version of S2D with efficient attention enhancement, denoted as S2D-TR, provides significantly improved accuracy while having better efficiency than latest transformer-based depth completion models.</p><p>Considering the 3D nature of depth completion, it is important for the model to properly exploit 3D geometric information when processing the features.</p><p>Given the large number of 3D points uplifted from the 2D feature map, it is computationally intractable to perform attention on all the points simultaneously.</p><p>As such, we adopt a neighborhood-based attention, by finding the K-Nearest-Neighboring points for each point in the point cloud.</p><p>We perform such 3D cross-attention in multiple transformer layers, to which we refer as 3D-TR layers.</p><p>Point cloud normalization: We normalize the constructed point cloud from S2D-TR outputs into a unit ball, before proceeding to the 3D attention layers.</p><p>We find this technique effectively improves depth completion, as we shall show in the experiments.</p><p>To enable global understanding while keeping computation costs under control, we propose to perform global 3D cross-attention only on a downsampled point set.</p><p>We apply global attention after the local neighborhood-based attentions.</p><p>We train DeCoTR with a masked â„“1 loss between the final completed depth maps and the ground-truth depth maps.</p><p>We conduct extensive experiments to evaluate our proposed DeCoTR on standard depth completion benchmarks and compare with the latest state-of-the-art solutions.</p><p>We further perform zero-shot evaluation to assess the model generalizability and carry out ablation studies to analyze different parts of our proposed approach.</p><p>Our proposed DeCoTR approach sets the new SOTA performance, with the lowest RMSE of 0.086 outperforming all existing solutions.</p><p>Specifically, our DeCoTR considerably outperforms latest SOTA methods that also leverage 3D representation and/or transformers, such as GraphCSPN, PointDC, and CompletionFormer.</p><p>DeCoTR achieves SOTA depth completion accuracy and is among the top-ranking methods on KITTI-DC leaderboard.</p><p>This indicates that DeCoTR has the right combination of dense 3D representation and transformer-based learning.</p><p>We see that DeCoTR generalizes better to unseen datasets when comparing to existing SOTA models.</p><p>DeCoTR has significantly lower depth errors as compared to both NLSPN and CompletionFormer.</p><p>This study verifies the usefulness of our proposed components and techniques.</p><p>This confirms the importance of getting more accurate initial depth before applying 3D feature learning.</p><p>Through extensive experiments, we have shown that DeCoTR achieves SOTA performance on standard benchmarks like NYUD-v2 and KITTI-DC.</p><p>Furthermore, zero-shot evaluation on unseen datasets such as ScanNet and DDAD shows that DeCoTR has better generalizability as compared to existing methods.</p>
    <h2>Original Abstract</h2> 
    <p>In this paper, we introduce a novel approach that harnesses both 2D and 3D attentions to enable highly accurate depth completion without requiring iterative spatial propagations. Specifically, we first enhance a baseline convolutional depth completion model by applying attention to 2D features in the bottleneck and skip connections. This effectively improves the performance of this simple network and sets it on par with the latest, complex transformer-based models. Leveraging the initial depths and features from this network, we uplift the 2D features to form a 3D point cloud and construct a 3D point transformer to process it, allowing the model to explicitly learn and exploit 3D geometric features. In addition, we propose normalization techniques to process the point cloud, which improves learning and leads to better accuracy than directly using point transformers off the shelf. Furthermore, we incorporate global attention on downsampled point cloud features, which enables long-range context while still being computationally feasible. We evaluate our method, DeCoTR, on established depth completion benchmarks, including NYU Depth V2 and KITTI, showcasing that it sets new state-of-the-art performance. We further conduct zero-shot evaluations on ScanNet and DDAD benchmarks and demonstrate that DeCoTR has superior generalizability compared to existing approaches.</p>
</body>
</html>
