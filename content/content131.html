<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>Self-supervised learning is vulnerable to backdoor attacks.</p><p>We identify key properties of existing attacks, where the poisoned data is out-of-distribution and highly clustered in a small region.</p><p>We propose a new attack, DRUPE, which significantly alleviates these problems and evades existing defenses.</p><p>The poisoned samples are all tightly clustered within a very small region in comparison to the clean data.</p><p>This results in a high degree of pairwise similarity among the poisoned samples, providing an opportunity to defenders.</p><p>DECREE reverse-engineers a trigger for a given encoder, by minimizing the pairwise similarity of a set of samples (when the trigger is inserted).</p><p>The encoder is considered backdoored if a small trigger can be found.</p><p>This paper reveals a key insight that the success of backdoor attacks in self-supervised learning is not necessarily dependent on the out-of-distribution property.</p><p>We propose to transform poisoned samples into in-distribution data (w.r.t. the clean data) such that the backdoor information is not detectable in the feature space.</p><p>To achieve this, we estimate the distribution of the clean data leveraging Kernel Density Estimation (KDE).</p><p>We then move the poisoned samples closer to the clean distribution by reducing the difference between the two distributions, which is estimated by the sliced-Wasserstein distance, a metric for measuring distributional differences in a high-dimensional space.</p><p>In addition, we reduce the concentration of the poisoned distribution by distributing the poisoned samples to a wider region, making it similar to that of the target-class distribution in the downstream task.</p><p>We implement an attack prototype called DRUPE (DistRibUtion Preserving backdoor attack in sElf-supervised learning).</p><p>Compared to existing backdoor attacks, DRUPE achieves 10 times smaller distributional difference between the poisoned and the clean data, and 3 times smaller pairwise similarity among poisoned samples, while obtaining comparable benign accuracy and attack success rate on downstream tasks.</p><p>In addition, DRUPE successfully evades two state-of-the-art backdoor defenses in self-supervised learning.</p><p>We observe that, in Section 4, the embeddings of poisoned samples have a few large values, compared to those of clean inputs.</p><p>One can address this by enforcing the L2 norm of poisoned embeddings to be similar to that of clean embeddings.</p><p>DRUPE first initializes the backdoored encoder weights from a clean encoder.</p><p>It also constructs the poisoned set and the expanded reference set.</p><p>At each training epoch, the distributional difference between poisoned samples and clean data is measured.</p><p>DRUPE also considers the tightness of the clean distribution to avoid the negative impact on clean data.</p><p>In order to reduce the concentration of the poisoned distribution, DRUPE maximizes the similarity of expanded reference inputs on backdoored and clean encoders in the embedding space.</p><p>It also maximizes the distance among reference inputs and among poisoned samples.</p><p>The poisoned samples are pulled close to their nearest reference inputs to satisfy the attack goal.</p><p>Finally, DRUPE keeps the normal functionality of the backdoored encoder by comparing its produced embeddings on clean data to those from a clean encoder.</p><p>We evaluate the attack performance of DRUPE on five datasets and compare to existing attacks in Section 6.2.</p><p>We leverage two existing defenses Beatrix and DECREE to evaluate different attacks.</p><p>Beatrix detects poisoned samples based on the abnormality in the feature space.</p><p>Beatrix fails to detect poisoned samples by WB attack, AdvEmbed, and DRUPE.</p><p>The detection accuracy is near 50% on both CIFAR-10 and STL-10 (close to random guessing).</p><p>The results demonstrate that DRUPE effectively blends the poisoned distribution in the clean distribution, resulting in indistinguishable poisoned samples in the feature space.</p><p>DECREE can successfully detect backdoored encoders by BadEncoder, WB attack, Carlini et al., and AdvEmbed with 100.0% accuracy.</p><p>However, it fails to detect DRUPEâ€™s backdoored encoders with only 50% accuracy (which is the same as random guessing).</p><p>This is because DRUPE approximates the target-class distribution and distributes the poisoned samples across the entire target-class space, reducing the concentration of the poisoned distribution that DECREE is based on.</p><p>It is challenging to defend against DRUPE.</p>
    <h2>Original Abstract</h2> 
    <p>Self-supervised learning is widely used in various domains for building foundation models. It has been demonstrated to achieve state-of-the-art performance in a range of tasks. In the computer vision domain, self-supervised learning is utilized to generate an image feature extractor, called an encoder, such that a variety of downstream tasks can build classifiers on top of it with limited data and resources. Despite the impressive performance of self-supervised learning, it is susceptible to backdoor attacks, where an attacker injects a backdoor into its unlabeled training data. A downstream classifier built on the backdoored encoder will misclassify any inputs inserted with the trigger to a target label. Existing backdoor attacks in self-supervised learning possess a key out-of-distribution property, where the poisoned samples significantly differ from the clean data in the feature space. The poisoned distribution is also exceptionally concentrated, inducing high pairwise similarity among poisoned samples. As a result, these attacks can be detected by state-of-the-art defense techniques. We propose a novel distribution preserving attack, which transforms the poisoned samples into in-distribution data by reducing their distributional distance to the clean data. We also distribute the poisoned data to a wider region in the target-class distribution, mitigating the concentration problem. Our evaluation of five popular datasets demonstrates that our attack, DRUPE, significantly reduces the distributional distance and concentration of the poisoned distribution compared to existing attacks. DRUPE successfully evades two state-of-the-art backdoor defenses in self-supervised learning and is robust against knowledgeable defenders.</p>
</body>
</html>
