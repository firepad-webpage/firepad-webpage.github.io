<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>Self-supervised learning is vulnerable to backdoor attacks.</p><p>We identify key properties of existing attacks, where the poisoned data is out-of-distribution and highly clustered in a small region.</p><p>We propose a new attack, DRUPE, which significantly alleviates these problems and evades existing defenses.</p><p>The poisoned samples are all tightly clustered within a very small region in comparison to the clean data.</p><p>This results in a high degree of pairwise similarity among the poisoned samples, providing an opportunity to defenders.</p><p>DECREE reverse-engineers a trigger for a given encoder, by minimizing the pairwise similarity of a set of samples (when the trigger is inserted).</p><p>The encoder is considered backdoored if a small trigger can be found.</p><p>This paper reveals a key insight that the success of backdoor attacks in self-supervised learning is not necessarily dependent on the out-of-distribution property.</p><p>We propose to transform poisoned samples into in-distribution data (w.r.t. the clean data) such that the backdoor information is not detectable in the feature space.</p><p>To achieve this, we estimate the distribution of the clean data leveraging Kernel Density Estimation (KDE).</p><p>We then move the poisoned samples closer to the clean distribution by reducing the difference between the two distributions, which is estimated by the sliced-Wasserstein distance, a metric for measuring distributional differences in a high-dimensional space.</p><p>In addition, we reduce the concentration of the poisoned distribution by distributing the poisoned samples to a wider region, making it similar to that of the target-class distribution in the downstream task.</p><p>We implement an attack prototype called DRUPE (DistRibUtion Preserving backdoor attack in sElf-supervised learning).</p><p>Compared to existing backdoor attacks, DRUPE achieves 10 times smaller distributional difference between the poisoned and the clean data, and 3 times smaller pairwise similarity among poisoned samples, while obtaining comparable benign accuracy and attack success rate on downstream tasks.</p><p>In addition, DRUPE successfully evades two state-of-the-art backdoor defenses in self-supervised learning.</p><p>We observe that, in Section 4, the embeddings of poisoned samples have a few large values, compared to those of clean inputs.</p><p>One can address this by enforcing the L2 norm of poisoned embeddings to be similar to that of clean embeddings.</p><p>DRUPE first initializes the backdoored encoder weights from a clean encoder.</p><p>It also constructs the poisoned set and the expanded reference set.</p><p>At each training epoch, the distributional difference between poisoned samples and clean data is measured.</p><p>DRUPE also considers the tightness of the clean distribution to avoid the negative impact on clean data.</p><p>In order to reduce the concentration of the poisoned distribution, DRUPE maximizes the similarity of expanded reference inputs on backdoored and clean encoders in the embedding space.</p><p>It also maximizes the distance among reference inputs and among poisoned samples.</p><p>The poisoned samples are pulled close to their nearest reference inputs to satisfy the attack goal.</p><p>Finally, DRUPE keeps the normal functionality of the backdoored encoder by comparing its produced embeddings on clean data to those from a clean encoder.</p><p>We evaluate the attack performance of DRUPE on five datasets and compare to existing attacks in Section 6.2.</p><p>We leverage two existing defenses Beatrix and DECREE to evaluate different attacks.</p><p>Beatrix detects poisoned samples based on the abnormality in the feature space.</p><p>Beatrix fails to detect poisoned samples by WB attack, AdvEmbed, and DRUPE.</p><p>The detection accuracy is near 50% on both CIFAR-10 and STL-10 (close to random guessing).</p><p>The results demonstrate that DRUPE effectively blends the poisoned distribution in the clean distribution, resulting in indistinguishable poisoned samples in the feature space.</p><p>DECREE can successfully detect backdoored encoders by BadEncoder, WB attack, Carlini et al., and AdvEmbed with 100.0% accuracy.</p><p>However, it fails to detect DRUPE’s backdoored encoders with only 50% accuracy (which is the same as random guessing).</p><p>This is because DRUPE approximates the target-class distribution and distributes the poisoned samples across the entire target-class space, reducing the concentration of the poisoned distribution that DECREE is based on.</p><p>It is challenging to defend against DRUPE.</p>
    <h2>Original Abstract</h2> 
    <p>Self-supervised learning (SSL) has shown susceptibility to backdoor attacks, where adversarially crafted poisoned samples compromise downstream model behavior. Existing attacks often rely on out-of-distribution, highly clustered poisoned data, which can be detected via their abnormal concentration in feature space. In this work, we introduce DRUPE (DistRibUtion Preserving backdoor attack in sElf-supervised learning), a novel attack that transforms poisoned samples to appear in-distribution and reduces their pairwise similarity, making detection significantly harder. DRUPE estimates the clean data distribution using Kernel Density Estimation and minimizes the distributional gap via sliced-Wasserstein distance. It also disperses poisoned data across the feature space to mimic the target-class distribution. This design evades detection by state-of-the-art defenses such as Beatrix and DECREE, which rely on concentration or embedding similarity. Empirically, DRUPE achieves a 10× smaller distributional difference and 3× lower sample similarity than prior attacks, while maintaining high benign and attack performance across five datasets. Our results reveal that distributional blending is an effective and stealthy strategy for backdoor injection in SSL, highlighting new challenges for developing robust defenses.</p>
</body>
</html>
