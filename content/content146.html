<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excerpt</title>
</head>
<body>
    <h2>Excerpt</h2>
    <p>Recently, we have witnessed the explosive development of generative large language models such as GPT series and LLaMA.</p><p>Undergone extensive pretraining on document corpora and instruction tuning, these language models have demonstrated an impressive ability to memorize a lot of knowledge in their parameters and effectively recall them to answer users’ instructions and queries.</p><p>Building upon the advancements of LLMs, multimodal LLMs (MLLMs) have been developed to expand the capabilities beyond text and allow users to express their needs using visual input.</p><p>Despite the impressive capabilities of LLMs and MLLMs, their responses are limited to textual outputs.</p><p>It would greatly enhance the response capabilities of MLLMs if they could give visual outputs, like a photograph in this case.</p><p>A straightforward solution is to enhance MLLMs with external image synthesis tools, like diffusion models and Generative Adversarial Networks, for visual output capabilities.</p><p>However, a significant challenge with these modules is their propensity to produce unrealistic or hallucinatory images, which cannot accurately describe real-world images.</p><p>The integration of an image retrieval module seems a more viable solution.</p><p>A bold and innovative idea emerges: Is it possible to equip MLLMs with the ability to memorize visual information within their parameters for retrieval and beyond?</p><p>In this light, we formulate a generative cross-modal retrieval task: given a user query for visual content, MLLMs are expected to recall desired images from their parameters directly as the response.</p><p>Accomplishing this task poses a significant challenge, necessitating the presence of two essential abilities of MLLMs: 1) Visual memory and 2) Visual recall.</p><p>In this work, we propose a novel GeneRAtive Cross-modal rEtrieval framework, GRACE, to overcome the above issues.</p><p>GRACE assigns images unique identifiers, where each identifier is a distinct string representing an image.</p><p>GRACE comprises two training steps: 1) Learning to memorize and 2) Learning to retrieve.</p><p>GRACE enables generative cross-modal retrieval: given a textual query, the MLLM generates an identifier string corresponding to a real image.</p><p>We evaluate GRACE on text-image matching datasets to verify the feasibility of generative cross-modal retrieval.</p><p>GRACE performs comparably to the advance one-tower approaches and demonstrates higher efficiency with large-scale image sizes.</p><p>GRACE transforms the original matching problem into a generation problem, eliminating the need for negative samples during training and retrieval index during inference.</p><p>Inbuilt visual memory serves for retrieval, yet its utility extends beyond mere retrieval.</p><p>The MLLM could describe the memorized image and even answer questions about the memorized images.</p><p>This opens up the possibility of injecting personalized visual experiences of humans into MLLMs.</p><p>The current cross-modal retrieval approaches can be categorized into the two frameworks and the one-tower framework.</p><p>Both frameworks formulate the cross-modal retrieval as a discriminative problem, which relies on discriminative loss and negative samples to learn an embedding space.</p><p>Generative retrieval is an emerging new retrieval paradigm in text retrieval, which generates identifier strings of passages as the retrieval target.</p><p>Generative retrieval gains a lot of attention in text retrieval, as it could take advantage of the powerful generative language models.</p><p>However, how to facilitate cross-modal retrieval in a generative way is still an untapped problem.</p><p>Despite the success of MLLMs in various vision-language tasks, they currently lack the ability to unify cross-modal retrieval into their application.</p><p>Generative cross-modal retrieval defines new requirements, i.e., removing visual input during inference.</p><p>Text-to-image retrieval aims to retrieve relevant images from a database when given a textual query.</p><p>Considering convenience and model sizes, we have chosen Flamingo as the backbone for our method.</p><p>Flamingo consists of three main components: a generative language model, a visual encoder, and cross-attention layers.</p><p>The generative language model receives text input that includes a special token, “&lt;image&gt;”, which indicates the presence of an image.</p><p>GRACE assigns unique identifiers to images in the dataset.</p><p>The model could generate identifiers as retrieval results rather than generate real images.</p><p>The two training steps are designed to enable the model to effectively memorize images in parameters and subsequently learn to recall them in response to textual queries.</p><p>String, numeric, and atomic identifiers do not provide any prior knowledge about the image content, whereas semantic and structured identifiers do.</p><p>Structured identifiers achieved good performance by effectively utilizing the image’s embedding information through a clustering approach.</p><p>Atomic identifiers were found to be the most effective, even outperforming the CLIP model.</p><p>This approach assigns a unique token in the vocabulary for each image, ensuring distinct identification.</p><p>For an image, we train the model to associate this image with its corresponding identifier.</p><p>This learning to memorize step allows the model to learn the mappings from visual inputs to their corresponding identifiers.</p><p>The model must be capable of recalling the corresponding images in response to users’ queries.</p><p>We train the MLLM to predict the appropriate identifier when given a specific query.</p><p>Post-training, the MLLM model could retrieve images akin to text generation.</p><p>We implement constrained beam search in the MLLM.</p><p>This mechanism ensures that every generated identifier accurately matches an existing image’s identifier.</p><p>GRACE demonstrated the capability to recall relevant images in response to textual queries without input of image content.</p><p>There was variability in performance among GRACE with different identifiers.</p><p>Structured identifiers achieved good performance by effectively utilizing the image’s embedding information.</p><p>Atomic identifiers were the most effective.</p><p>However, increasing the number of images directly enlarges the vocabulary size of the MLLM, potentially impacting scalability.</p><p>During the inference stage, we employed constrained generation to ensure the prediction of valid identifiers.</p><p>Removing constrained generation caused significant decline in performance.</p><p>Without constrained generation, the model tends to predict identifiers that do not correspond to any image in the corpus.</p><p>CLIP’s inference speed decreases as image size increases, owing to the escalating number of similarity calculations required.</p><p>The inference speed of our generative framework remains nearly constant.</p><p>When image sizes exceed a certain threshold, our generative framework surpasses CLIP in terms of inference speed.</p><p>The MLLM has successfully memorized certain images, it is capable of providing a description of the image’s content when prompted.</p><p>The model is capable of answering some questions over the memorized images.</p><p>This opens up the possibility of injecting personalized visual experiences of humans into MLLMs for them to understand an individual’s visual journey.</p><p>In this paper, we delved into a novel memorization mechanism for the MLLM to memorize images within its parameters.</p><p>We proposed a generative cross-modal retrieval framework, which introduces a fresh paradigm in cross-modal retrieval.</p><p>This paradigm transforms the original matching problem into a generation problem.</p><p>Our experiments demonstrate that the generative paradigm performs effectively and efficiently even with large-scale image sizes.</p><p>We showcased the MLLM’s ability to interact (e.g., describe and QA) with memorized images.</p><p>Exploring more effective identifiers, like “visual tokens”, would help to enhance generative cross-modal retrieval further.</p>
    <h2>Original Abstract</h2> 
    <p>The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to “recall” the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter step teaches the MLLM to generate the corresponding identifier of the target image, given the textual query input. By memorizing images in MLLMs, we introduce a new paradigm to cross-modal retrieval, distinct from previous discriminative approaches. The experiments demonstrate that the generative paradigm performs effectively and efficiently even with large-scale image candidate sets.</p>
</body>
</html>
