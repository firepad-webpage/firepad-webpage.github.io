[
    {
        "index": "40",
        "excerpt": "<p>RDMA NICs desire an accurate, scalable, and fast rate limiter.</p><p>Specifically, RNICs expect the rate limiter to be accurate and scalable to precisely execute policies such as congestion control and traffic isolation, enforcing a specific rate on each flow and injecting inter-packet gaps for smoother traffic, across tens of thousands of flows at end hosts.</p><p>In addition, RNICs require the rate limiter to sustain high packet rate to ensure its integration into the data path does not degrade the throughput of RNICs.</p><p>Prior works SENIC and PIEO can achieve accuracy and scalability, but they are not fast enough.</p><p>The root cause of their performance issue is that they follow a monolithic design, directly applying rate limiting to the flows to be scheduled, and consequently transmit only one packet after sorting all flows.</p><p>Sorting a large number of flows is slow.</p><p>We present Tassel, a hierarchical rate limiter for RDMA NICs that can deliver high packet rate by enabling the transmission of multiple packets after sorting all flows.</p><p>At its core, Tassel renovates the workflow of the rate limiter hierarchically: by first applying scalable rate limiting to the flows to be scheduled, followed by accurate rate limiting to the packets to be transmitted.</p><p>Tassel leverages adaptive batching, which fetches multiple packets from scheduled flows to hide PCIe and sorting latency.</p><p>It employs packet filtering, which reduces the number of packets requiring sorting to increase the sorting rate.</p><p>Tassel can achieve 125Mpps packet rate when supporting 16K flows, making it sufficient to transmit 100 B packets at 100Gbps link rate, outperforming SENIC and PIEO by 3.6×.</p><p>Tassel demonstrates its high scalability with low resource usage.</p><p>When supporting 16K flows, the computing and memory resources consume less than 5% and 1% of our FPGA, respectively.</p><p>Tassel preserves high accuracy as it precisely enforces the rate limits ranging from 100Kbps to 100Gbps and proportionally shares bandwidth when the link is oversubscribed.</p><p>RNICs incorporate the congestion control mechanisms to manage complex environments and address challenges such as high link rate, low fabric latency, large infrastructure scale, and incast.</p><p>Within RNIC, the congestion control module computes the sending rate for potentially tens of thousands of flows, and then utilizes the rate limiter to execute these policies.</p><p>Rate-based congestion control requires the rate limiter to enforce the calculated rate for each flow and pace the traffic accordingly.</p><p>Window-based congestion control needs the rate limiter to pace packets especially when setting the congestion window smaller than one to handle the large-scale incast.</p><p>Centralized congestion control uses a centralized network arbiter to allocate bandwidth for individual senders according to the traffic pattern.</p><p>The rate limiter is also a necessary component of programmable congestion control frameworks.</p><p>The rate limiter can additionally be used to implement native hardware rate limiting for network traffic isolation.</p><p>Using the rate limiter to implement native hardware rate limiting can be beneficial.</p><p>The rate limiter should precisely enforce the given rate on each flow and inject inter-packet gaps to smooth the traffic.</p><p>It must support rate limiting for tens of thousands of flows at end hosts.</p><p>It should operate fast and achieve a high packet rate, ensuring its integration into RNIC’s data path does not degrade its high performance.</p><p>Effective congestion control and traffic isolation desire a highly accurate rate limiter.</p><p>An accurate rate limiter should support a wide range of rate limits for each flow and allow for fine-grained adjustments.</p><p>RNICs need to integrate the rate limiter into their data path.</p><p>Consequently, the performance of the rate limiter directly impacts the performance of RNIC.</p><p>WF2Q+ is an accurate and classic time-based rate limiting algorithm, widely adopted in rate limiters to ensure accuracy.</p><p>SENIC optimizes memory usage by storing just flow metadata in hardware.</p><p>PIEO saves computing resources by implementing a two-dimensional compare-and-shift architecture.</p><p>SE-PIEO, which integrates SENIC and PIEO, achieves high accuracy and scalability, but only 34.5Mpps at 16K flows—31.4% of RNIC’s capability.</p><p>It transmits one packet per sorting, and the sorting performance determines the rate limiter’s performance.</p><p>The increasing number of flows to be sorted increases the complexity of combinational circuit, which then naturally decreases the maximum achievable clock frequency.</p><p>Instead of transmitting only the head packet, we seek to transmit multiple packets after sorting all flows.</p><p>The workflow of monolithic rate limiting consists of two steps, flow-level scheduling and packet-level transmission.</p><p>We first sort flows and schedule the nearest flow.</p><p>Once a flow is scheduled, we fetch multiple packets.</p><p>Then, we apply rate limiting to this batch of fetched packets to ensure accuracy.</p><p>This extra tier of rate limiting ensures the accuracy of packet transmission and injects inter-packet gaps.</p><p>We call this packet-level rate limiting.</p><p>To bound the packet number for packet-level rate limiting, Tassel facilitates packet filtering that only filters a handful of packets that are the latest for transmission and discards the rest.</p><p>We observe that the maximum number of imminent packets in RNIC is bounded.</p><p>In RNIC with a high packet rate of 110Mpps and a scheduling latency of 1 µs, the total number of imminent packets typically amounts to a few hundred.</p><p>Packet filtering effectively reduces the number of packets that require packet-level rate limiting.</p><p>Tassel only stores the packet metadata for imminent packets.</p><p>These dropped packets can be retrieved from the host in time when this flow is scheduled again.</p><p>The overhead of this fetch-and-drop policy is low.</p><p>The packet scheduler monitors the global timer to transmit packets on time.</p><p>It supports strict rate limiting with a fallback to weighted sharing by adjusting the timing rate in the global timer.</p><p>When the aggregate flows’ rate limits are larger than the link rate, the timer proportionally slows down the system time.</p><p>Tassel’s hierarchical rate limiter can be integrated into the architecture of RNIC by replacing the original QP scheduler.</p><p>Tassel’s data path includes a flow scheduler for flow-level rate limiting and a packet scheduler for packet-level rate limiting.</p><p>Tassel uses a pipelined heap to store and sort QP scheduling time, a timing wheel for eligibility evaluation, and a register array for rank sorting.</p><p>Tassel’s memory consumption is negligible compared to 10Mb total memory size.</p><p>When supporting 10 K QPs, Tassel consumes theoretically 166.5 KB on-chip SRAM in total.</p><p>Tassel employs a combination of practical and scalable data structures to fulfill algorithm demands, save computing resources, and reduce circuit complexity.</p><p>We prototyped Tassel on an Intel Agilex FPGA board, running at a clock frequency of 250MHz.</p><p>Our FPGA board comprises 782 K ALMs, 1565 K registers, and 10464 BRAMs.</p><p>We realize 16 K QPs in Tassel and the resource consumption is broken down in Table 1.</p><p>Tassel’s implementation consumes 34.2 K ALMs, 10.1 K registers, and 115KB on-chip SRAM in total, which occupies 4.4%, 0.65%, and 0.44% available resources.</p><p>Tassel can saturate the 100Gbps link bandwidth with a 128 B message size, achieving 125Mpps high packet rate.</p><p>In contrast, SE-PIEO achieves 34.5Mpps, 27.6% as compared to Tassel.</p><p>Tassel meets RNIC’s desired high performance of 110Mpps.</p><p>Tassel achieves a high clock frequency and exhibits a slower decline in frequency as the number of flows increases.</p><p>Tassel consistently achieves line rate as the number of QPs increases.</p><p>Tassel accurately enforces any given rate limit, including both small and large rates.</p><p>Tassel maintains the accuracy of different configured rate limits with numerous flows.</p><p>Tassel supports weighted fair sharing when the link is oversubscribed.</p><p>We present the design and implementation of Tassel, a fast, scalable, and accurate rate limiter for RDMA NICs.</p><p>Tassel’s hierarchical rate limiter combines scalable flow-level rate limiting with fast and accurate packet-level rate limiting, thereby achieving high performance, scalability, and accuracy at the system level.</p>",
        "abstract_gpt": "<p>RDMA NICs demand a rate limiter that is simultaneously accurate, scalable, and capable of high-speed operation to enforce traffic policies such as congestion control and isolation across thousands of flows. Existing solutions like SENIC and PIEO provide accuracy and scalability but fall short in performance due to their monolithic design and inefficient flow sorting. This work introduces Tassel, a hierarchical rate limiter that separates flow-level and packet-level rate limiting to enable high packet throughput without compromising accuracy. Tassel employs adaptive batching to hide PCIe and sorting latency, and packet filtering to reduce the number of packets requiring sorting. Implemented on an Intel Agilex FPGA, Tassel supports 16K flows and achieves a 125Mpps packet rate—outperforming prior solutions by up to 3.6×—while consuming minimal computing and memory resources. It precisely enforces rate limits from 100Kbps to 100Gbps and supports proportional bandwidth sharing when links are oversubscribed. By maintaining accuracy at scale and integrating efficiently into RNIC architecture, Tassel meets the stringent performance and functionality demands of modern data center networks. This hierarchical approach enables programmable congestion control and traffic isolation without degrading RNIC throughput, setting a new standard for RDMA-compatible rate limiting.</p>",
        "abstract_human": "<p>RDMA NICs desire a rate limiter that is accurate, scalable, and fast: to precisely enforce the policies such as congestion control and traffic isolation, to support a large number of flows, and to sustain high packet rates. Prior works such as SENIC and PIEO can achieve accuracy and scalability, but they are not fast enough, thus fail to fulfill the performance requirement of RNICs, due primarily to their monolithic design and one-packet-per-sorting transmission. We present Tassel, a hierarchical rate limiter for RDMA NICs that can deliver high packet rates by enabling multiple-packet-per-sorting transmission, while preserving accuracy and scalability. At its heart, Tassel renovates the workflow of the rate limiter hierarchically: by first applying scalable rate limiting to the flows to be scheduled, followed by accurate rate limiting to the packets to be transmitted, while leveraging adaptive batching and packet filtering to improve the performance of these two steps. We integrate Tassel into the RNIC architecture by replacing the original QP scheduler module and implement the prototype of Tassel using FPGA. Experimental results show that Tassel delivers 125 Mpps packet rate, outperforming SENIC and PIEO by 3.6×, while supporting 16 K flows with low resource usage, 7.5% - 25.6% as compared to SENIC and PIEO, and preserving high accuracy, precisely enforcing rate limits from 100 Kbps to 100 Gbps.</p>"
    },
    {
        "index": "41",
        "excerpt": "<p>Historically, approaches to Information Extraction (IE) have been divided into two paradigms: rule-based IE, such as IBM’s SystemT, and ML-based IE of which Transformers and LLMs are the predominant example.</p><p>While the latter have shown impressive capabilities, it is increasingly recognized that to reach acceptable accuracy in practical scenarios one has to orchestrate LLM-based extractors using scenario-specific logic.</p><p>This logic is often implemented imperatively in languages such as Python.</p><p>In this paper, we present SpannerLib, a rule-based approach to IE that facilitates the orchestration of ML-based IE in a declarative framework.</p><p>Document spanners have been studied over the past decade as the theoretical core of industrial systems for Information Extraction (IE) from text such as IBM’s SystemT.</p><p>The framework casts IE as relational querying: generic IE functions (such as regular expressions with capture variables) extract base relations that, in turn, are manipulated by the relational algebra.</p><p>Thus, the framework allows to utilize the simplicity and popularity of query languages, such as SQL, to greatly simplify the development of solutions for text analysis.</p><p>The logical nature of document spanners does not preclude the potential of utilizing generic solutions for Natural Language Processing (NLP) that are nowadays dominated by statistical and numerical analysis, from basic classifiers to Large Language Models.</p><p>Extensions such as Spannerlog showed how the theoretical framework can be elegantly combined with custom code as special user-defined functions called IE functions.</p><p>In this demonstration, we showcase SpannerLib: a library for integrating document spanners in Python, resulting in an IE development that enjoys the benefits of the imperative paradigm (arbitrary Python code, including calling ML models) and the declarative one (Datalog), thereby making rule-based IE development accessible to a wider developer audience.</p><p>We describe three pillars of our system: a full-fledged implementation of Spannerlog, an extension of Python with embedded Spannerlog via IPython’s Magic system, where the two types of code communicate via Pandas DataFrames, and a framework for constructing and registering IE functions to invoke Python code from within Spannerlog rules, which now serve as callback functions.</p><p>SpannerLib is a library for expressing document spanners.</p><p>In particular, it provides a concrete implementation of Spannerlog, which is a variant of Datalog that operates on strings and spans in addition to ordinary relational data.</p><p>By a schema we mean here a sequence of types, where each type is either “str” (string) or “span.”</p><p>An IE function is a stateless function that takes a tuple over a fixed schema and returns a relation over a fixed schema derived from the input.</p><p>For example, a standard NLP tool like part-of-speech tagging can be associated with an IE function that takes a string as input and returns word spans and corresponding tags.</p><p>IE functions that are widely studied in this context are regular expressions.</p><p>Assume a relational database schema.</p><p>A Datalog program is a collection of rules where predicate symbols are applied to variables or constants and can be recursive.</p><p>Spannerlog is essentially Datalog over relations on strings and spans, extended with IE clauses that refer to an execution of an IE function as a means of deriving facts using rules.</p><p>More precisely, an IE predicate binds all facts that can be derived from an application of the IE function over the values bound to its input variables.</p><p>A Spannerlog program is a set of Spannerlog rules, where a Spannerlog rule is defined similarly to a Datalog rule, except that rule bodies can include IE atoms.</p><p>This extension of Datalog rules allows us to elegantly compose multiple IE functions as part of a rule in a readable way.</p><p>The bedrock of SpannerLib is a Python implementation of the Spannerlog language which allows us to programmatically invoke the Spannerlog runtime from Python, and vice versa.</p><p>To build our Spannerlog implementation, we extended the naive bottom-up evaluation method to include evaluation of IE clauses.</p><p>Since Spannerlog requires a more intricate definition of rule safety, which in turn determines IE function execution order within a rule, we also implemented a semantic safety checker according to the safety definitions.</p><p>Additionally, we added aggregation functions to SpannerLib, and we will later illustrate their importance.</p><p>Note that the syntax and semantics are similar to previous Datalog formalisms for aggregation.</p><p>To be useful in the setting of a Python program, we embedded our Spannerlog engine inside the Python runtime, via IPython’s Magic system.</p><p>The Magic system allows us to write and register Spannerlog rules directly inside a Jupyter Notebook Cell.</p><p>SpannerLib uses markers to separate between Python code and Spannerlog rules.</p><p>The session object facilitates communication between the Python and Spannerlog runtimes.</p><p>This code constructs the input relation by taking a DataFrame and importing it to our engine, applies the Spannerlog rule to construct the output relation, and then populates the output DataFrame by querying it.</p><p>SpannerLib enables the user to empower Spannerlog with user-defined functions and integrate them within as novel callbacks to IE functions.</p><p>Any stateless Python function can be added as an IE callback via the session object, as long as the function accepts a relation as input and returns a relation (or sequence of relations) as output.</p><p>We first illustrate how SpannerLib can help build LLM pipelines by concisely expressing programmer intent and, in particular, avoiding orchestration boilerplate and data management.</p><p>Consider the task of developing a code completion program focused on suggesting documentation for the function currently containing the user cursor in a code editor.</p><p>One of the challenges of performing code completions is that the entire code base is too big to fit into the context window.</p><p>Copilot’s approach is to take the last files that were accessed in the code editor and feed them all as context to the LLM model.</p><p>This approach has two main weaknesses: there would mostly be a lot of irrelevant context in some of these files, and there could be very relevant instances of interacting with the function among the files that have not been touched recently.</p><p>We would like to provide the LLM with the following context: the code of the current function, and the code of all functions that call the current function.</p><p>This task requires integration of basic LLM core functionalities, can leverage basic human understanding of the text, and is unique in the orchestration of the specific solution while using very similar tooling as other solutions in the same field.</p><p>Using these generic primitives, it is quite simple and elegant to convey our pipeline’s logic via SpannerLib.</p><p>To study the ability of SpannerLib to simplify text analysis tasks on a bigger scale, we took the source code for an existing rule-based NLP pipeline and rewrote it with SpannerLib.</p><p>The original pipeline deals with analyzing medical text documents to classify patients based on their COVID-19 risk.</p><p>The pipeline consists of 4335 lines of Python code and is based on the open-source NLP library spaCy.</p><p>Over 90 percent of the code was either data management or pipeline orchestration.</p><p>Hence, we were able to reduce around 4000 lines of code into fewer than 400 lines of either Spannerlog rules or tabular data.</p><p>All demonstration scenarios are interactive Jupyter notebooks that contain step-by-step walkthroughs of how to design and implement increasingly complex programs using SpannerLib.</p>",
        "abstract_gpt": "<p>This work introduces SpannerLib, a Python-based library for rule-based information extraction (IE) that bridges the declarative and imperative programming paradigms. While machine learning-based IE methods, particularly those using large language models (LLMs), have become dominant, their practical deployment often requires extensive orchestration through scenario-specific logic written in imperative languages like Python. SpannerLib addresses this challenge by implementing Spannerlog, a declarative framework that extends Datalog to operate over strings and spans, enabling the integration of user-defined IE functions, including ML models, as callbacks within declarative rules. The system supports seamless communication between Python and Spannerlog via IPython’s Magic system and Pandas DataFrames. Through practical demonstrations—including an LLM-powered code completion pipeline and a rewritten medical text analysis pipeline—SpannerLib significantly reduces orchestration complexity and code volume. For instance, a 4335-line spaCy-based COVID-19 risk classification pipeline was condensed into fewer than 400 lines using SpannerLib. This work highlights the potential of combining logical and statistical approaches to simplify and scale IE development in modern NLP applications.</p>",
        "abstract_human": "<p>Document spanners have been proposed as a formal framework for declarative Information Extraction (IE) from text, following IE products from the industry and academia. Over the past decade, the framework has been studied thoroughly in terms of expressive power, complexity, and the ability to naturally combine text analysis with relational querying. This demonstration presents SpannerLib—a library for embedding document spanners in Python code. SpannerLib facilitates the development of IE programs by providing an implementation of Spannerlog (Datalog-based document spanners) that interacts with the Python code in two directions: rules can be embedded inside Python, and they can invoke custom Python code (e.g., calls to ML-based NLP models) via user-defined functions. The demonstration scenarios showcase IE programs, with increasing levels of complexity, within Jupyter Notebook.</p>"
    },
    {
        "index": "42",
        "excerpt": "<p>Sparse matrix-matrix multiplication (SpMM) is a computational process where two sparse matrices are multiplied.</p><p>SpMM is a cornerstone in scientific simulations, linear algebra, graph analytics, and the rapidly evolving fields of deep learning.</p><p>Its crucial role in efficiently processing large-scale data structures and complex algorithms makes the effective acceleration of SpMM not just a computational challenge, but a key enabler in advancing researches and applications in these diverse and impactful domains.</p><p>The processing efficiency of SpMM is heavily influenced by the characteristics of the input sparse matrices.</p><p>The high proportion of zero elements in these matrices leads to challenges such as low utilization of computational and memory resources.</p><p>These irregular sparse patterns, coupled with unpredictable memory access patterns, present significant obstacles for conventional cache-based computing architectures.</p><p>As a result, SpMM often becomes a performance bottleneck, particularly in an era where processing large datasets is increasingly crucial.</p><p>Given the critical role of SpMM in various computational domains, developing specialized accelerators to enhance SpMM performance is important.</p><p>Existing SpMM accelerators predominantly utilize fixed execution flows, such as inner-product (InP), outer-product (OutP), or row-by-row (ROW), each tailored to optimize either input or output data reuse.</p><p>However, the efficiency of each execution flow is determined by sparse patterns, resulting in inconsistent performance across different sparse matrices.</p><p>This variability in performance due to differing sparse patterns underscores the need for SpMM execution flow that can dynamically adapt to optimize performance across a range of matrix structures.</p><p>Additionally, the existing SpMM accelerators exhibit limited capabilities in exploiting the inherent parallelism in SpMM, leading to several inefficiencies.</p><p>These inefficiencies impede the effective utilization of hardware resources and limit the overall performance of SpMM operations.</p><p>This underscores the need for designs that better align execution flows with the architecture, enhance fine-grained parallelism, and mitigate the synchronization overhead.</p><p>Furthermore, cache performance is crucial for the efficiency of SpMM accelerators, but is often overlooked.</p><p>Conventional cache replacement policies used in SpMM accelerators aim to reduce the number of cache misses but overlook the importance of concurrency.</p><p>Moreover, the design of caches in current accelerators does not incorporate non-blocking features.</p><p>As a result, a single cache miss causes delays in subsequent accesses, thereby exacerbating performance bottlenecks.</p><p>In this paper, we introduce ACES, an innovative accelerator for SpMM, specifically designed to dynamically adapt its execution flow to accommodate varying sparsity patterns, optimize parallel execution, and implement locality-concurrency co-optimizations for on-chip cache.</p><p>ACES has the following unique features.</p><p>ACES is equipped with an adaptive execution flow that intelligently adjusts to varying sparsity patterns of input matrices.</p><p>This adaptability is achieved through a spectrum of condensing degrees, implemented with minimal overhead and without altering the original encoding formats of the matrices.</p><p>ACES considers the reuse of input data and the synchronization needed for merging partial output results from each execution flow.</p><p>ACES employs PureFiber, a concurrency-aware cache replacement policy, to optimize cache management.</p><p>ACES incorporates a non-blocking buffer to manage cache miss accesses, ensuring that cache misses do not significantly disrupt subsequent accesses.</p><p>The hardware architecture of ACES is specifically designed to complement its adaptive execution flow and cache optimizations.</p><p>ACES not only consistently provides optimal performance across all workloads, with average speedups of 25.5× over SIGMA, 8.9× over SpArch, and 2.1× over SPADA, but also achieves this with the lowest area cost.</p><p>While conventional execution flows in SpMM provide distinct characteristics to matrix multiplication, there is no universally optimal solution.</p><p>The complexity of handling highly irregular sparsity patterns, coupled with the demands of parallel computing, underscores the need for innovative, adaptive execution flows.</p><p>SIGMA is an InP-based SpMM accelerator that enhances index intersection efficiency through a bitmap format.</p><p>SpArch adopts an OutP execution flow and proposes an aggressively condensed matrix representation for matrix A.</p><p>SPADA inherits ROW and introduces a window-based adaptive (WA) execution flow.</p><p>WA introduces a collective dependency among the multipliers.</p><p>ACES differentiates itself with an adaptive execution flow that balances data reuse and parallelism.</p><p>ACES emphasizes the co-optimizations of locality and concurrency in on-chip cache design and management.</p><p>The key components of ACES consist of a condensing adaptor, multiple PEs, two schedulers, and a global cache integrated with a non-blocking buffer.</p><p>ACES incorporates a condensation adapter that dynamically tunes the condensed matrix representation for matrix A.</p><p>ACES employs parallel computing, utilizing MPEs and APEs to support its adaptive execution flow.</p><p>A synchronization scheduler is designed to mitigate synchronization conflicts between APEs.</p><p>The global cache in ACES is a critical component, designed to efficiently manage both matrix B rows and matrix C partial output rows.</p><p>The PureFiber cache replacement policy is employed to manage cache lines effectively by considering both data locality and concurrency.</p><p>The integration of a NB buffer in the global cache supports the PureFiber policy and enhances performance by facilitating non-blocking accesses.</p><p>ACES introduces a mechanism offering a spectrum of condensing degrees to ensure an optimized execution flow tailored to each workload.</p><p>We first partition the entire matrix into bands.</p><p>For large bands, consisting of at least 256 rows, we identify the optimal condensing degree through the sampling phase.</p><p>For small bands, we apply moderate condensing by default.</p><p>Each MPE in ACES is specifically designed for executing scalar-vector multiplications.</p><p>ACES adopts a distinctive one-to-one pairing of MPEs with APEs.</p><p>Each APE handles the merging of newly produced partial output fibers with corresponding partial output fibers stored in the global cache.</p><p>In instances where there is no partial fiber in the global cache that can be merged, the APE writes the partial fiber into the cache directly.</p><p>The independent and concurrent processing by the APEs, in collaboration with the synchronization scheduler, enhances system parallelism.</p><p>After completing all scalar-vector multiplications, every APE in the system participates in the final merging stage.</p><p>ACES employs a merging scheduler to optimize this final merging process.</p><p>ACES utilizes the synchronization scheduler to efficiently assign fibers to APEs.</p><p>The synchronization scheduler coordinates with SQs and APEs to minimize stalls.</p><p>ACES adapts the Huffman tree to orchestrate the merging process for each row of the output matrix.</p><p>In the practical implementation of ACES, the Huffman tree is constructed using a priority queue.</p><p>The global cache in ACES stores fibers of matrix B and partial output fibers of matrix C.</p><p>We define pure fiber as a scenario where cache lines of a fiber are accessed concurrently without any cache misses.</p><p>We have developed PureFiber, a concurrency-aware cache replacement policy designed to prioritize achieving more pure fibers.</p><p>PureFiber integrates data locality and concurrency considerations for each cache line when making eviction decisions.</p><p>PureFiber optimizes cache management, focusing on increasing the number of pure fibers.</p><p>By prioritizing the eviction of higher-density fibers, PureFiber improves the overall pipeline efficiency in ACES.</p><p>ACES integrates an NB buffer with the global cache, creating an efficient non-blocking cache system.</p><p>The non-blocking cache ensures that cache misses do not stall subsequent cache accesses.</p><p>By allowing the cache to handle other requests during miss processing concurrently, the non-blocking design in ACES significantly reduces memory stall cycles.</p><p>ACES consistently outperforms all state-of-the-art accelerators across every workload.</p><p>On average, ACES achieves significant performance gains of 25.5×, 8.9×, and 2.1× over SIGMA, SpArch, and SPADA, respectively.</p><p>ACES incurs the lowest off-chip memory traffic compared to recent SpMM accelerators.</p><p>ACES consistently maintains PE utilization rates above 90.0% in 15 of the 18 evaluated workloads.</p><p>ACES achieves reduced area overhead while improving performance relative to accelerators with larger caches.</p><p>ACES supports an adaptive execution flow, adept at efficiently processing matrices with a wide range of sparse patterns.</p><p>It also integrates co-optimizations of data locality and concurrency within its global cache.</p><p>The hardware architecture of ACES is meticulously tailored to complement its adaptive execution capabilities.</p><p>Our comprehensive evaluations indicate that ACES consistently outperforms current state-of-the-art SpMM accelerators.</p>",
        "abstract_gpt": "<p>Sparse matrix-matrix multiplication (SpMM) is a fundamental operation in scientific computing and machine learning but remains a significant performance bottleneck due to the irregular sparsity patterns and poor cache utilization of sparse matrices. Existing accelerators rely on fixed execution flows, which are inefficient across varying sparsity structures and limit hardware utilization. This paper introduces ACES, a novel SpMM accelerator designed to dynamically adapt execution flows, optimize parallelism, and co-optimize data locality and concurrency for on-chip caches. ACES features an adaptive execution engine that intelligently selects execution strategies based on condensing degrees without modifying matrix formats. It incorporates PureFiber, a concurrency-aware cache replacement policy, and a non-blocking (NB) cache buffer to minimize access stalls. The architecture includes MPE-APE pairings, specialized schedulers, and Huffman tree-based merging to optimize throughput and synchronization. ACES consistently achieves superior performance, with average speedups of 25.5×, 8.9×, and 2.1× over SIGMA, SpArch, and SPADA, respectively, while maintaining over 90% processing element utilization in most workloads and reducing memory traffic and area overhead. These results demonstrate ACES as a high-performance, low-overhead, and adaptable solution to the longstanding inefficiencies of SpMM, offering substantial improvements in scalability and execution efficiency across diverse sparse workloads.</p>",
        "abstract_human": "<p>Sparse matrix-matrix multiplication (SpMM) is a critical computational kernel in numerous scientific and machine learning applications. SpMM involves massive irregular memory accesses and poses great challenges to conventional cache-based computer architectures. Recently dedicated SpMM accelerators have been proposed to enhance SpMM performance. However, current SpMM accelerators still face challenges in adapting to varied sparse patterns, fully exploiting inherent parallelism, and optimizing cache performance. To address these issues, we introduce ACES, a novel SpMM accelerator in this study. First, ACES features an adaptive execution flow that dynamically adjusts to diverse sparse patterns. The adaptive execution flow balances parallel computing efficiency and data reuse. Second, ACES incorporates locality-concurrency co-optimizations within the global cache. ACES utilizes a concurrency-aware cache management policy, which considers data locality and concurrency for optimal replacement decisions. Additionally, the integration of a non-blocking buffer with the global cache enhances concurrency and reduces computational stalls. Third, the hardware architecture of ACES is designed to integrate all innovations. The architecture ensures efficient support across the adaptive execution flow, advanced cache optimizations, and fine-grained parallel processing. Our performance evaluation demonstrates that ACES significantly outperforms existing solutions, providing a 2.1× speedup and marking a substantial advancement in SpMM acceleration.</p>"
    },
    {
        "index": "43",
        "excerpt": "<p>Autonomous Vehicles (AVs) are set to bring about a paradigm shift in transportation.</p><p>AVs operate through the use of advanced Autonomous Driving Systems (ADSs), which eliminate the need for human drivers to control the vehicle’s movements.</p><p>ADSs are considered highly security-critical systems, as malfunctions can result in severe consequences.</p><p>Given that on-road testing suffers from several limitations (such as safety risks and high expenses), simulation-based testing in high-fidelity simulators such as SVL and CARLA has emerged as a popular approach for evaluating AVs.</p><p>While these methods are effective at finding different violations, they typically do not provide insight into the specific decisions and actions of the AV that ultimately caused the violations.</p><p>Such information is critical for engineers to improve the safety and reliability of AVs but is time-consuming and laborious to extract manually, especially in large-scale testing frameworks.</p><p>Causality analysis has been proposed within the software engineering community as a means to assist developers in deducing the underlying causes of faulty behaviors observed in a failed test case.</p><p>Unfortunately, given an AV accident recording extracted from a simulator, it is non-trivial to apply existing causality analysis techniques due to two main challenges.</p><p>First, ADSs consist of multiple independent, decoupled modules that communicate via message passing.</p><p>Second, the analysis space of a typical accident recording is huge, requiring new approaches for identifying the accident-related segments that should be focused on.</p><p>To address these challenges, we present ACAV, a framework for Automatic Causality analysis of AV accident recordings.</p><p>In the first stage, we define and apply feature extraction schemas based on the messages exchanged between ADS modules.</p><p>These schemas are used to vectorize information about the map, as well as the AV’s perception, prediction, and planning.</p><p>In the second stage, we identify safety-critical frames using an a priori method based on safety specifications extracted from the driver’s handbook and traffic laws of California.</p><p>To evaluate the effectiveness of our framework, we implemented it for Apollo 7.0 and the SVL simulator.</p><p>We applied ACAV to vectorize and simplify these recordings, finding that ACAV achieved a 62.23% reduction ratio rate without discarding critical frames.</p><p>Upon analyzing the simplified recordings with CAT, our approach identifies five distinct types of causal events in 93.64% of the recordings.</p><p>The ADSs of AVs are composed of various modules, including perception, localization, prediction, planning, and control.</p><p>Each module subscribes to one or more channels in the ADS to obtain the required inputs and publishes its output as a message to the corresponding channels.</p><p>Behavioral and motion planning are critical tasks of the planning module, translating the path obtained from route planning into a series of waypoints by calculating specific speed and acceleration plans.</p><p>The longitudinal behavioral and motion planning can be visualized effectively in a Station-Time graph (ST graph), where time is the horizontal axis, the planned longitudinal trajectory distance is the vertical axis and the planned longitudinal trajectory is a curve.</p><p>Our framework is designed to automatically identify and exclude such segments that are unrelated to the accident.</p><p>This reduces the workload of ADS engineers who can then immediately focus on the most important parts of the recording.</p><p>Our framework uses a priori knowledge to label frames containing potential accident risks, such as this one, as safety-critical frames.</p><p>The causal factors of this accident can be attributed to the incorrect priority and trajectory prediction by the prediction module, the flawed decision made by the planning module, and the vehicle’s skidding.</p><p>We use a scenario-based recording segmentation technique to merge the frames, and subsequently, we use specifications derived from the driver handbook and traffic laws of California to identify segments of the recording that are relevant to accidents.</p><p>Our adaptation extends the fuzzing engine by adding a recorder that captures the corresponding driving recording for each test case.</p><p>To facilitate causality analysis, we select and align messages from the communication channels of the map, localization, perception, prediction, and planning modules.</p><p>In the vectorization phase, our primary objective is to extract information related to accidents, which can be associated with the map, perception, prediction, and planning modules in the ADS.</p><p>The planning schema includes information on the main driving decision the AV currently executes, the operational design domain (ODD), motion, and whether it is safe according to the responsibility-sensitive safety (RSS) rules.</p><p>Our framework converts each frame into three feature vectors based on these three schemas.</p><p>The framework segments the recording by comparing the similarity of consecutive feature vectors.</p><p>Our framework generates segmentation plans for each of the three types of vector schemas previously described.</p><p>Our approach creates an overapproximation of relevant frames to narrow down our focus to the most crucial situations.</p><p>To classify a frame as irrelevant, we consider several factors.</p><p>If all of these conditions are met, we classify the frame as an irrelevant frame.</p><p>In the second stage, we automatically analyze the accident-related segments that were generated in the first stage to identify potential causes of the accident.</p><p>Next, we implement a causal analysis tool, CAT, that works by examining speed planning.</p><p>CAT compares their current speed planning and actual trajectory to deduce the causal events of the accident.</p><p>To ensure compliance with the rules outlined in the driver’s handbook, it is necessary to have a robust specification language that allows us to precisely describe these rules.</p><p>The specification language consists of propositions based on a set of pre-defined variables, as well as the usual logical connectors.</p><p>CAT analyzes frames labeled as safety-critical to determine whether the planning trajectory could intersect with other traffic participants in a way that might cause an accident.</p><p>CAT firstly checks the priority prediction of the NPC involved in the accident.</p><p>If the NPC’s priority prediction is ‘ignore’, it means that the cause of the collision is wrong priority prediction.</p><p>If the prediction of the NPC made no error, CAT checks the AV’s behavioral planning and then the motion planning.</p><p>If CAT finds that the AV’s planning is collision-free, it compares the actual trajectory with the planned trajectory.</p><p>While we have presented ACAV in the context of Apollo, the overall approach can be generalized to other ADSs.</p><p>To mitigate these differences in message format, we populated the missing fields with default values.</p><p>To evaluate the performance of our framework, we conducted experiments to answer the following research questions.</p><p>The optimal balance between recall and pruning efficiency is crucial for an effective segmenting method.</p><p>A threshold of 0.8 is optimal, as it strikes a balance between high record reduction performance and the retention of a sufficient number of safety-critical frames.</p><p>We assessed the effectiveness of the ACAV by analyzing the number of different causal events it could automatically identify.</p><p>ACAV successfully identified the causal events for 103 of these accident recordings.</p><p>We injected eight distinct fault types into the ADS.</p><p>ACAV performs well, accurately identifying causal events in 1064 out of the accident recordings.</p><p>We acknowledge certain limitations and threats to the validity of our evaluation.</p><p>Our evaluation is exclusively focused on the Apollo ADS.</p><p>It is imperative to acknowledge that the faults injected in RQ4 do not reflect the real-world faults in the ADSs.</p><p>We presented ACAV, an automated framework for determining the causal events in AV accidents.</p><p>We successfully implemented it in both Apollo and Autoware.universe and evaluated our framework using 110 accident driving recordings.</p><p>In future work, we are interested in developing automatic program repair techniques for ADSs, leveraging the results of causality analyses from accidents.</p>",
        "abstract_gpt": "<p>Autonomous Vehicles (AVs) rely on complex Autonomous Driving Systems (ADSs), whose failures can lead to critical safety issues. While high-fidelity simulation platforms like SVL and CARLA enable scalable testing, they often lack insight into the AV’s decision-making process during accidents. Existing causality analysis tools are ill-suited for AVs due to the modular, message-passing architecture of ADSs and the vast analysis space of simulation recordings. This paper introduces ACAV, an automated framework for causality analysis of AV accident recordings. ACAV first vectorizes simulation data through schemas that encode perception, prediction, planning, and map modules. It then segments the recordings using similarity metrics and labels safety-critical frames using specifications derived from driver handbooks and traffic laws. A second-stage analysis tool, CAT, identifies causal factors by examining speed planning, trajectory deviation, and module-level errors. Applied to Apollo 7.0 and Autoware.universe in the SVL simulator, ACAV achieved a 62.23% reduction in recording size without losing critical frames and identified causal events in 93.64% of accident scenarios. The system successfully pinpointed causes in 1064 recordings across various fault types. ACAV presents a generalizable approach to streamline post-accident analysis, reducing manual effort and aiding AV engineers in enhancing ADS safety.</p>",
        "abstract_human": "<p>The rapid progress of autonomous vehicles (AVs) has brought the prospect of a driverless future closer than ever. Recent fatalities, however, have emphasized the importance of safety validation through large-scale testing. Multiple approaches achieve this fully automatically using high-fidelity simulators, i.e., by generating diverse driving scenarios and evaluating autonomous driving systems (ADSs) against different test oracles. While effective at finding violations, these approaches do not identify the decisions and actions that caused them—information that is critical for improving the safety of ADSs. To address this challenge, we propose ACAV, an automated framework designed to conduct causality analyses for AV accident recordings in two stages. First, we apply feature extraction schemas based on the messages exchanged between ADS modules, and use a weighted voting method to discard frames of the recording unrelated to the accident. Second, we use safety specifications to identify safety-critical frames and deduce causal events by applying CAT—our causal analysis tool—to a station-time graph. We evaluated ACAV on the Apollo ADS, finding that it can identify five distinct types of causal events in 93.64% of 110 accident recordings generated by an AV testing engine. We further evaluated ACAV on 1206 accident recordings collected from versions of Apollo injected with specific faults, finding that it can correctly identify causal events in 96.44% of the accidents triggered by prediction errors, and 85.73% of the accidents triggered by planning errors.</p>"
    },
    {
        "index": "44",
        "excerpt": "<p>Although critical for economic planning, disaster preparedness, and policy-making, subseasonal-to-seasonal (S2S) prediction is lagging behind the more established field of short/medium-range weather, or long-range climate predictions.</p><p>For instance, many natural hazards tend to manifest in the S2S scale, including the slow-onset of droughts that lead to wildfire, heavy precipitations that lead to flooding, and persistent weather anomalies that lead to extremes.</p><p>So far, current approaches to weather and climate prediction are heavily reliant on physics-based models in the form of Numerical Weather Prediction (NWP).</p><p>Many NWPs are based on the discretization of governing equations that describe thermodynamics, fluid flows, etc.</p><p>However, these models are expensive to run especially in high-resolution setting.</p><p>Furthermore, their relative inaccessibility to non-experts is a major roadblock to the broader community.</p><p>As a result, there is a growing interest to apply data-driven models to emulate NWPs, as they tend to have faster inference speed, are less resource-hungry, and more accessible.</p><p>Nevertheless, many data-driven benchmarks have so far been focused on the short (1–5 days), medium (5–15 days), and long (years–decades) forecasting ranges.</p><p>In this work, we include S2S as a more challenging task that requires different emulation strategies.</p><p>It is doubly sensitive to initial conditions (IC) as in the case for short/medium-range weather, and boundary conditions (BC) as in the case for long-range climate.</p><p>We propose ChaosBench to bridge these gaps.</p><p>It is comprised of variables beyond the typical surface-atmospheric ERA5 to also include ocean, ice, and land reanalysis products that span over 45 years to allow for full Earth system emulation that respects boundary processes.</p><p>We also provide 44-day ahead physics-based control (deterministic) and perturbed (ensemble) forecasts from four national weather agencies over the last 8 years as baselines.</p><p>In addition, we introduce physics-based and incorporate probabilistic, in addition to deterministic metrics, for a more physically-consistent ensemble that accounts for butterfly effect.</p><p>As far as we know, ChaosBench is one of the first to systematically evaluate several state-of-the-art data-driven models including ViT/ClimaX, PanguWeather, GraphCast, and FourCastNetV2 on S2S predictability.</p><p>We demonstrate that existing physics-based and data-driven models are indistinguishable from unskilled climatology as the forecasting range approaches the S2S timescale.</p><p>The high spectral divergence observed in many state-of-the-art models suggests the lost of predictive accuracy of multi-scale structures.</p><p>This leads to significant blurring and a tendency towards smoother predictions.</p><p>Performing comparably worse than climatology renders them operationally unusable.</p><p>This highlights the urgent need for a robust and unified data-driven S2S intercomparison project.</p><p>Many existing benchmarks are built for short/medium-range weather (up to 15 days), and long-term climate (annual to decadal scale).</p><p>These problems tend to be easier due to the lack of combined sensitivities to IC and BC.</p><p>Many S2S benchmarks tend to focus on regional forecasts.</p><p>ChaosBench has the most extensive overlapping temporal coverage yet, extending to 45+ years of inputs covering multiple reanalysis products beyond ERA5.</p><p>Having a large set of physics-based forecasts as baselines is key to reducing bias and diversifying the target goal-posts.</p><p>ChaosBench also places weights on expanding the diversity of physics-based models.</p><p>ChaosBench introduces physics-based metrics that can be used for comparison and integrated into ML pipeline.</p><p>ERA5 Reanalysis provides a comprehensive record of the global atmosphere combining physics and observations for correction.</p><p>ORAS5 provides an extensive record of sea-ice variables that incorporate multiple depth levels.</p><p>LRA5 provides a detailed record of variables governing global terrestrial processes with specific corrections tailored for land surface applications.</p><p>Forecasts are generated from 2016 to present.</p><p>The UK Meteorological Office uses the Global Seasonal Forecast System Version 6 (GloSea6) to generate daily ensemble/control forecasts for 60-day lead time.</p><p>The National Centers for Environmental Prediction uses the Climate Forecast System 2 (CFSv2) model to generate daily ensemble/control forecast for 45-day lead time.</p><p>The China Meteorological Administration uses the Beijing Climate Center (BCC) model to generate ensemble/control forecasts at 3-day interval for 60-day lead time.</p><p>The European Centre for Medium-Range Weather Forecasts uses the CY41R1 version of the IFS to generate ensemble/control forecasts twice weekly for 46-day lead time.</p><p>We provide an assortment of metrics, which we divide into deterministic, probabilistic, and several proposed physics-based criteria.</p><p>RMSE is useful to penalize outliers.</p><p>Bias assists us to identify misspecification and systematic errors present in the model.</p><p>ACC measures the correlation between predicted and observed anomalies.</p><p>MS-SSIM compares structural similarity between forecast and ground-truth label across scales.</p><p>We propose two physics-based metrics that measure the deviation or difference between the power spectra of prediction and target.</p><p>Spectral Divergence (SpecDiv) computes the expectation of the log ratio between target and prediction spectra.</p><p>Spectral Residual (SpecRes) computes the root of the expected squared residual.</p><p>CRPS evaluates the accuracy of the ensemble distribution against the target.</p><p>CRPSS evaluates the skill of probabilistic forecast relative to climatology variability.</p><p>Spread quantifies the uncertainty in ensemble forecasts.</p><p>Spread/Skill Ratio balances the ensemble spread with the forecast skill.</p><p>We find that deterministic ML models perform worse than climatology on S2S timescale.</p><p>These forecasts exhibit significantly higher spectral divergence.</p><p>This leads to blurring artifacts.</p><p>The pervasive lack of predictive skill underscores the notoriously difficult challenge of S2S forecasting.</p><p>The performance of ensembles across physics-based models improves relative to their deterministic counterparts.</p><p>Higher ensemble size appears to improve skillfulness.</p><p>Controlling for stability is key to extend the predictability range of weather emulators.</p><p>Models trained directly have better performance than those used autoregressively.</p><p>Models that explicitly incorporate physical knowledge have better performance across metrics.</p><p>ECMWF high-resolution ensemble still has the best performance in terms of CRPSS.</p><p>Its predictability range is around 15–20 days ahead before skill collapses to climatology.</p><p>The resurgence of data-driven models is rapidly transforming the field.</p><p>We present ChaosBench, a challenging benchmark to extend the predictability range of weather emulators into the S2S timescale.</p><p>We perform extensive benchmarking on state-of-the-art data-driven and physics-based models.</p><p>Skillfulness can be extended by ensemble forecasting, controlling for exponential error growth, and incorporating physical knowledge.</p>",
        "abstract_gpt": "<p>Subseasonal-to-seasonal (S2S) forecasting remains a major challenge in climate and weather prediction, despite its critical importance for disaster preparedness, economic planning, and policy-making. Traditional Numerical Weather Prediction (NWP) models are computationally intensive and inaccessible to non-experts, while most data-driven methods have focused on shorter or longer time horizons, leaving S2S underexplored. To address this gap, we introduce ChaosBench, a comprehensive benchmark designed to evaluate data-driven and physics-based models on the S2S scale. ChaosBench integrates over 45 years of global Earth system reanalysis data—spanning atmospheric, oceanic, ice, and land variables—and includes 44-day ensemble/control forecasts from four national weather agencies. We propose a suite of deterministic, probabilistic, and novel physics-based metrics to evaluate forecasting skill and structural fidelity. Benchmarking results reveal that state-of-the-art data-driven models degrade rapidly on the S2S horizon, often performing worse than climatology due to spectral divergence and blurring. In contrast, ensemble-based and physically-informed models show improved robustness. Our findings highlight the need for a unified and physically-consistent S2S evaluation framework and establish ChaosBench as a foundational resource for advancing the frontier of data-driven weather emulation.</p>",
        "abstract_human": "<p>Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster preparedness and robust decision making amidst climate change. Yet, forecasting beyond the weather timescale is challenging because it deals with problems other than initial condition, including boundary interaction, butterfly effect, and our inherent lack of physical understanding. At present, existing benchmarks tend to have shorter forecasting range of up-to 15 days, do not include a wide range of operational baselines, and lack physics-based constraints for explainability. Thus, we propose ChaosBench, a challenging benchmark to extend the predictability range of data-driven weather emulators to S2S timescale. First, ChaosBench is comprised of variables beyond the typical surface-atmospheric ERA5 to also include ocean, ice, and land reanalysis products that span over 45 years to allow for full Earth system emulation that respects boundary conditions. We also propose physics-based, in addition to deterministic and probabilistic metrics, to ensure a physically-consistent ensemble that accounts for butterfly effect. Furthermore, we evaluate on a diverse set of physics-based forecasts from four national weather agencies as baselines to our data-driven counterpart such as ViT/ClimaX, PanguWeather, GraphCast, and FourCastNetV2. Overall, we find methods originally developed for weather-scale applications fail on S2S task: their performance simply collapse to an unskilled climatology. Nonetheless, we outline and demonstrate several strategies that can extend the predictability range of existing weather emulators, including the use of ensembles, robust control of error propagation, and the use of physics-informed models. Our benchmark, datasets, and instructions are available at https://leap-stc.github.io/ChaosBench.</p>"
    },
    {
        "index": "45",
        "excerpt": "<p>Pathological examination serves as the gold standard for cancer diagnosis.</p><p>With the fast development of digital scanning devices, traditional glass slides can be rapidly digitized into the whole slide image.</p><p>WSI exhibits a hierarchical structure and huge size, which typically has about one billion pixels at its highest resolution.</p><p>Therefore, obtaining a substantial dataset with accurate pixel-level annotations can often be challenging and unattainable.</p><p>To alleviate this problem, various weakly supervised learning methods have been proposed.</p><p>Among them, multiple instance learning has gained significant popularity for tackling WSI classification tasks.</p><p>MIL utilizes small patches (i.e., instances) to generate the slide-level (i.e., bag-level) representation for analysis.</p><p>MIL-based methods usually follow a three-step pipeline: patch cropping, feature extraction using a pretrained encoder, and slide-level feature aggregation for WSI classification.</p><p>They have achieved significant success in various pathological diagnostic tasks like cancer subtyping, staging, and tissue segmentation.</p><p>However, training these MIL-based models still heavily relies on a large number of slides with bag-level labels which are often unreachable for rare diseases.</p><p>Moreover, these models only learn from the original slide, making them vulnerable to variations in data distribution and leading to sub-optimal generalization performance.</p><p>Recently, a series of emerging works based on visual language model such as CLIP and BLIP introduced the language information, bringing new advancements to the fields of natural image classification, segmentation, object detection, etc.</p><p>In the field of pathology, several VLM-based methods like MI-Zero, BiomedCLIP, PLIP, and QUILT have also been proposed and achieved promising results in diverse pathological diagnostic tasks and datasets.</p><p>These methods adopt the two-tower encoders (i.e., image and text encoders), which are pre-trained on a large number of patch-text pairs collected from the Internet.</p><p>Firstly, the text prompt does not provide effective guidance for identifying ambiguous categories, as it lacks the consideration of pathological prior knowledge.</p><p>Introducing visual descriptive texts can help the model focus on diagnosis-related features and enhance its discriminative ability in classes with subtle differences.</p><p>Secondly, transferring the VLM-based model to the field of pathology in a parameter-efficient way is challenging.</p><p>To address the above limitations, in this work, we propose a dual-scale vision-language multiple instance learning framework for whole slide image classification.</p><p>We construct our dual-scale visual descriptive text prompt, which corresponds to WSIs at different resolutions, based on the frozen large language model.</p><p>The low-scale visual descriptive text prompt mainly focuses on the global tumor structure presented in a WSI of low magnification.</p><p>The high-scale visual descriptive text prompt pays more attention to local finer details on a high-resolution WSI.</p><p>We propose a prototype-guided patch decoder to progressively guide the fusion process of patch features by grouping similar patch features into the same prototype.</p><p>We propose a context-guided text decoder to enhance the text features by incorporating the multi-granular image contexts.</p><p>ViLa-MIL achieves the best results compared with the current state-of-the-art MIL-based methods under the few-shot settings.</p><p>Multiple Instance Learning in WSI.</p><p>The whole process mainly includes three steps: 1) a series of patches are cropped from the original WSI; 2) a pre-trained encoder is utilized to extract the patch features; 3) the patch features are aggregated to generate the final slide features.</p><p>CLAM utilizes a pre-trained image encoder for patch feature extraction and proposes a multi-branch pooling operator trained for weakly-supervised WSI classification tasks.</p><p>Based on CLAM, a series of methods have been proposed to explore how to aggregate the patch features effectively.</p><p>Although these methods have achieved great success in many pathological diagnostic tasks, they rely solely on bag-level labels for training, thus requiring the model to learn discriminative patterns from a large quantity of WSIs, without fully utilizing pathological prior knowledge as the guideline.</p><p>In this work, our ViLa-MIL introduces the dual-scale visual descriptive text prompt as the language prior to guide the training of the model effectively.</p><p>The vision language models, like CLIP and FLIP, have shown remarkable performance in a wide variety of visual recognition tasks.</p><p>Although these works have demonstrated significant classification performance and transferability in a new dataset, collecting a large number of image-text pairs is extremely time-consuming and labor-intensive.</p><p>In this work, we utilize the frozen large language model to generate the dual-scale visual descriptive text prompt, which aids the model in transferring to the target dataset with the guidance of limited labeled data.</p><p>Moreover, two lightweight and trainable decoders are proposed, with one for the image branch and the other for text, to transfer the VLM model to the field of pathology efficiently.</p><p>Combining this diagnostic prior with the multi-scale characteristics of WSI, we propose our dual-scale visual descriptive text prompt to guide the CLIP model for WSI classification.</p><p>These dual-scale visual descriptive texts are consistent with the daily practice of pathologists, which can help the model distinguish subtle and fine-grained pathological morphological features and improve the model’s classification performance.</p><p>To apply the CLIP model to process WSI efficiently, another important question is how to aggregate a large number of patch features.</p><p>Following the current embedding-based MIL methods, a non-overlapping sliding window method is utilized to crop patches I from the WSI.</p><p>By introducing the guidance of learnable prototypes, the patches with high semantic similarity will be grouped into the same prototype.</p><p>Compared with the local patches with the limited receptive field, each prototype captures more global context information.</p><p>By bridging the gap between the two modalities, the model can achieve better alignment of images and texts.</p><p>Finally, the whole model is trained end-to-end, and the cross-entropy loss is formally defined.</p><p>We evaluate our ViLa-MIL on three real-world WSI subtyping datasets, namely TIHD-RCC, TCGA-RCC, and TCGA-Lung, under the few-shot setting.</p><p>GPT-3.5 is taken as the frozen large language model.</p><p>ViLa-MIL achieves significant performance improvement across all metrics on all three datasets.</p><p>Specifically, with the guidance of the visual descriptive text prompt, the single-scale ViLa-MILs already demonstrate promising performance, while the performance is boosted further with the dual-scale text prompt and image features.</p><p>ViLa-MIL learns an embedding space that exhibits a higher level of intra-class compactness and inter-class separability.</p><p>ViLa-MIL still achieves an improvement of 5.5% in the AUC metric compared with the best baseline, indicating its superior cross-domain adaptability and robustness.</p><p>Our dual-scale visual descriptive text prompt contains comprehensive pathological diagnostic information, ensuring ViLa-MIL to have a better generalization ability across multiple centers.</p><p>This indicates that our dual-scale visual descriptive text prompt can boost the WSI classification performance by effectively utilizing the complementary image features at multiple magnifications.</p><p>The model’s performance also shows a certain amount of improvement, which denotes that the multi-granular patch and prototype features can refine the text features further.</p><p>Our decoder achieves the best results under all three metrics on the TIHD-RCC dataset.</p><p>Our ViLa-MIL significantly outperforms all the other methods, indicating its capability to learn more discriminative visual features and improve the model’s classification ability for challenging and ambiguous samples.</p><p>All variants significantly outperform other baselines and superior performances have been achieved with different LLMs, indicating high robustness of ViLa-MIL to different kinds of frozen LLMs.</p><p>In this work, we proposed a dual-scale vision-language multiple instance learning framework for whole slide image classification.</p><p>To transfer the VLM to process the WSI efficiently, a prototype-guided patch decoder was proposed to progressively aggregate the patch features.</p><p>A context-guided text decoder was also proposed to refine the text prompt features further by utilizing the multi-granular image contexts.</p><p>Extensive comparative and ablation experiments on three cancer subtyping datasets demonstrated that ViLa-MIL achieved state-of-the-art results for whole slide image classification.</p>",
        "abstract_gpt": "<p>Pathological examination through whole slide images (WSIs) is the gold standard for cancer diagnosis, but their immense size and the scarcity of pixel-level annotations pose challenges for training robust models. Multiple instance learning (MIL) methods have achieved success in WSI classification by aggregating patch-level features, yet they rely heavily on large labeled datasets and lack incorporation of pathological prior knowledge, limiting generalization. Recent advances in vision-language models (VLMs) offer new opportunities, but transferring them effectively to pathology remains difficult. To address these issues, we propose ViLa-MIL, a dual-scale vision-language MIL framework for WSI classification. ViLa-MIL employs a frozen large language model to generate dual-scale visual descriptive text prompts, capturing both global tumor structures and local fine-grained details, consistent with pathologists’ diagnostic practice. A prototype-guided patch decoder and a context-guided text decoder further enhance feature alignment and aggregation. Extensive evaluation on three cancer subtyping datasets demonstrates that ViLa-MIL consistently outperforms state-of-the-art baselines, achieving superior robustness, generalization, and cross-domain adaptability in few-shot settings.</p>",
        "abstract_human": "<p>Multiple instance learning (MIL)-based framework has become the mainstream for processing the whole slide image (WSI) with giga-pixel size and hierarchical image context in digital pathology. However, these methods heavily depend on a substantial number of bag-level labels and solely learn from the original slides, which are easily affected by variations in data distribution. Recently, vision language model (VLM)-based methods introduced the language prior by pre-training on large-scale pathological image-text pairs. However, the previous text prompt lacks the consideration of pathological prior knowledge, therefore does not substantially boost the model’s performance. Moreover, the collection of such pairs and the pre-training process are very time-consuming and source-intensive. To solve the above problems, we propose a dual-scale vision-language multiple instance learning (ViLa-MIL) framework for whole slide image classification. Specifically, we propose a dual-scale visual descriptive text prompt based on the frozen large language model (LLM) to boost the performance of VLM effectively. To transfer the VLM to process WSI efficiently, for the image branch, we propose a prototype-guided patch decoder to aggregate the patch features progressively by grouping similar patches into the same prototype; for the text branch, we introduce a context-guided text decoder to enhance the text features by incorporating the multi-granular image contexts. Extensive studies on three multi-cancer and multi-center subtyping datasets demonstrate the superiority of ViLa-MIL.</p>"
    },
    {
        "index": "46",
        "excerpt": "<p>Recently, we have witnessed the explosive development of generative large language models such as GPT series and LLaMA.</p><p>Undergone extensive pretraining on document corpora and instruction tuning, these language models have demonstrated an impressive ability to memorize a lot of knowledge in their parameters and effectively recall them to answer users’ instructions and queries.</p><p>Building upon the advancements of LLMs, multimodal LLMs (MLLMs) have been developed to expand the capabilities beyond text and allow users to express their needs using visual input.</p><p>Despite the impressive capabilities of LLMs and MLLMs, their responses are limited to textual outputs.</p><p>It would greatly enhance the response capabilities of MLLMs if they could give visual outputs, like a photograph in this case.</p><p>A straightforward solution is to enhance MLLMs with external image synthesis tools, like diffusion models and Generative Adversarial Networks, for visual output capabilities.</p><p>However, a significant challenge with these modules is their propensity to produce unrealistic or hallucinatory images, which cannot accurately describe real-world images.</p><p>The integration of an image retrieval module seems a more viable solution.</p><p>A bold and innovative idea emerges: Is it possible to equip MLLMs with the ability to memorize visual information within their parameters for retrieval and beyond?</p><p>In this light, we formulate a generative cross-modal retrieval task: given a user query for visual content, MLLMs are expected to recall desired images from their parameters directly as the response.</p><p>Accomplishing this task poses a significant challenge, necessitating the presence of two essential abilities of MLLMs: 1) Visual memory and 2) Visual recall.</p><p>In this work, we propose a novel GeneRAtive Cross-modal rEtrieval framework, GRACE, to overcome the above issues.</p><p>GRACE assigns images unique identifiers, where each identifier is a distinct string representing an image.</p><p>GRACE comprises two training steps: 1) Learning to memorize and 2) Learning to retrieve.</p><p>GRACE enables generative cross-modal retrieval: given a textual query, the MLLM generates an identifier string corresponding to a real image.</p><p>We evaluate GRACE on text-image matching datasets to verify the feasibility of generative cross-modal retrieval.</p><p>GRACE performs comparably to the advance one-tower approaches and demonstrates higher efficiency with large-scale image sizes.</p><p>GRACE transforms the original matching problem into a generation problem, eliminating the need for negative samples during training and retrieval index during inference.</p><p>Inbuilt visual memory serves for retrieval, yet its utility extends beyond mere retrieval.</p><p>The MLLM could describe the memorized image and even answer questions about the memorized images.</p><p>This opens up the possibility of injecting personalized visual experiences of humans into MLLMs.</p><p>The current cross-modal retrieval approaches can be categorized into the two frameworks and the one-tower framework.</p><p>Both frameworks formulate the cross-modal retrieval as a discriminative problem, which relies on discriminative loss and negative samples to learn an embedding space.</p><p>Generative retrieval is an emerging new retrieval paradigm in text retrieval, which generates identifier strings of passages as the retrieval target.</p><p>Generative retrieval gains a lot of attention in text retrieval, as it could take advantage of the powerful generative language models.</p><p>However, how to facilitate cross-modal retrieval in a generative way is still an untapped problem.</p><p>Despite the success of MLLMs in various vision-language tasks, they currently lack the ability to unify cross-modal retrieval into their application.</p><p>Generative cross-modal retrieval defines new requirements, i.e., removing visual input during inference.</p><p>Text-to-image retrieval aims to retrieve relevant images from a database when given a textual query.</p><p>Considering convenience and model sizes, we have chosen Flamingo as the backbone for our method.</p><p>Flamingo consists of three main components: a generative language model, a visual encoder, and cross-attention layers.</p><p>The generative language model receives text input that includes a special token, “&lt;image&gt;”, which indicates the presence of an image.</p><p>GRACE assigns unique identifiers to images in the dataset.</p><p>The model could generate identifiers as retrieval results rather than generate real images.</p><p>The two training steps are designed to enable the model to effectively memorize images in parameters and subsequently learn to recall them in response to textual queries.</p><p>String, numeric, and atomic identifiers do not provide any prior knowledge about the image content, whereas semantic and structured identifiers do.</p><p>Structured identifiers achieved good performance by effectively utilizing the image’s embedding information through a clustering approach.</p><p>Atomic identifiers were found to be the most effective, even outperforming the CLIP model.</p><p>This approach assigns a unique token in the vocabulary for each image, ensuring distinct identification.</p><p>For an image, we train the model to associate this image with its corresponding identifier.</p><p>This learning to memorize step allows the model to learn the mappings from visual inputs to their corresponding identifiers.</p><p>The model must be capable of recalling the corresponding images in response to users’ queries.</p><p>We train the MLLM to predict the appropriate identifier when given a specific query.</p><p>Post-training, the MLLM model could retrieve images akin to text generation.</p><p>We implement constrained beam search in the MLLM.</p><p>This mechanism ensures that every generated identifier accurately matches an existing image’s identifier.</p><p>GRACE demonstrated the capability to recall relevant images in response to textual queries without input of image content.</p><p>There was variability in performance among GRACE with different identifiers.</p><p>Structured identifiers achieved good performance by effectively utilizing the image’s embedding information.</p><p>Atomic identifiers were the most effective.</p><p>However, increasing the number of images directly enlarges the vocabulary size of the MLLM, potentially impacting scalability.</p><p>During the inference stage, we employed constrained generation to ensure the prediction of valid identifiers.</p><p>Removing constrained generation caused significant decline in performance.</p><p>Without constrained generation, the model tends to predict identifiers that do not correspond to any image in the corpus.</p><p>CLIP’s inference speed decreases as image size increases, owing to the escalating number of similarity calculations required.</p><p>The inference speed of our generative framework remains nearly constant.</p><p>When image sizes exceed a certain threshold, our generative framework surpasses CLIP in terms of inference speed.</p><p>The MLLM has successfully memorized certain images, it is capable of providing a description of the image’s content when prompted.</p><p>The model is capable of answering some questions over the memorized images.</p><p>This opens up the possibility of injecting personalized visual experiences of humans into MLLMs for them to understand an individual’s visual journey.</p><p>In this paper, we delved into a novel memorization mechanism for the MLLM to memorize images within its parameters.</p><p>We proposed a generative cross-modal retrieval framework, which introduces a fresh paradigm in cross-modal retrieval.</p><p>This paradigm transforms the original matching problem into a generation problem.</p><p>Our experiments demonstrate that the generative paradigm performs effectively and efficiently even with large-scale image sizes.</p><p>We showcased the MLLM’s ability to interact (e.g., describe and QA) with memorized images.</p><p>Exploring more effective identifiers, like “visual tokens”, would help to enhance generative cross-modal retrieval further.</p>",
        "abstract_gpt": "<p>This paper introduces GRACE, a novel generative cross-modal retrieval framework that enables multimodal large language models (MLLMs) to retrieve images directly from their parameters in response to textual queries. Unlike existing discriminative retrieval approaches that rely on negative samples and visual input at inference, GRACE formulates retrieval as a generation task. It assigns unique identifier strings to images and trains MLLMs in two stages: memorization of image-identifier associations and retrieval via identifier generation. Using Flamingo as the base MLLM, GRACE employs constrained beam search to ensure identifier validity and supports retrieval without access to the original image corpus. Experiments show that GRACE achieves performance comparable to one-tower baselines while offering improved inference efficiency at scale. Among identifier types, atomic identifiers yield the highest accuracy, even outperforming CLIP. Furthermore, GRACE enables MLLMs to describe and answer questions about memorized images, suggesting a path toward integrating personalized visual experiences into language models. This generative paradigm transforms cross-modal retrieval by embedding visual memory into the model’s parameters, eliminating reliance on external image databases during inference and opening new avenues for vision-language integration and user-specific visual grounding.</p>",
        "abstract_human": "<p>The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to “recall” the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter step teaches the MLLM to generate the corresponding identifier of the target image, given the textual query input. By memorizing images in MLLMs, we introduce a new paradigm to cross-modal retrieval, distinct from previous discriminative approaches. The experiments demonstrate that the generative paradigm performs effectively and efficiently even with large-scale image candidate sets.</p>"
    },
    {
        "index": "47",
        "excerpt": "<p>After the Viking King Harald Bluetooth united the Danish tribes over 1,000 years ago, his “legacy”, the Bluetooth protocol, has become the de facto standard for short-range wireless communications today.</p><p>Since the introduction of Bluetooth (Basic Rate, BR) in 1999, new features, such as Enhanced Data Rate (EDR) and High Speed (HS), have been added to improve its performance.</p><p>Later, Bluetooth Low Energy (BLE) and Bluetooth Mesh (Mesh) were released paving the way for its domination in the era of Internet of Things (IoT) and 5G.</p><p>Meanwhile, attacks against Bluetooth (e.g., BlueBorne, BleedingBit, KNOB, BIAS, and BLESA) have been booming in the past few years, impacting billions of devices.</p><p>These attacks exploit both design issues in the Bluetooth specification and implementation flaws in its implementations allowing for privilege escalation, remote code execution, breaking cryptography, spoofing, etc.</p><p>While Bluetooth security as a research field has already drawn significant attention from the community, a systematic understanding of this field is still missing, impeding the advancement of this field.</p><p>To address these issues, in this paper, we provide a systematic study to categorize what has been done in the field of Bluetooth security, and to highlight what is missing and should be done as future work.</p><p>We first provide an overview of the evolution of Bluetooth security and privacy features since its first version.</p><p>Then, we systematize the field of Bluetooth security by diving into 76 attacks and 33 defenses.</p><p>To reason about known attacks, we categorize them based on the layers of the Bluetooth stack they target and the attack techniques they use.</p><p>Similarly, we categorize defenses according to the layers of the Bluetooth stack they are designed to protect and the defense techniques they use.</p><p>To provide more insights, we also specify their scope (e.g., targeting only BC or more), affected phase (e.g., device discovery phase and data transmission phase), and attack models (e.g., whether the device is compromised or not).</p><p>These studies allow us to cross-check the attack and defense systematization to have a big picture of today’s Bluetooth security.</p><p>Our findings, to name a few, include: 1) the BLE device discovery phase draws most privacy attacks; 2) pairing security faces challenges caused by users’ mistakes; 3) Bluetooth specification has different assumptions from its implementations on modern operating systems (OSes); 4) while being effective in finding firmware vulnerabilities, Bluetooth fuzzing has limited support for the host code; 5) there is no effective defense against attacks exploiting users’ mistakes; 6) defenses against attacks targeting BC and BLE encryption are still missing.</p><p>Building upon our systematization, we find that formal analysis of Bluetooth is effective in finding design vulnerabilities in the specification.</p><p>However, the existing formal analysis of Mesh only considers its provisioning protocol, leaving other security-related protocols uncovered.</p><p>Thus, to fill this gap, we take one step towards securing Mesh by proposing a comprehensive Mesh formal model covering all security-related protocols.</p><p>We implement our formal model using ProVerif and verify 11 properties covering all 8 different modes.</p><p>Using our model, we rediscover 2 known attacks against Mesh.</p><p>In addition, we also use our model to confirm that Mesh is not vulnerable to any known attacks anymore after applying the fixes suggested by Bluetooth SIG.</p><p>To foster future research in this field, our model is publicly available.</p><p>To the best of our knowledge, our work is the first to systematically examine Bluetooth security.</p><p>Moreover, we provide promising directions, e.g., more comprehensive exploration of BLE and Mesh privacy, cross-stack security and privacy evaluation, and comprehensive Bluetooth fuzzing, to shed some light on future Bluetooth security research.</p><p>Bluetooth is a short-range radio frequency standard, maintained by the Bluetooth Special Interest Group (SIG), for data exchange between different devices, such as smartphones, laptops, and headsets.</p><p>During the past 24 years, from version 1.0 to version 5.4 (the latest version at the time of the paper writing), Bluetooth has evolved into three different protocols, namely Bluetooth Classic, Bluetooth Low Energy, and Bluetooth Mesh.</p><p>Regardless of the used protocol, Bluetooth communication can be divided into three phases: device discovery, key sharing, and data exchange.</p><p>Since version 5.2, the specification requires that the entropy of the encryption key is at least 7 bytes to mitigate the KNOB attacks against encryption exploiting the allowance of and encryption key with one-byte-entropy.</p><p>Since Bluetooth 5.4, the Secure Connections Only (SCO) mode has been introduced.</p><p>In this mode, a device can only use state-of-the-art security features and FIPS-approved algorithms, i.e., SSP with authenticated pairing methods (e.g., NC and PE), secure authentication, and AES-CCM with a 128-bit-entropy encryption key.</p><p>Thus, if both devices are Bluetooth 5.4-compliant and in the SCO mode, they are immune to all the above-mentioned attacks exploiting design flaws of the security features.</p><p>To address this issue, since Bluetooth 5.2, the specification has required that the maximum interval between MAC address change is 15 minutes.</p><p>In the latest version of Mesh specification at the time of the paper writing, the weaknesses of the provisioning protocol mentioned earlier still exist.</p><p>However, these weaknesses are addressed via advisories from the SIG.</p><p>Bluetooth is getting more secure.</p><p>Bluetooth is getting more private.</p><p>Backward compatibility comes before security.</p><p>To study existing attacks and defenses of Bluetooth and summarize their similarities and differences, we first conduct a survey of Bluetooth attacks and defenses of existing research covering all the papers published in the big four top-tier conferences (i.e., S&P, USENIX Security, CCS, and NDSS) from 2000 to March 2023.</p><p>To further understand the capabilities of the attacks and the scope of the proposed defenses, we classify the surveyed attacks and defenses by their affected layers.</p><p>In general, attacks and defenses targeting BLE and Mesh require weaker hardware capabilities compared with BC.</p><p>BLE device discovery phase draws most privacy attacks.</p><p>Pairing security faces challenges caused by users’ mistakes.</p><p>Bluetooth specification has different assumptions from its implementations on modern OSes.</p><p>Bluetooth fuzzing tools are effective yet not comprehensive.</p><p>There is no effective defense against attacks exploiting users’ mistakes.</p><p>Defenses against authentication bypass and breaking encryption are still missing.</p><p>Formal analysis of Bluetooth is incomplete.</p><p>As the table shows, our model can discover existing attacks, including BlueMAN, BlueMirror, and the MitM attacks against the non-authenticated configuration procedure.</p><p>We verify all the properties (P1 - P11) in all the 8 modes denoted by M1 to M8, respectively.</p><p>We also verify these properties after applying the fix to known attacks following the adversaries from Bluetooth SIG.</p><p>Using our model, we rediscover 2 known attacks against Mesh.</p><p>In addition, we also use our model to confirm that Mesh is not vulnerable to any known attacks anymore after applying the fixes suggested by Bluetooth SIG.</p><p>To foster future research in this field, our model is publicly available.</p><p>Privacy exploration of BLE and Mesh.</p><p>Cross-stack security and privacy evaluation.</p><p>Host layer fuzzing.</p><p>Better UI design of Bluetooth pairing for security and its evaluation.</p><p>Minimizing the functionality of firmware.</p><p>Deprecation of legacy pairing methods.</p><p>To understand Bluetooth security as a research domain, we have analyzed 76 attacks and 33 defenses since Bluetooth was introduced, covering more than 20 years of Bluetooth security research.</p><p>Our systematization provides new insights regarding Bluetooth attacks and defenses and suggests promising research directions.</p><p>The formal verification of Mesh we provided takes one step towards securing Bluetooth Mesh.</p><p>In summary, our work provides a foundation to secure King Harald’s “legacy” in the future.</p>",
        "abstract_gpt": "<p>Bluetooth has become the standard for short-range wireless communication, evolving over 24 years into Bluetooth Classic, Low Energy (BLE), and Mesh. Despite widespread adoption, the protocol has faced a surge of security attacks exploiting design flaws and implementation weaknesses. While prior research has explored individual vulnerabilities, a comprehensive and systematic understanding of Bluetooth security has been lacking. This paper fills that gap by analyzing 76 attacks and 33 defenses drawn from top-tier conference publications. The study categorizes attacks and defenses by Bluetooth stack layers, targeted phases (e.g., discovery, pairing), and threat models, uncovering key insights such as BLE’s susceptibility to privacy attacks during discovery, the persistent risk of user error in pairing, and insufficient defenses against authentication bypass. To address gaps in formal analysis, the authors introduce a comprehensive model for Bluetooth Mesh, implemented in ProVerif and validated across eight modes and eleven properties. The model successfully rediscovers known attacks and confirms the effectiveness of recent mitigations. The publicly released model fosters future research in formal verification and security evaluation. This work offers the first large-scale systematization of Bluetooth security, outlining critical vulnerabilities and proposing future research directions to strengthen the protocol’s security and privacy foundations.</p>",
        "abstract_human": "<p>Named after the Viking King Harald Bluetooth, Bluetooth is the de facto standard for short-range wireless communications. The introduction of Bluetooth Low Energy (BLE) and Mesh protocols has further paved the way for its domination in the era of IoT and 5G. Meanwhile, attacks against Bluetooth, such as BlueBorne, BleedingBit, KNOB, BIAS, and BLESA, have been booming in the past few years, impacting billions of devices. While Bluetooth security has drawn significant attention from the security research community, a systematic understanding of this field is still missing, impeding the advancement of this field.</p> <p>In this paper, we first summarize the evolution of Bluetooth security in the specification in the past 24 years. Then, we provide a systematization of Bluetooth security by diving into 76 attacks and 33 defenses presented by previous research in this area. We first categorize attacks and defenses based on their affected layers and protocols in the Bluetooth stack as well as their threat models. Then, we cross-check the attacks and defenses to have a big picture of Bluetooth security. Based on the systematization, we find that the existing formal analyses of Bluetooth do not cover most of the security aspects of Bluetooth Mesh. Lastly, we take a step towards securing Bluetooth Mesh by designing and implementing a comprehensive formal model of Bluetooth Mesh covering all its security-related protocols. Our systematization reveals, for instance, that the security of Bluetooth pairing faces challenges caused by users’ mistakes, and that Bluetooth fuzzing is effective yet not comprehensive. Based on the systematization, we provide promising future directions to shed some light on future Bluetooth security research.</p>"
    },
    {
        "index": "48",
        "excerpt": "<p>Thanks to their capability to support customizable functions, programmable network chips are becoming more and more popular with a burgeoning arsenal which enlists NVIDIA Spectrum, AMD Pensando, Intel Tofino and IPU, Cisco Silicon One, Juniper Trio, Broadcom Trident, and many others.</p><p>The architectures of these chips can be categorized into three types: pipeline, multi-core Run-To-Completion (RTC), and the hybrid of the two.</p><p>Albeit with high throughput, a pipeline is awkward or even incapable in handling complex and stateful network tasks which involve long dependency chains, feedback loops, substantial computations, or large flow tables.</p><p>RTC cores, on the other hand, are flexible enough for such tasks, but suffer from performance inefficiencies.</p><p>The hybrid architecture combines a pipeline and a number of RTC processors, trying to close the gaps of performance and capability.</p><p>However, without knowing the application requirements in advance, the mechanical hardware juxtaposition can hardly achieve the optimal resource allocation and performance.</p><p>Another architectural difficulty is the interconnection of the two types of resources.</p><p>Moreover, the programming model requires to program for each type of resources separately with manual function mapping, which is often inefficient and nonoptimal.</p><p>We consider three typical network applications to illustrate the limitations of the current network chips.</p><p>The distributed machine learning, and especially the large language model training, may engage the programmable network devices for in-network parameter aggregation to accelerate the process and reduce the network traffic.</p><p>When a cache miss occurs, the corresponding key-value pair are fetched from the server for cache replacement.</p><p>An ideal solution would use the pipeline for rapid cache lookups and, upon a cache miss, use the RTC core to fetch the replacement value from the server.</p><p>Other essential network functions include server load balancing and NAT.</p><p>There are numerous proposals to integrate certain network functions in programmable data plane devices (e.g., switches and NICs).</p><p>Constrained by its complexity, a single function might occupy most of the resources on a pipeline-based chip, leaving little room for other equally important functions.</p><p>Ideally, the basic and common functions which apply to all the traffic should be handled by a pipeline and the complex functions which apply to only selected packets should be handled by RTC cores.</p><p>With a hardened architecture, this is unrealistic because of the absence of advance knowledge on both applications and traffic distribution.</p><p>Our objective is to design a better hybrid architecture which synergizes the benefits of the pipeline and multi-core RTC architectures, while ensuring efficient resource utilization under dynamic conditions, to achieve a versatile, one-size-fits-all solution.</p><p>Essentially, we aim to create a software-defined transformable data path.</p><p>The new chip architecture, OptimusPrime, enables dual programmability for both hardware roles and functions for each block.</p><p>The pipeline in OptimusPrime is decoupled from the RTC processors and used to handles the common “match-action”-type tasks in only one pass to achieve the best possible performance.</p><p>This gives OptimusPrime an advantage in throughput.</p><p>Such an architecture brings new challenges to network programming languages and compilers.</p><p>Given that our architecture involves both pipeline and RTC cores, we consider the fusion of languages reasonable.</p><p>Specifically, we propose to combine P4 and C while using P4 as the base.</p><p>Users can embed C code in their P4 programs which can be compiled by our compiler and mapped to the target chip.</p><p>Our major contributions are summarized as follows.</p><p>We analyze the similarities and differences between the pipeline and multi-core RTC architectures.</p><p>Through switch configuration, each block can function as either a pipeline stage or a multi-core RTC processor.</p><p>We preserve the P4 programming paradigm and integrate C programming into the framework named P4X.</p><p>To validate our architecture, we conduct software-based ASIC simulations and realize an FPGA-based prototype.</p><p>We implement and analyze three aforementioned use cases on OptimusPrime: machine learning parameter aggregation, in-network caching, and network function integration.</p><p>They all present much superior performance and flexibility.</p><p>The key to achieve this is twofold: in hardware, we explore the opportunities for full hardware resource sharing and multiplexing, and in software, we allow users to program in a unified language.</p><p>Instead, OptimusPrime uses a ring bus to connect the blocks, which is sufficient to meet the communication requirements at low cost.</p><p>We advocate a pipeline-oriented programming style by extending the P4 language and rely on the compiler to finish the program mapping and block transformation.</p><p>We find that the constructs of such a processor share a lot in common with a pipeline stage, making it feasible and efficient to play two roles based on the same hardware.</p><p>When a block is transformed into a pipeline stage, it is basically equivalent to an MAU in Tofino, supporting full P4 pipeline programming.</p><p>When a block is transformed to multiple RTC cores, the structure of each core is similar to the RV32I five-stage pipeline CPU.</p><p>The transformable blocks are interconnected through a ring-based network.</p><p>Each core maintains a FIFO buffer for PHVs in its private memory.</p><p>The inner ring is used for inter-block memory access and payload transmission.</p><p>The capability of transmitting payload on the inner ring allows OptimusPrime to support DPI.</p><p>After the packet completes ingress processing and exits, it passes through the traffic manager module, and reenters the ring for egress processing.</p><p>The architecture shown in Fig. 2 can be multiplexed to support both ingress and egress.</p><p>We leave the compiler design for joint ingress-egress optimization for future work.</p><p>Currently no unified programming language is available for the simple hybrid architecture, let alone the transformable architecture.</p><p>P4X allows a program to include the C programming elements such as the switch and loop statements, the ternary expressions, and their nested combinations.</p><p>The block annotation @CPU{...} can be applied to compile a specific portion of code to the RTC cores.</p><p>The back-end compiler then partitions and allocates the entire program based on certain optimization goals.</p><p>The compiler cannot predict the actual traffic characteristics, it can only provide an optimization under the worst-case traffic conditions.</p><p>The back-end compiler then analyzes the CFG and executes the allocation algorithm.</p><p>We implement a hardware prototype of OptimusPrime with 8,937 lines of Scala code on a Xilinx UltraScale+ XCVU13P-based FPGA board.</p><p>The prototype comprises 4 transformable blocks, each of which can be configured as a pipeline stage or an 8-core RTC processor.</p><p>We also analyze the ASIC prototype using Design Compiler on an open-source 45nm ASIC technology library at a frequency of 1 GHz.</p><p>These results confirm the moderate overhead for supporting transformable blocks.</p><p>We evaluate the OptimusPrime prototype and its programming by implementing three representative use cases.</p><p>On the pipeline, the packets with the same task ID are allocated to the same RTC core to ensure the correctness.</p><p>The performance bottleneck is the port bandwidth rather than the packet processing throughput.</p><p>Our system outperforms P4LRU across all scenarios, achieving a reduction of the queries to the remote server by 40.9% to 44.9%.</p><p>The cache performance is related to the partitioning ratio of the transformable blocks.</p><p>When integrating multiple network functions, the dependency chain of the program topology can be too long to fit in one pipeline.</p><p>Network functions are usually deployed at network gateways.</p><p>Because no packet recirculation is needed, implementations on OptimusPrime always show lower average latency and higher throughput.</p><p>The aforementioned experiments demonstrate that contention on the inner ring may affect the performance of OptimusPrime, but the impact is acceptable in most situations.</p><p>OptimusPrime is a novel programmable network chip architecture which supports coexistence of pipeline and RTC cores with transformable hardware blocks.</p><p>With a little extra cost, OptimusPrime allows switches and smartNICs to expand their programmability potential to a new level.</p><p>OptimusPrime retains high performance while endowing the programmable pipeline with richer packet processing capabilities.</p>",
        "abstract_gpt": "<p>Programmable network chips are increasingly important for executing complex, stateful network tasks, yet current architectures—pipeline, run-to-completion (RTC), and their hybrids—suffer from rigidity, inefficiency, or poor resource utilization under dynamic conditions. To address these limitations, this work introduces OptimusPrime, a novel programmable network chip architecture with transformable hardware blocks capable of functioning as either pipeline stages or multi-core RTC processors. OptimusPrime supports dual programmability for both hardware roles and user-defined functions, enabling efficient execution of diverse applications such as machine learning aggregation, in-network caching, and network function integration. To program this architecture, the authors propose P4X, a unified language framework combining P4 and embedded C, compiled via a custom backend for resource-optimized deployment. The system is validated through FPGA prototyping and ASIC simulation, demonstrating reduced server queries (by up to 44.9%), improved throughput, and lower latency compared to existing solutions. Overall, OptimusPrime significantly enhances flexibility and performance in programmable data planes, pushing the boundary of software-defined networking.</p>",
        "abstract_human": "<p>Network dataplane calls for better programmability. Current programmable network processing chips are based on either pipeline or multi-core Run-To-Completion (RTC) architecture with various trade-offs in flexibility, performance, and cost. The existing attempts to amalgamate the strengths of the two are stilted and inflexible. In this paper, we challenge the status quo by introducing a more fluid and organic programmable chip architecture, OptimusPrime, built from identical hardware blocks. Unlike the conventional static hybrid architecture, OptimusPrime allows each block to be transformed into either a pipeline stage processor or a multi-core RTC processor through software-defined configuration, enabling versatile data plane programming tailored to a wide range of applications (e.g., stateful packet processing and in-network computing). We integrate the C and P4 languages for application programming and develop algorithms to map a user program to the optimal distribution of pipeline stages and RTC cores. We demonstrate the viability of OptimusPrime through practical use cases such as in-network aggregation, in-network caching, and network function integration. We developed an FPGA-based prototype and a software-based ASIC simulator to validate the feasibility of OptimusPrime, which can be used by switches and smartNICs to enhance their programmability to a new level with high performance and low cost.</p>"
    }
]