[
    {
        "index": "31",
        "excerpt": "<p>Self-supervised learning is vulnerable to backdoor attacks.</p><p>We identify key properties of existing attacks, where the poisoned data is out-of-distribution and highly clustered in a small region.</p><p>We propose a new attack, DRUPE, which significantly alleviates these problems and evades existing defenses.</p><p>The poisoned samples are all tightly clustered within a very small region in comparison to the clean data.</p><p>This results in a high degree of pairwise similarity among the poisoned samples, providing an opportunity to defenders.</p><p>DECREE reverse-engineers a trigger for a given encoder, by minimizing the pairwise similarity of a set of samples (when the trigger is inserted).</p><p>The encoder is considered backdoored if a small trigger can be found.</p><p>This paper reveals a key insight that the success of backdoor attacks in self-supervised learning is not necessarily dependent on the out-of-distribution property.</p><p>We propose to transform poisoned samples into in-distribution data (w.r.t. the clean data) such that the backdoor information is not detectable in the feature space.</p><p>To achieve this, we estimate the distribution of the clean data leveraging Kernel Density Estimation (KDE).</p><p>We then move the poisoned samples closer to the clean distribution by reducing the difference between the two distributions, which is estimated by the sliced-Wasserstein distance, a metric for measuring distributional differences in a high-dimensional space.</p><p>In addition, we reduce the concentration of the poisoned distribution by distributing the poisoned samples to a wider region, making it similar to that of the target-class distribution in the downstream task.</p><p>We implement an attack prototype called DRUPE (DistRibUtion Preserving backdoor attack in sElf-supervised learning).</p><p>Compared to existing backdoor attacks, DRUPE achieves 10 times smaller distributional difference between the poisoned and the clean data, and 3 times smaller pairwise similarity among poisoned samples, while obtaining comparable benign accuracy and attack success rate on downstream tasks.</p><p>In addition, DRUPE successfully evades two state-of-the-art backdoor defenses in self-supervised learning.</p><p>We observe that, in Section 4, the embeddings of poisoned samples have a few large values, compared to those of clean inputs.</p><p>One can address this by enforcing the L2 norm of poisoned embeddings to be similar to that of clean embeddings.</p><p>DRUPE first initializes the backdoored encoder weights from a clean encoder.</p><p>It also constructs the poisoned set and the expanded reference set.</p><p>At each training epoch, the distributional difference between poisoned samples and clean data is measured.</p><p>DRUPE also considers the tightness of the clean distribution to avoid the negative impact on clean data.</p><p>In order to reduce the concentration of the poisoned distribution, DRUPE maximizes the similarity of expanded reference inputs on backdoored and clean encoders in the embedding space.</p><p>It also maximizes the distance among reference inputs and among poisoned samples.</p><p>The poisoned samples are pulled close to their nearest reference inputs to satisfy the attack goal.</p><p>Finally, DRUPE keeps the normal functionality of the backdoored encoder by comparing its produced embeddings on clean data to those from a clean encoder.</p><p>We evaluate the attack performance of DRUPE on five datasets and compare to existing attacks in Section 6.2.</p><p>We leverage two existing defenses Beatrix and DECREE to evaluate different attacks.</p><p>Beatrix detects poisoned samples based on the abnormality in the feature space.</p><p>Beatrix fails to detect poisoned samples by WB attack, AdvEmbed, and DRUPE.</p><p>The detection accuracy is near 50% on both CIFAR-10 and STL-10 (close to random guessing).</p><p>The results demonstrate that DRUPE effectively blends the poisoned distribution in the clean distribution, resulting in indistinguishable poisoned samples in the feature space.</p><p>DECREE can successfully detect backdoored encoders by BadEncoder, WB attack, Carlini et al., and AdvEmbed with 100.0% accuracy.</p><p>However, it fails to detect DRUPE’s backdoored encoders with only 50% accuracy (which is the same as random guessing).</p><p>This is because DRUPE approximates the target-class distribution and distributes the poisoned samples across the entire target-class space, reducing the concentration of the poisoned distribution that DECREE is based on.</p><p>It is challenging to defend against DRUPE.</p>",
        "abstract_gpt": "<p>Self-supervised learning (SSL) has shown susceptibility to backdoor attacks, where adversarially crafted poisoned samples compromise downstream model behavior. Existing attacks often rely on out-of-distribution, highly clustered poisoned data, which can be detected via their abnormal concentration in feature space. In this work, we introduce DRUPE (DistRibUtion Preserving backdoor attack in sElf-supervised learning), a novel attack that transforms poisoned samples to appear in-distribution and reduces their pairwise similarity, making detection significantly harder. DRUPE estimates the clean data distribution using Kernel Density Estimation and minimizes the distributional gap via sliced-Wasserstein distance. It also disperses poisoned data across the feature space to mimic the target-class distribution. This design evades detection by state-of-the-art defenses such as Beatrix and DECREE, which rely on concentration or embedding similarity. Empirically, DRUPE achieves a 10× smaller distributional difference and 3× lower sample similarity than prior attacks, while maintaining high benign and attack performance across five datasets. Our results reveal that distributional blending is an effective and stealthy strategy for backdoor injection in SSL, highlighting new challenges for developing robust defenses.</p>",
        "abstract_human": "<p>Self-supervised learning is widely used in various domains for building foundation models. It has been demonstrated to achieve state-of-the-art performance in a range of tasks. In the computer vision domain, self-supervised learning is utilized to generate an image feature extractor, called an encoder, such that a variety of downstream tasks can build classifiers on top of it with limited data and resources. Despite the impressive performance of self-supervised learning, it is susceptible to backdoor attacks, where an attacker injects a backdoor into its unlabeled training data. A downstream classifier built on the backdoored encoder will misclassify any inputs inserted with the trigger to a target label. Existing backdoor attacks in self-supervised learning possess a key out-of-distribution property, where the poisoned samples significantly differ from the clean data in the feature space. The poisoned distribution is also exceptionally concentrated, inducing high pairwise similarity among poisoned samples. As a result, these attacks can be detected by state-of-the-art defense techniques. We propose a novel distribution preserving attack, which transforms the poisoned samples into in-distribution data by reducing their distributional distance to the clean data. We also distribute the poisoned data to a wider region in the target-class distribution, mitigating the concentration problem. Our evaluation of five popular datasets demonstrates that our attack, DRUPE, significantly reduces the distributional distance and concentration of the poisoned distribution compared to existing attacks. DRUPE successfully evades two state-of-the-art backdoor defenses in self-supervised learning and is robust against knowledgeable defenders.</p>"
    },
    {
        "index": "32",
        "excerpt": "<p>With more and more applications running in the cloud, traffic demands in DCNs increase continuously.</p><p>Among the applications, tasks like high performance computing (HPC) are greatly affected by both goodput and latency, putting stringent requirements on the network.</p><p>However, existing packet-switched DCNs struggle to meet these requirements due to inadequate capacity of switching chips which will worsen with the slowdown of Moore’s Law.</p><p>To address this, the development of optical switching technology especially fast optical switching has led researchers to turn to reconfigurable optical DCNs, which provide higher capacity as well as lower cost compared with packet switching.</p><p>Unlike packet-switched networks that rely on buffers to absorb conflicts, bufferless optical switching requires synchronous scheduling and reconfiguration to accommodate the dynamic traffic demands including incasts.</p><p>Recently, traffic-oblivious reconfigurable DCNs have drawn researchers’ attention because of their simplicity.</p><p>They schedule the network according to predefined rules and use data relay to cope with dynamic traffic patterns.</p><p>However, despite their simplicity, the data-relay design can lead to compromised performance in goodput and latency.</p><p>Consequently, there is a growing need for a new design that can effectively handle dynamic traffic demands, ensuring both high goodput and low latency while still maintaining feasibility.</p><p>We present NegotiaToR, an on-demand reconfigurable DCN architecture with a simple design.</p><p>With in-band distributed scheduling, it dynamically adapts the optical links among top-of-rack (ToR) switches to traffic demands, and sends data directly to destinations through one-hop paths.</p><p>Beyond the scheduled connections, NegotiaToR also provides unscheduled connections, mitigating the impact of scheduling delays even under incasts.</p><p>Utilizing existing arrayed waveguide grating routers (AWGR) and fast-tunable lasers, NegotiaToR is compatible with prevalent flat topologies, achieving a better performance than the state-of-the-art traffic-oblivious scheme on the same hardware with similar complexity.</p><p>On-demand is an intuitive solution to handle the dynamic traffic in DCNs, especially for unpredictable ToR-ToR traffic where demand forecasting based on historical data is difficult.</p><p>The key challenge of NegotiaToR thus lies in realizing scalable on-demand scheduling with high practicality.</p><p>NegotiaToR’s demand information is binary.</p><p>No data size information is needed by the scheduling algorithm.</p><p>Meanwhile, NegotiaToR distributes the scheduling to ToRs, where each ToR only accounts for the scheduling of ingoing and outgoing traffic, achieving a comparable low complexity with traffic-oblivious designs.</p><p>NegotiaToR implements an in-band control plane, where all ToRs periodically establish all-to-all connectivity with fast optical switching.</p><p>The scheduling algorithm runs on it in a pipelined manner.</p><p>Senders can access the scheduling results locally and tune lasers to the derived wavelength without the need to deliver scheduling results.</p><p>NegotiaToR utilizes the periodical all-to-all connectivity to piggyback a small volume of mice flow data along with scheduling messages, providing opportunities to bypass scheduling delays.</p><p>This effectively manages incasts, promising application performance even under concurrent traffic demands.</p><p>The design of NegotiaToR is guided by the principle of Occam’s Razor.</p><p>We evaluate NegotiaToR through simulations on two representative flat topologies.</p><p>Results show that NegotiaToR outperforms the state-of-the-art traffic-oblivious reconfigurable DCN design under comparable complexity, both in FCT and goodput.</p><p>Particularly, NegotiaToR’s mice flow FCT is one to two orders of magnitude better.</p><p>NegotiaToR is a simple network architecture for reconfigurable DCNs where bufferless optical links connect buffered ToRs.</p><p>With minimal traffic demand information, it schedules traffic distributedly on the ToRs via the in-band control plane in an on-demand while scalable manner.</p><p>NegotiaToR employs a distributed scheduling mechanism on a per-epoch basis, where an epoch is a fixed-length time interval.</p><p>Each epoch comprises two phases, working as the in-band control plane and data plane.</p><p>All packets are transmitted directly to their destinations through one-hop paths.</p><p>To mitigate the impact of scheduling delays, especially under incasts, NegotiaToR also piggybacks a small volume of data with scheduling messages utilizing the unscheduled connections in the predefined phase, bypassing the scheduling delay especially for latency-sensitive mice flows.</p><p>NegotiaToR Matching enables scalable on-demand coordination among ToRs in a distributed manner.</p><p>NegotiaToR Matching achieves high performance in both goodput and FCT across these topologies.</p><p>NegotiaToR can still achieve good performance, showing the good generality of its design.</p><p>NegotiaToR achieves better mice flow FCT at all loads regardless of topologies.</p><p>With priority queues enabled, NegotiaToR’s FCT is consistently one to two orders of magnitude better.</p><p>Regarding goodput, NegotiaToR remarkably outperforms the traffic-oblivious scheme on both topologies at high loads.</p><p>NegotiaToR’s on-demand scheduling can better utilize the network’s bandwidth and reduce bandwidth waste.</p><p>NegotiaToR maintains comparable performance under identical parameter settings, where the performance on the thin-clos topology is marginally lower than on the parallel network due to its limited connectivity.</p><p>NegotiaToR incorporates a mechanism for handling link failures.</p><p>Upon detecting a link failure, ToRs will alert the maintainers and broadcast the detected failures to update their scheduling rules.</p><p>Upon link recovery, the bandwidth usage returns to its pre-failure level.</p><p>With data piggybacking in the predefined phase and mice flow priority queues in ToRs, mice flow FCT is reduced, most of which is even below the scheduling delay.</p><p>NegotiaToR converges to its present form, which is orchestrated towards a minimalist design.</p><p>We acknowledge that our exploration has not covered the entire design spectrum.</p><p>As DCN applications continue to develop, we believe there are opportunities for further optimizing NegotiaToR.</p><p>We presented NegotiaToR, an on-demand reconfigurable DCN architecture with a simple design.</p><p>With the two-phase epoch, it runs NegotiaToR Matching distributedly in-band to reconfigure the network according to dynamic real-time traffic demands.</p><p>It also provides an incast-optimized scheduling delay bypassing scheme to mitigate the impact of scheduling delays.</p><p>NegotiaToR is compatible with prevalent flat topologies, and is tailored towards a minimalist design for on-demand reconfigurable DCNs, enhancing practicality.</p><p>By exploiting the fast optical switching technology to provide high performance with low complexity, we hope that NegotiaToR can facilitate the development of next-generation reconfigurable DCNs.</p>",
        "abstract_gpt": "<p>Data center networks (DCNs) face increasing pressure from high-performance applications such as HPC, which require both high goodput and low latency. Conventional packet-switched DCNs are constrained by the limited capacity of switching chips, a problem exacerbated by the slowdown of Moore’s Law. While reconfigurable optical DCNs offer greater capacity and cost efficiency, traffic-oblivious designs relying on data relay often degrade performance. To address this challenge, we propose NegotiaToR, an on-demand reconfigurable DCN architecture that adapts optical links among top-of-rack (ToR) switches based on dynamic traffic demands. NegotiaToR employs a lightweight distributed scheduling mechanism using only binary demand information and an in-band control plane that enables ToRs to make local scheduling decisions. Its design integrates scheduled one-hop connections with unscheduled connections that piggyback small volumes of mice flows, effectively mitigating scheduling delays, particularly under incasts. Implemented with existing AWGRs and fast-tunable lasers, NegotiaToR remains compatible with flat topologies while maintaining minimal complexity. Simulation results demonstrate that NegotiaToR significantly outperforms state-of-the-art traffic-oblivious schemes, improving goodput and reducing flow completion time (FCT) by one to two orders of magnitude for mice flows. By combining scalability, practicality, and high performance, NegotiaToR advances the design of next-generation reconfigurable DCNs.</p>",
        "abstract_human": "<p>Recent advances in fast optical switching technology show promise in meeting the high goodput and low latency requirements of datacenter networks (DCN). We present NegotiaToR, a simple network architecture for optical reconfigurable DCNs that utilizes on-demand scheduling to handle dynamic traffic. In NegotiaToR, racks exchange scheduling messages through an in-band control plane and distributedly calculate non-conflicting paths from binary traffic demand information. Optimized for incasts, it also provides opportunities to bypass scheduling delays. NegotiaToR is compatible with prevalent flat topologies, and is tailored towards a minimalist design for on-demand reconfigurable DCNs, enhancing practicality. Through large-scale simulations, we show that NegotiaToR achieves both small mice flow completion time (FCT) and high goodput on two representative flat topologies, especially under heavy loads. Particularly, the FCT of mice flows is one to two orders of magnitude better than the state-of-the-art traffic-oblivious reconfigurable DCN design.</p>"
    },
    {
        "index": "33",
        "excerpt": "<p>The increasing workload diversity in modern data use cases has led to the proliferation of specialized data management systems, each targeted to a somewhat narrow class of workloads.</p> <p>Based on the “one size does not fit all” engine specialization tenet, hundreds of database system offerings were developed in the last few decades and are today available in the industry.</p> <p>While workloads, requirements, and environmental trends have dramatically evolved since the first databases were developed, our software development practices have not; data management systems continue to be, by and large, developed and distributed as vertically integrated monoliths.</p> <p>While modern specialized data systems may seem distinct at first, at the core, they are all composed of a similar set of logical components.</p> <p>However, this fragmentation and consequent lack of reuse across systems has slowed us down.</p> <p>It has forced developers to reinvent the wheel, duplicating work and hurting our ability to quickly adapt systems as requirements evolve.</p> <p>Our development model still leads to siloed systems, high maintenance costs, and wasted engineering cycles, suggesting we can be more efficient as an engineering community.</p> <p>More importantly, the byproducts of this fragmentation — incompatible SQL and non-SQL APIs, disparate functionality, distinct function packages, and inconsistent semantics across the board — impact the productivity of end users who are commonly required to interact with multiple distinct data systems to finish a particular task, each with their own quirks.</p> <p>We believe it is time for a paradigm shift.</p> <p>We envision that by decomposing data management systems into a more modular stack of reusable components, the development of new engines can be streamlined, while reducing maintenance costs and ultimately providing a more consistent user experience.</p> <p>By relying on a modular stack that reuses execution engine and language frontend, data systems code could provide a more consistent experience and semantics to users, from transactional to analytic systems, from stream processing to machine learning workloads.</p> <p>Given these trends, we foresee that composability is soon to cause another major disruption to how data management systems are designed.</p> <p>We foresee that monolithic systems will become obsolete, and give space to a new composable era for data management.</p> <p>We highlight the importance of composability in data management systems and argue that now is the right moment for a paradigm shift.</p> <p>We summarize previous work that describe individual projects in the composability space, their significance so far, and the investments required going forward.</p> <p>We extend the state-of-the-art by presenting a novel reference composable architecture, and discussing the parts of this stack that have received less attention but are key to component composability and reusability, highlighting open questions and areas that require additional research.</p> <p>Decomposing software complexity into smaller subsets of relatively independent components is a well-known software design technique.</p> <p>We believe that increasing the degree of composability in data management systems, by developing and adopting reusable components, provides the benefits discussed below.</p> <p>By reducing the duplication of work, more engineers could work on fewer systems and components.</p> <p>For large organizations, having fewer codebases reduces operational burden, and allows engineering teams to focus on new features, optimizations, and other enhancements.</p> <p>Data system diversity disincentivizes hardware vendors from investing in hardware support for data processing.</p> <p>By reusing the same components, from language to execution, users can expect consistent semantics across data systems, in addition to a more even set of available features.</p> <p>Libraries and frameworks are complicated and take time to learn.</p> <p>Developers are often passionate about writing their own code, and commonly find reading documentation and reviewing someone else’s code to be a tedious process.</p> <p>It is also common for developers to believe that a quick prototype containing a subset of functionality decreases the time-to-market for their products.</p> <p>In many cases, a component that provides the required functionality exists, but is written in the wrong programming language, has too many dependencies, is too hard to use, or is distributed under an incompatible license.</p> <p>Developers are usually not compelled to write reusable components because there are few incentives to do so.</p> <p>We believe the path forward is to focus on the following principles: (a) define and agree on a standard set of logical components across data management systems, (b) define stable (yet extensible) APIs for communication between these components, (c) provide canonical implementations for these components and APIs which are efficient and consistent, and (d) provide extensibility APIs in every layer of the stack to allow developers to implement specialized behavior.</p> <p>The new modular data stack emerging in the open source community provides a stronger separation between language and execution, in such a way that the execution is language-independent, and takes a well-defined and system-agnostic intermediate representation (IR) as input.</p> <p>This model is general enough, and allows every existing data management system to be mapped to it, from OLTP to OLAP systems, stream processing, log analytics, ML preprocessing and more.</p> <p>These components are predominantly consistent across specialized data management systems, and the areas where they specialize/diverge are the exception rather than the norm.</p> <p>Despite the current fragmentation, language frontend is the most straightforward layer in the stack to be made composable due to its simple API: translate user input into an IR.</p> <p>We believe that, more generally, language will meet execution through a unified IR.</p> <p>We believe language frontend modularization to also pave the way for language unification, or supporting a single unified SQL dialect, and a single unified dataframe dialect across data management systems.</p> <p>Intermediate representation (IR) is a term commonly used in the field of compilers to describe any structured representation of a program that carries enough information to allow it to be accurately executed.</p> <p>Substrait is a recent pioneering effort at providing a unified and cross-language IR specification.</p> <p>However, a few challenges arise as we work towards making this unification practical in real-life data systems.</p> <p>IRs will need to become part of the system’s external API.</p> <p>IRs are not yet descriptive enough to ensure runtime semantic equivalence.</p> <p>The set of functions available today in different systems are fully disparate.</p> <p>We believe the path forward to be, in the future, to bypass dialect and function packages incompatibilities through language and execution unification.</p> <p>Query optimization is a very diverse and well-studied field.</p> <p>While the majority of the industry query optimizers are tailored to the target database system, there has been a significant body of work related to building extensible and composable query optimizers.</p> <p>Execution is the layer responsible for taking a query fragment as input (represented by an IR), and executing it leveraging the local resources provided by the execution runtime.</p> <p>Execution is a highly fragmented domain, posing challenges to large organizations which are required to individually maintain dozens of siloed and specialized codebases due to user workload diversity.</p> <p>Velox is the first large-scale open source project aimed at providing a unified execution engine for data management systems.</p> <p>Velox demonstrates that it is possible not just to componentize execution, but also to unify it across stacks.</p> <p>The execution runtime provides the environment needed by the execution engine to perform the computation.</p> <p>Although only a few of these systems have become truly widely used, there is currently no standardization to the level of abstraction that strikes the best balance between ease of programming and tight control of execution.</p> <p>We believe one principle should hold true: runtimes should not be coupled with data management systems, and preferably be interchangeable through a standard API.</p> <p>We encourage developers to consider the described logical stack, and ask themselves: which parts of this stack am I planning to specialize?</p> <p>We believe composable is the future of data management, and hope more individuals and organizations will join us in this effort.</p>",
        "abstract_gpt": "<p>The growing diversity of data workloads has led to the proliferation of specialized, vertically integrated data management systems, resulting in redundant engineering effort, high maintenance costs, and inconsistent user experiences. Despite sharing a common set of core components, these siloed systems hinder composability and slow innovation. This work advocates for a shift toward a modular architecture, where reusable components with stable APIs enable consistent semantics across transactional, analytical, streaming, and machine learning workloads. A reference composable stack is proposed, highlighting challenges such as inconsistent intermediate representations, lack of runtime standardization, and limited incentives for reusable component development. The approach emphasizes language and execution unification through system-agnostic intermediate representations, and introduces Velox as an example of a unified execution engine. Composability in data systems promises reduced duplication, improved maintainability, and a more coherent user experience, paving the way for a more interoperable and efficient data management ecosystem.</p>",
        "abstract_human": "<p>The requirement for specialization in data management systems has evolved faster than our software development practices. After decades of organic growth, this situation has created a siloed landscape composed of hundreds of products developed and maintained as monoliths, with limited reuse between systems. This fragmentation has resulted in developers often reinventing the wheel, increased maintenance costs, and slowed down innovation. It has also affected the end users, who are often required to learn the idiosyncrasies of dozens of incompatible SQL and non-SQL API dialects, and settle for systems with incomplete functionality and inconsistent semantics. In this vision paper, considering the recent popularity of open source projects aimed at standardizing different aspects of the data stack, we advocate for a paradigm shift in how data management systems are designed. We believe that by decomposing these into a modular stack of reusable components, development can be streamlined while creating a more consistent experience for users. Towards that goal, we describe the state-of-the-art, principal open source technologies, and highlight open questions and areas where additional research is needed. We hope this work will foster collaboration, motivate further research, and promote a more composable future for data management.</p>"
    },
    {
        "index": "34",
        "excerpt": "<p>As the memory size of modern servers increasingly outpaces the TLB capacity, more TLB misses occur when running large-memory workloads, or when a process/thread migrates between NUMA nodes.</p><p>A TLB miss triggers a high overhead page-table walk.</p><p>If the page-table is in remote memory, the overhead is even higher, which we call page-table caused NUMA effect.</p><p>To mitigate the page-table caused NUMA effect, recent studies proposed page-table self-replication (PTSR).</p><p>The key idea is to replicate the page-tables of an application and make each NUMA node have the same replica.</p><p>Such PTSR is transparent to applications, indicating that PTSR can be enabled for an application without modifying it.</p><p>However, current PTSR techniques including Mitosis and vMitosis all involve manual decision.</p><p>Mitosis depends on users to manually enable PTSR in a system-wide or per-process manner.</p><p>It is extremely difficult for users to make such a decision because whether PTSR increases or decreases performance of an application depends on the application’s characteristics and the co-located applications.</p><p>To tackle this issue, we firstly carefully characterize the performance behavior of PTSR on different applications.</p><p>Subsequently, we identify three indicators including memory access rate (MAR), DTLB miss rate, and page-table access latency (PTL) for PTSR automation.</p><p>Finally, we design a hierarchical as well as gradual mechanism to leverage the three indicators to automatically enable/disable PTSR for an application to improve performance.</p><p>We call the proposed approach WASP.</p><p>WASP may make an application access remote page-tables when PTSR is enabled, as long as the latency is the shortest.</p><p>This is beneficial for an application’s performance when the application and other co-located ones contend for the local memory controller.</p><p>We devise a hierarchical as well as gradual mechanism based on the indicators to automatically enable/disable PTSR to reduce the page-table caused NUMA effect.</p><p>We evaluate WASP on x86 and ARM servers using 11 large-memory workloads.</p><p>The results show that WASP can achieve at least the same performance improvement as that obtained by manually enabling PTSR.</p><p>Paging is the current de facto standard for implementing virtual memory and page-tables are used to translate virtual addresses (VA) into physical addresses (PA).</p><p>To translate a memory address, the hardware walks over each level of the tree sequentially.</p><p>In virtualized environment, the number of memory accesses can be as large as 24 for an address translation.</p><p>On a NUMA architecture, page-tables of an application may locate at the memory of different NUMA nodes because of different memory allocation policies and thread migration made by OS.</p><p>Page-tables can be evenly distributed to all NUMA node memories with the interleave policy.</p><p>In such migration, the data pages also migrate with the help of AutoNUMA to the target node but its page-tables do not, making local page tables become remote ones.</p><p>Remote page table accessing is inevitable, degrading performance.</p><p>We employ the application GUPS to conduct experiments on Kunpeng 920 to compare its performance with and without PTSR enabled.</p><p>The performance of GUPS with RPI-LD is significantly lower (6.69×) than baseline.</p><p>When PTSR is enabled, the performance of GUPS with RPI-LD can be significantly improved, and is extremely close to the baseline.</p><p>This implies that PTSR can almost remove the page-table caused NUMA effect.</p><p>However, our experiments show that enabling PTSR may also significantly hurt performance in other cases.</p><p>This experiment demonstrates that system-wide PTSR can not remove the page-table caused NUMA effect in all cases.</p><p>The per-process PTSR is preferred.</p><p>However, it depends on an application’s characteristics as well as the complex co-location situation, which is hard for an end user to know.</p><p>Automatically enabling or disabling PTSR for a process is therefore desired, which motivates this work.</p><p>WASP may access the local or remote page-table replica depending on latency.</p><p>We propose three indicators to enable/disable PTSR: memory access rate (MAR), DTLB miss rate, and page-table access latency (PTL).</p><p>These indicators are easily available as well as lightweight on most processor architectures such as x86_64 and ARM64.</p><p>We devise a hierarchical as well as gradual mechanism to automatically enable PTSR for an application to improve performance.</p><p>At Step1, WASP measures MAR and compares it with a threshold.</p><p>If MAR is less than or equal to the threshold, WASP disables PTSR.</p><p>If DTLB miss rate is larger than the threshold, WASP proceeds to enable PTSR.</p><p>If the local PTL is the shortest, WASP makes the application access the local page-table replica.</p><p>Otherwise, it accesses the remote replica that has the shortest latency.</p><p>WASP implementation on x86_64 employs two performance counters, mem-loads and mem-stores, to calculate MAR.</p><p>For DTLB miss rate, we use four counters.</p><p>We now present how to apply WASP on an application running on a NUMA server.</p><p>WASP calls the system call “perf_event_open()” to measure performance events.</p><p>WASP then calls a newly developed syscall to enable PTSR if conditions are met.</p><p>WASP improves the overall performance of the workload.</p><p>The performance of applications with WASP is at least the same as that of them with manually enabled PTSR.</p><p>WASP does not degrade the performance of any application.</p><p>WASP can make the right decision for any application to guarantee non-performance-degradation.</p><p>WASP can help most users.</p><p>WASP can improve the performance of some applications compared to that of them with manually enabled PTSR.</p><p>For the first-touch policy, the geometric mean of the speedups brought by WASP over the manually enabled PTSR is 27%.</p><p>WASP still outperforms manual PTSR with large pages.</p><p>The geometric mean of speedups made by WASP over manual PTSR is 15%.</p><p>WASP always shows the same or better performance for the experimented applications with 4KB and 2MB pages.</p><p>This indicates WASP can also work well on x86 NUMA servers.</p><p>Compared to RPI-LD, WASP can improve the performance of GUPS by as high as 6.69×.</p><p>WASP can achieve the same performance as that obtained by the manual PTSR technique.</p><p>WASP successfully handles the local memory controller contention.</p><p>WASP can achieve higher performance than PTSR.</p><p>WASP can be at least used to improve the performance of AutoNUMA.</p><p>WASP does not cause high overheads for memory management operations in practice.</p><p>The general overhead of WASP comes from three parts.</p><p>WASP only adds 0.72% and 0.39% overheads for GUPS and Redis.</p><p>WASP can automatically improve the overall performance of popular data center applications by up to 1.36×.</p><p>WASP can improve the performance of workloads in multi-NUMA-node case by up to 1.82× and that in process migration case by up to 6.69×.</p>",
        "abstract_gpt": "<p>As modern servers scale their memory capacity, TLB misses have become increasingly frequent, especially in large-memory workloads and under thread migration across NUMA nodes. These misses incur high overhead due to remote page-table accesses—a phenomenon we term the page-table caused NUMA effect. While Page-Table Self-Replication (PTSR) mitigates this issue by replicating page-tables across nodes, existing solutions like Mitosis and vMitosis require manual configuration and can degrade performance when misapplied. To address this, we propose WASP, a lightweight, automatic mechanism that uses three hardware-accessible indicators—memory access rate (MAR), DTLB miss rate, and page-table access latency (PTL)—to dynamically enable or disable PTSR per application. WASP applies a hierarchical and gradual decision strategy to select the optimal replica (local or remote) based on latency, ensuring performance improvements even under memory controller contention. We evaluate WASP on x86 and ARM64 servers using 11 large-memory workloads and show that it achieves the same or better performance than manual PTSR, with up to 6.69× improvement and less than 1% overhead. WASP guarantees non-degradation, improves AutoNUMA behavior, and adapts effectively to complex co-location scenarios. Our results demonstrate that WASP is a practical, architecture-independent solution for managing page-table locality in NUMA systems.</p>",
        "abstract_human": "<p>Recently, page-table self-replication (PTSR) has been proposed to reduce the page-table caused NUMA effect for large-memory workloads on NUMA servers. However, PTSR may improve or hurt performance of an application, depending on its characteristics and the co-located applications. This is hard for users to know, but current PTSR can only be manually enabled/disabled by users. <p>To address this issue, we propose WASP (Workload-Aware Self-Replication) to automatically enable/disable PTSR to reduce the page-table caused NUMA effect. WASP innovates two techniques. First, it identifies a set of indicators, which are generally available on most processor architectures, to indicate if PTSR should be enabled/disabled. Second, WASP devises a hierarchical as well as gradual mechanism using these indicators to enable/disable PTSR automatically for aspecific workload to reduce the page-table caused NUMA effect during its execution. </p> <p>We implement WASP in Linux and evaluate it on x86 and ARM NUMA servers. Experimental results show that WASP can automatically enable/disable PTSR successfully according to workload characteristics, achieving at least the same performance improvement as that obtained by manually enabling PTSR on both x86 and ARM NUMA servers.</p>"
    },
    {
        "index": "35",
        "excerpt": "<p>With the advancement of artificial intelligence techniques and supercomputer performance, numerical software with extensive use of floating-point (FP) arithmetic has become increasingly prevalent, accompanied by a rapid escalation in power consumption.</p><p>Unfortunately, designing compute-intensive applications that are both reliable and energy-efficient remains a significant challenge in recent years.</p><p>Although high precision guarantees program accuracy and reliability, it may also compromise efficiency and result in unnecessary energy consumption.</p><p>A trade-off between accuracy and performance is often achieved by mixed precision, i.e., performing different operations in different precisions.</p><p>Automated precision tuning is regarded as a promising direction for finding mixed-precision programs that achieve the best trade-off between performance and accuracy.</p><p>Precision tuning entails replacing the original precision assigned to FP variables in numerical programs with lower precision in a manner that ensures accuracy standards are maintained.</p><p>However, it is non-trivial to reason about mixed precision due to the higher potential for numerical errors arising from minor changes in the precision of FP variables.</p><p>Existing automated precision tuners mainly use either static analysis or dynamic search-based approaches.</p><p>Although static approaches are generally sound and do not require executing programs with input data, they are restricted to FP expressions or small programs and unable to tune large codes with conditionals and loops.</p><p>On the other hand, dynamic search-based approaches have been applied to larger-scale numerical programs but require running numerous mixed-precision program versions.</p><p>Dynamic approaches are time-intensive and face the challenge of an exponential search space of mixed-precision programs.</p><p>As far as we are aware, all search-based precision tuners suffer from scalability issues when applied to large HPC programs.</p><p>In this paper, we present FPLearner, a Machine Learning (ML) based approach to learn the representation of floating-point mixed-precision programs for predicting their performance and computation accuracy.</p><p>Our insight is straightforward: reducing the number of program runs required during the search by automatically predicting “promising” mixed-precision programs.</p><p>We propose a novel GNN-based approach to learn features from a customized graph representation, named Precision Interaction Graph (PIG), which is designed to represent mixed-precision programs by modeling interactions of precision among FP variables across the program.</p><p>To overcome the challenge, we innovatively deploy a Gated Graph Neural Network (GGNN) architecture to capture long dependencies among FP operations in such programs.</p><p>We build a dataset with 1228 mixed-precision programs from five representative HPC applications.</p><p>Our models are effective at accurately predicting both execution performance (96.34% F1 score) and computation accuracy (97.03% F1 score), outperforming other baseline methods.</p><p>The results show that our models improve the efficiency of precision tuners by an average of 25.54% and up to 61.07% in time cost while generating a mixed-precision program of comparable or better quality.</p><p>Dynamic Precision Tuning. Given a target FP program, the dynamic FP precision tuning process seeks to find a lower-precision variant of the program, often a mixed-precision program, that improves performance while adhering to specified computation accuracy constraints.</p><p>The majority of existing precision tuners rely on a search-based approach with a trial-and-fail paradigm.</p><p>Despite their potential benefits, dynamic precision tuners face significant scalability challenges.</p><p>An Example of Precision Tuning. We present a motivating example of precision tuning on LULESH version 2.0, a proxy application developed at Lawrence Livermore National Laboratory.</p><p>If we assume each mixed-precision program version of LULESH also takes around 18 seconds, then evaluating all possible mixed-precision programs would take approximately 3.76 × 10¹⁰⁷ hours.</p><p>This is a large amount of time compared to the running time of the target program, and limits the scalability of existing search-based precision tuners.</p><p>This motivates the need for predicting the performance and accuracy of mixed-precision programs.</p><p>Our goal is to train models that predict if a mixed-precision version of a given initial FP program (i) achieves performance speedup with respect to the initial program, and (ii) produces a result within a predefined error threshold.</p><p>FPLearner analyzes a mixed-precision program and extracts the necessary information to build a graph representation, named Precision Interaction Graph (PIG).</p><p>Representing programs is challenging due to the abundance of structural information contained within them, which cannot be effectively captured by conventional text-based representations.</p><p>FPLearner utilizes the Abstract Syntax Tree (AST) as the backbone of a PIG and extracts FP arithmetic related features of the nodes in the AST to obtain their initial representation.</p><p>The final PIG serves as input to the second stage of FPLearner for graph-level prediction tasks.</p><p>We create the initial node representation using three node features that are most relevant to FP characteristics: the node’s type, its precision (if applicable), and the name of the operator (if applicable).</p><p>We use word2vec to encode each feature and concatenate the three encodings together into a fixed-length vector to initialize the node representation.</p><p>FPLearner creates four additional types of edges using the graph backbone: TypeCasting, AssignedFrom, Control Flow, and Program Dependence.</p><p>FPLearner constructs the TypeCasting edges based on our observation that excessive type castings can have adverse effects on program performance.</p><p>The use of AssignedFrom edges is motivated by a prior study that leverages variable dependence in FP arithmetic assignments to model programs.</p><p>To build the PIG across a wider range of contexts, we employ two classic program analysis techniques: control flow analysis and program dependence analysis.</p><p>Data dependence (DD) edges are a type of program dependence edge that is created by calculating reaching definitions for each statement and predicate.</p><p>The second stage of FPLearner uses a GNN architecture to learn features on the input graph PIG.</p><p>We train separate models for the performance and accuracy prediction respectively.</p><p>We formulate the PIG as a multi-relational graph, which is a type of information network.</p><p>We set the training epochs as 500, and use the early stopping manner with the patience set to 30 epochs.</p><p>Our approach achieves 96.34% F1 score on the performance prediction task and 97.03% on the accuracy prediction task.</p><p>Compared to using all types of edges, excluding any one type of edge decreases the accuracy score by 5.46%–12.55% for performance prediction, and 4.69%–8.60% for accuracy prediction.</p><p>Each type of edge provides a distinct context for learning the FP precision interactions.</p><p>We evaluate our models on two dynamic precision tuners: Precimonious and HiFPTuner.</p><p>Across all case studies, FPLearner achieves a 35.14%–62.32% reduction in program runs compared to the total number of programs in the search.</p><p>When compared to Vanilla Precision Tuners, using FPLearner successfully reduces program runs by 57.34%–65.98%</p><p>Total time cost reductions are observed in all cases (17.18%–61.07%) except for MG.</p><p>Using FPLearner yields comparable or slightly superior results in terms of final program speedup.</p><p>Fine-tuning our pre-trained models on all target programs yields a substantial improvement in model performance of up to 31.9% compared to training from scratch.</p><p>FPLearner w/o FT requires 3.83× training cost on average than our fine-tuned FPLearner.</p><p>Our work is the first to utilize a DL-based approach to replace program execution with a model prediction to improve the efficiency of precision tuning.</p><p>FPLearner is the first to predict both performance and accuracy of numerical software that uses mixed precision.</p><p>We proposed a distinct graph representation, PIG, specialized for such programs by modeling precision interactions among FP variables across the program.</p><p>We presented FPLearner, a novel approach for predicting the performance and accuracy of mixed-precision programs.</p><p>FPLearner substantially improved time efficiency in two dynamic precision tuners, Precimonious and HiFPTuner, boasting an average enhancement of 25.54% and reaching up to 61.07%, all while maintaining precision tuning results of comparable or superior quality.</p>",
        "abstract_gpt": "<p>Designing numerical software that balances energy efficiency and computational accuracy remains a major challenge due to the trade-offs inherent in floating-point (FP) precision. While mixed-precision programming offers a viable solution, existing automated precision tuning techniques—particularly dynamic search-based approaches—struggle with scalability due to the vast search space. To address this, we introduce FPLearner, a machine learning-based system that predicts the performance and accuracy of mixed-precision programs, significantly reducing the number of program executions needed. FPLearner uses a novel graph representation called the Precision Interaction Graph (PIG) to model the interactions among FP variables. This representation incorporates features derived from the abstract syntax tree and augments them with multiple edge types capturing type casting, data dependencies, control flow, and program structure. A Gated Graph Neural Network is trained on a dataset of 1,228 mixed-precision programs across five high-performance computing (HPC) applications. FPLearner achieves high predictive performance (96.34% F1 for execution speed, 97.03% F1 for accuracy) and reduces search time by an average of 25.54%, reaching up to 61.07%, while preserving or improving final program quality. This work represents the first deep learning-based framework capable of accurately and jointly predicting the performance and correctness of mixed-precision numerical programs.</p>",
        "abstract_human": "<p>A mixed-precision program is a floating-point program that utilizes different precisions for different operations, providing the opportunity of balancing the trade-off between accuracy and performance. Precision tuning aims to find a mixed-precision version of a program that improves its performance while maintaining a given accuracy. Unfortunately, existing precision tuning approaches are either limited to small-scale programs, or suffer from efficiency issues. In this paper, we propose FPLearner, a novel approach that addresses these limitations. Our insight is to leverage a Machine Learning based technique, Graph Neural Networks, to learn the representation of mixed-precision programs to predict their performance and accuracy. Such prediction models can then be used to accelerate the process of dynamic precision tuning by reducing the number of program runs. We create a dataset of mixed-precision programs from five diverse HPC applications for training our models, which achieve 96.34% F1 score in performance prediction and 97.03% F1 score in accuracy prediction. FPLearner improves the time efficiency of two dynamic precision tuners, Precimonious and HiFPTuner, by an average of 25.54% and up to 61.07% while achieving precision tuning results of comparable or better quality.</p>"
    },
    {
        "index": "36",
        "excerpt": "<p>Diffusion models (DMs) are powerful generative models that demonstrate promising performance across various domains.</p><p>Motivated by their remarkable capability in complex distribution modeling and conditional generation, researchers have developed a series of works applying diffusion models for decision-making tasks in recent years.</p><p>DMs can play various roles in decision-making tasks, such as acting as planners to make better decisions from a long-term perspective, serving as policies to support complex multimodal-distribution modeling, and working as data synthesizers to assist reinforcement learning (RL) training.</p><p>Among these roles, diffusion planning is the most widely applied paradigm.</p><p>Unlike auto-regressive planning in previous model-based RL approaches, diffusion planning avoids severe compounding errors by directly generating the entire trajectory rather than one-step transition.</p><p>Also, its powerful conditional generation capability allows planning at the trajectory level without being limited to step-wise shortsightedness.</p><p>The diffusion planning paradigm has achieved state-of-the-art (SOTA) performance in various offline RL tasks, including single-agent RL, multi-agent RL, meta RL, and more.</p><p>One key issue that diffusion planning faces is the expensive iterative sampling cost.</p><p>The low decision frequency is primarily attributed to modeling a denoising process for a long-horizon trajectory distribution, which requires a heavy neural network backbone and multiple forward passes.</p><p>Experimental results indicate that it is not, for the detailed trajectory information in long-horizon segments is highly redundant.</p><p>Besides, in practice, agents often struggle to reach the planned distant state.</p><p>These facts argue that while long-horizon planning helps improve foresight, it introduces redundant information in distinct parts.</p><p>Ignoring the modeling of these redundant parts in the diffusion planning process will significantly reduce the complexity of the trajectory distribution to be fitted, making it possible to build a fast and lightweight diffusion planning framework.</p><p>Motivated by these insights, we propose to build a plan refinement process (PRP) to speed up diffusion planning.</p><p>First, we perform “rough” planning, where jumpy planning is executed, only considering the states at intervals that are far apart and ignoring other individual states.</p><p>Then, we refine a small portion of the plan, focusing on the steps closer to the current state.</p><p>This approach has three advantages: 1) It reduces the length of the sequences generated by the diffusion model, simplifying the complexity of the probability distribution to be fitted.</p><p>2) It significantly reduces the search space of plans, making it easier for the planner to find well-performed trajectories.</p><p>3) Since only the first action of each step is executed, rough planning of steps further away causes no noticeable performance drop.</p><p>Diffusion planning with PRP, which we call DiffuserLite, is simple, fast, and lightweight.</p><p>Our experiments have demonstrated the effectiveness of PRP, significantly increasing decision-making frequency while achieving SOTA performance.</p><p>Moreover, it can be easily adapted to other existing diffusion planning methods.</p><p>This paradigm relies on multiple forwarding complex neural networks, resulting in extremely low decision-making frequencies (typically 1-10Hz, or even less than 1Hz), severely hindering its real-world deployment.</p><p>Both facts indicate that terms in distant parts of a plan become increasingly redundant, whereas the closer parts are more crucial.</p><p>Specifically, our proposed PRP consists of L planning levels.</p><p>By this design, only the first planned intervals are refined in the next level, and the other redundant details are all ignored, resulting in a coarse-to-fine generation process.</p><p>PRP ensures that long-term planning maintains foresight while alleviating the burden of modeling redundant information.</p><p>The absence of redundant details in PRP allows for a significant reduction in the complexity of the fitted distribution at each level.</p><p>This reduction in complexity enables us to utilize a lighter neural network backbone, shorter network input sequence lengths, and a reduced number of denoising steps.</p><p>Key points generated at former levels often sufficiently reflect the quality of the entire trajectory, which allows the planner to focus more on finding distant key points and planning actions for the immediate steps, reducing search space and complexity.</p><p>Employing PRP results in a new lightweight architecture for diffusion planning, which we refer to as DiffuserLite.</p><p>DiffuserLite can reduce the complexity of the fit distribution and significantly increase the decision-making frequency, achieving 122Hz on average for the need of real-time control.</p><p>We employ DiT as the noise predictor backbone, instead of the more commonly used UNet due to the significantly reduced length of the generated sequences in each level (typically around 5).</p><p>For conditional sampling, we utilize CFG instead of CG, as the slow gradient computation process of CG reduces the frequency of decision-making.</p><p>The Critic C in DiffuserLite plays two important roles: providing generation conditions during the diffusion training process and selecting the optimal plan from the candidates generated by the diffusion model during inference.</p><p>To address this challenge, we introduce an option to use the sum of discounted rewards and the value of the last state as an additional property design.</p><p>After obtaining the optimal trajectory from the last level through critic selection, we utilize an additional inverse dynamic model at = h(ot, ot+1) to extract the action to be executed.</p><p>DiffuserLite aims to achieve real-time diffusion planning to support its application in real-world scenarios.</p><p>Therefore, we introduce Rectified flow for further increasing the decision-making frequency.</p><p>We explored the performance of the DiffuserLite on various tasks on D4RL, Robomimic, and FinRL.</p><p>The runtime cost of DiffuserLite with D, R1, and R2 backbones is only 1.23%, 0.89%, and 0.51% of the average runtime cost of Diffuser and DD, respectively.</p><p>These improvements are attributed to ignoring redundant information in PRP, which reduces the complexity of the distribution that the backbone generative model needs to fit, allowing us to employ a light neural network backbone and use fewer sampling steps to conduct perfect-enough planning.</p><p>DiffuserLite continues to exhibit its superiority in these real-world tasks, achieving performance comparable to SOTA algorithms.</p><p>This illustrates the potential application of DiffuserLite in real-world scenarios.</p><p>AlignDiff-Lite achieves a 560% improvement in decision-making frequency compared to AlignDiff, while only experiencing a small performance drop of 3.2%.</p><p>This result demonstrates the potential of DiffuserLite serving as a plugin to accelerate diffusion planning across various domains.</p><p>The notable performance drop demonstrates the importance of a long-enough planning horizon.</p><p>The large standard deviation and the significant performance drop provide strong evidence for the limitations of one-shot generation planning, having difficulties in modeling the distribution of detailed long-horizon trajectories.</p><p>However, DiffuserLite can maintain high performance with fast decision-making frequency due to its lite architecture and simplified fitted distribution.</p><p>DiffuserLite has more clean plugin design and undoubtedly contributes to increasing decision-making frequency and performance.</p>",
        "abstract_gpt": "<p>Diffusion models have shown great promise in decision-making tasks, particularly in offline reinforcement learning, due to their ability to model complex distributions and generate entire trajectories. However, their high computational cost and low decision-making frequency hinder real-world deployment. To address this, we propose DiffuserLite, a lightweight diffusion planning framework that incorporates a Plan Refinement Process (PRP). PRP begins with rough, interval-based trajectory planning and refines only portions near the current state, reducing sequence length and distribution complexity. This hierarchical, coarse-to-fine strategy enables faster inference, simplified network design, and efficient sampling. DiffuserLite achieves up to 122Hz decision-making frequency, significantly outperforming prior diffusion-based planners in both speed and efficiency, with minimal performance trade-offs. Empirical evaluations on D4RL, Robomimic, and FinRL benchmarks confirm its state-of-the-art competitiveness and real-time applicability. DiffuserLite also demonstrates versatility as a plugin for existing diffusion planners, offering a 560% speed improvement over AlignDiff with only a 3.2% performance drop. These results highlight the potential of PRP-driven lightweight planning in bridging the gap between high-performance trajectory modeling and real-time decision-making.</p>",
        "abstract_human": "<p>Diffusion planning has been recognized as an effective decision-making paradigm in various domains. The capability of generating high-quality long-horizon trajectories makes it a promising research direction. However, existing diffusion planning methods suffer from low decision-making frequencies due to the expensive iterative sampling cost. To alleviate this, we introduce DiffuserLite, a super fast and lightweight diffusion planning framework, which employs a planning refinement process (PRP) to generate coarse-to-fine-grained trajectories, significantly reducing the modeling of redundant information and leading to notable increases in decision-making frequency. Our experimental results demonstrate that DiffuserLite achieves a decision-making frequency of 122.2Hz (112.7x faster than predominant frameworks) and reaches state-of-the-art performance on D4RL, Robomimic, and FinRL benchmarks. In addition, DiffuserLite can also serve as a flexible plugin to increase the decision-making frequency of other diffusion planning algorithms, providing a structural design reference for future works. More details and visualizations are available at project website.</p>"
    },
    {
        "index": "37",
        "excerpt": "<p>Depth is crucial for 3D perception in various downstream applications, such as autonomous driving, augmented and virtual reality, and robotics.</p><p>However, sensor-based depth measurement is far from perfect.</p><p>Such measurements often exhibit sparsity, low resolution, noise interference, and incompleteness.</p><p>Various factors, including environmental conditions, motion, sensor power constraints, and the presence of specular, transparent, wet, or non-reflective surfaces, contribute to these limitations.</p><p>Consequently, the task of depth completion, aimed at generating dense and accurate depth maps from sparse measurements alongside aligned camera images, has emerged as a pivotal research area.</p><p>Thanks to the advances in deep learning, there has been significant progress in depth completion.</p><p>Earlier papers leverage convolutional neural networks to perform depth completion with image guidance and achieve promising results.</p><p>In order to improve accuracy, researchers have studied various spatial propagation methods, which performs further iterative processing on top of depth maps and features computed by an initial network.</p><p>These propagation algorithms, however, focus on 2D feature processing and do not fully exploit the 3D nature of the problem.</p><p>A few recent papers utilize transformers for depth completion.</p><p>However, they apply transformer operations mainly to improve feature learning on the 2D image plane and fail to achieve acceptable accuracy without employing spatial propagation.</p><p>Several studies have looked into harnessing 3D representation more comprehensively.</p><p>For instance, construct a point cloud from the input sparse depth, yet coping with extreme sparsity poses challenges in effective feature learning.</p><p>Another approach, as seen in, uplifts 2D features to 3D by using the initial dense depth predicted by a simple convolutional network, but it is impeded by the poor accuracy of the initial network and requires dynamic propagations to attain acceptable accuracy.</p><p>Very recently, researchers have proposed employing transformers for 3D feature learning in depth completion; however, this work applies transformer layers to extremely sparse points, which is ineffective for learning informative 3D features.</p><p>Here, we introduce DeCoTR to perform feature learning in full 3D.</p><p>It accomplishes this by constructing a dense feature point cloud derived from completed depth values obtained from an initial network and subsequently applying transformer processing to these 3D points.</p><p>To do this properly, it is essential to have reasonably accurate initial depths.</p><p>As such, we first enhance a commonly used convolution-based initial depth network, S2D, by integrating transformer layers on bottleneck and skip connection features.</p><p>This upgraded model, termed S2D-TR, achieves significantly improved depth accuracy, on par with state-of-the-art models, without requiring any iterative spatial propagation.</p><p>Given the initial depth map, we uplift 2D features to 3D to form a point cloud, which is subsequently processed by transformer layers, to which we refer as 3D-TR layers.</p><p>Prior to feeding the points to transformer layers, we normalize them, which regularizes the 3D feature learning and leads to better accuracy.</p><p>To facilitate long-range contextual understanding, we additionally incorporate global attention on lower-scale versions of the point cloud.</p><p>Finally, 3D features are projected back to the 2D image plane and consumed by a decoder to produce the final depth prediction.</p><p>We present DeCoTR, a novel transformer-based approach to perform full 3D feature learning for depth completion.</p><p>In order to properly do this, we upgrade the commonly used initial network S2D, by enhancing its bottleneck and skip connection features using transformers.</p><p>We devise useful techniques to normalize the uplifted 3D feature point cloud, which improves the model learning.</p><p>Through extensive evaluations on standard benchmarks, NYU Depth v2 and KITTI, we demonstrate the efficacy of DeCoTR and show that it sets the new SOTA.</p><p>Early depth completion approaches rely solely on the sparse depth measurements to estimate the dense depth.</p><p>Since these methods do not utilize the image, they usually suffer from artifacts like blurriness, especially at object boundaries.</p><p>Later, image-guided depth completion alleviates these issues by incorporating the image.</p><p>S2D, one of the first papers on this, leverages a convolutional network to consume both the image and sparse depth map.</p><p>Subsequent papers design more sophisticated convolutional models for depth completion.</p><p>In order to enhance depth quality, researchers have studied various spatial propagation algorithms.</p><p>These solutions utilize depth values and features given by an initial network, and performs iterative steps to mix and aggregate features on the 2D image plane.</p><p>While existing solutions predominately propose architectures to process features on 2D, several works explore 3D representations.</p><p>One of these works, GraphCSPN, employs S2D as an initial network to generate the full depth map, before creating a denser point cloud and performing feature learning on it.</p><p>However, this is limited by the insufficient accuracy of the initial depths by S2D and still needs iterative processing to achieve good accuracy.</p><p>More related to our paper are those that leverage vision transformers for depth completion, such as CompletionFormer and GuideFormer.</p><p>While they demonstrate the effectiveness of using vision transformers for depth completion, their feature learning is only performed on the 2D image plane.</p><p>A very recent paper, PointDC, proposes to apply transformer to 3D point cloud in the depth completion pipeline.</p><p>However, PointDC operates on very sparse points, which makes it challenging for learning 3D features.</p><p>Given aligned sparse depth map and an RGB image, the goal of image-guided depth completion is to recover a dense depth map based on the sparse input and with semantic guidance from the image.</p><p>It is a common approach to employ early fusion between the depth and RGB modalities.</p><p>The early-fusion architecture of S2D has been commonly used by researchers as a base network to predict an initial completed depth map.</p><p>This architecture, however, has limited accuracy and may provide erroneous depth values for subsequent operations.</p><p>As such, we propose to leverage self-attentions to enhance the S2D features.</p><p>Our upgraded version of S2D with efficient attention enhancement, denoted as S2D-TR, provides significantly improved accuracy while having better efficiency than latest transformer-based depth completion models.</p><p>Considering the 3D nature of depth completion, it is important for the model to properly exploit 3D geometric information when processing the features.</p><p>Given the large number of 3D points uplifted from the 2D feature map, it is computationally intractable to perform attention on all the points simultaneously.</p><p>As such, we adopt a neighborhood-based attention, by finding the K-Nearest-Neighboring points for each point in the point cloud.</p><p>We perform such 3D cross-attention in multiple transformer layers, to which we refer as 3D-TR layers.</p><p>Point cloud normalization: We normalize the constructed point cloud from S2D-TR outputs into a unit ball, before proceeding to the 3D attention layers.</p><p>We find this technique effectively improves depth completion, as we shall show in the experiments.</p><p>To enable global understanding while keeping computation costs under control, we propose to perform global 3D cross-attention only on a downsampled point set.</p><p>We apply global attention after the local neighborhood-based attentions.</p><p>We train DeCoTR with a masked ℓ1 loss between the final completed depth maps and the ground-truth depth maps.</p><p>We conduct extensive experiments to evaluate our proposed DeCoTR on standard depth completion benchmarks and compare with the latest state-of-the-art solutions.</p><p>We further perform zero-shot evaluation to assess the model generalizability and carry out ablation studies to analyze different parts of our proposed approach.</p><p>Our proposed DeCoTR approach sets the new SOTA performance, with the lowest RMSE of 0.086 outperforming all existing solutions.</p><p>Specifically, our DeCoTR considerably outperforms latest SOTA methods that also leverage 3D representation and/or transformers, such as GraphCSPN, PointDC, and CompletionFormer.</p><p>DeCoTR achieves SOTA depth completion accuracy and is among the top-ranking methods on KITTI-DC leaderboard.</p><p>This indicates that DeCoTR has the right combination of dense 3D representation and transformer-based learning.</p><p>We see that DeCoTR generalizes better to unseen datasets when comparing to existing SOTA models.</p><p>DeCoTR has significantly lower depth errors as compared to both NLSPN and CompletionFormer.</p><p>This study verifies the usefulness of our proposed components and techniques.</p><p>This confirms the importance of getting more accurate initial depth before applying 3D feature learning.</p><p>Through extensive experiments, we have shown that DeCoTR achieves SOTA performance on standard benchmarks like NYUD-v2 and KITTI-DC.</p><p>Furthermore, zero-shot evaluation on unseen datasets such as ScanNet and DDAD shows that DeCoTR has better generalizability as compared to existing methods.</p>",
        "abstract_gpt": "<p>Depth completion, which aims to generate dense depth maps from sparse measurements and aligned RGB images, is vital for 3D perception in applications such as autonomous driving and robotics. Existing methods either rely on 2D image-plane processing or struggle to exploit 3D geometry effectively due to sparse inputs or inaccurate initial predictions. This paper introduces DeCoTR, a novel approach that performs full 3D feature learning using transformers. DeCoTR begins by enhancing the commonly used S2D depth completion network with self-attention, resulting in S2D-TR, which provides more accurate initial depth estimates without iterative propagation. The method uplifts 2D features into a dense 3D point cloud, applies neighborhood-based and global cross-attention via transformer layers (3D-TR), and incorporates point cloud normalization to stabilize learning. The final 3D features are projected back to 2D for depth prediction. DeCoTR achieves state-of-the-art performance on NYUD-v2 and KITTI-DC benchmarks and demonstrates strong generalization to unseen datasets like ScanNet and DDAD. These results highlight the effectiveness of combining dense 3D representation with transformer-based learning for accurate and robust depth completion.</p>",
        "abstract_human": "<p>In this paper, we introduce a novel approach that harnesses both 2D and 3D attentions to enable highly accurate depth completion without requiring iterative spatial propagations. Specifically, we first enhance a baseline convolutional depth completion model by applying attention to 2D features in the bottleneck and skip connections. This effectively improves the performance of this simple network and sets it on par with the latest, complex transformer-based models. Leveraging the initial depths and features from this network, we uplift the 2D features to form a 3D point cloud and construct a 3D point transformer to process it, allowing the model to explicitly learn and exploit 3D geometric features. In addition, we propose normalization techniques to process the point cloud, which improves learning and leads to better accuracy than directly using point transformers off the shelf. Furthermore, we incorporate global attention on downsampled point cloud features, which enables long-range context while still being computationally feasible. We evaluate our method, DeCoTR, on established depth completion benchmarks, including NYU Depth V2 and KITTI, showcasing that it sets new state-of-the-art performance. We further conduct zero-shot evaluations on ScanNet and DDAD benchmarks and demonstrate that DeCoTR has superior generalizability compared to existing approaches.</p>"
    },
    {
        "index": "38",
        "excerpt": "<p>While the use of large-scale language models (LMs) and vision-language models (VLMs), pretrained on a massive amount of data, is becoming a dominant paradigm in machine learning.</p><p>Prompt tuning has emerged as a promising low-cost solution, discovering input prompts that effectively guide the pre-trained models to generate the desired outputs, while keeping the model parameters frozen.</p><p>Prompt tuning is generally categorized into two approaches, soft and hard prompting methods, based on their representation of prompts.</p><p>Soft prompt methods primarily focus on learning continuous embedding vectors at the token level, which are called as soft prompts.</p><p>However, the prompts learned through soft tuning are opaque to human interpretation and are not compatible with other pre-trained models that do not share the same embedding spaces.</p><p>These limitations necessitate an alternative approach: discovery of the prompts composed of human-readable discrete tokens, referred to as hard prompts.</p><p>Hard prompts offer numerous advantages over soft prompts: they are transferable from one pre-trained model to another since they are agnostic to the embedding.</p><p>Despite the benefits, they require large-scale discrete optimization in principle.</p><p>Reinforcement learning (RL) serves as a powerful alternative tool for optimization, as exemplified by RLPrompt that uses soft Q-learning.</p><p>One of the key ideas behind RLPrompt is an efficient parameterization leveraging a frozen pretrained LM.</p><p>We address this limitation in a principled manner.</p><p>Our approach leverages sparse Tsallis entropy regularization for RL to ignore very unlikely tokens from consideration.</p><p>We demonstrate the effectiveness of our algorithm across various tasks, including few-shot text classification, unsupervised text style transfer, and textual inversion from images, comparing it against various baselines.</p><p>Unlike hard prompts learned by baselines which are often referred to as the ‘secret language’ of models due to their opacity for human interpretation, our learned prompts are more natural and straightforward.</p><p>We propose a principled solution to the problem using sparse Tsallis entropy, referred to as PIN (Prompts made INterpretable).</p><p>Pioneering work by Brown et al. highlighted the efficacy of using prompts for task adaptation in pretrained language models, a technique now commonly referred to as instruction tuning.</p><p>Despite its success, the automated generation of effective text prompts, particularly hard prompts, remains a challenge.</p><p>AutoPrompt is an initial framework for discrete prompt optimization in transformer-based language models, inspiring a range of diverse methods.</p><p>RLPrompt introduces an efficiently parameterized network that maps continuous embedding vectors to adaptive vectors within the same space.</p><p>Despite its simplicity, RLPrompt struggles with accurately representing Q-values across all tokens, potentially leading to sub-optimal prompts.</p><p>Hard prompt tuning is the process of discovering an optimal prompt within the token space V, to efficiently tackle specific downstream tasks.</p><p>The reward function measures the appropriateness of the model output when prompted with z for input x.</p><p>To cope with the exponentially large action space, we can treat the optimization as a sequential decision-making process, as in RLPrompt.</p><p>RLPrompt employs SQL, an RL algorithm that incorporates entropy regularization.</p><p>An intrinsic characteristic of a softmax policy is its distribution of nonzero probability mass across all actions.</p><p>Employing sparse Tsallis entropy as regularization leads to a sparse optimal policy that concentrates probability mass on a subset set of actions.</p><p>The sparse policy assigns non-zero probabilities for only top-K actions, where smaller α makes the policy sparser.</p><p>Training the Q-network is essentially solving for an extremely overdetermined linear system, where approximation error is inevitable.</p><p>The estimated action value could become unreasonably high for low-probability tokens, promoting the RL algorithm to excessively try out these improbable tokens.</p><p>This results in an RL approach that is overly biased towards exploration, specifically favoring the selection of insignificant low-probability tokens.</p><p>We introduce the ignorable token set comprised of tokens deemed improbable from a general language model.</p><p>We empirically chose k = 10000 to ensure a sufficiently diverse set of tokens, disregarding about 80% of the vocabulary tokens.</p><p>We instead employ the sparse Tsallis entropy regularized Q-learning which yields a sparse policy that naturally suppresses the probability of choosing many unimportant tokens.</p><p>Our algorithm, PIN (Prompts made INterpretable), employs operator FI that systematically filters out the action values of ignorable tokens.</p><p>The goal of these tasks is to find the optimal prompt that assigns input text x to the class label, given a few examples in the context.</p><p>Our approach demonstrates competitive or superior performance compared to RLPrompt across all datasets.</p><p>A notable aspect of our PIN method is its efficiency: it achieves these strong results while necessitating fewer trials than required by RLPrompt.</p><p>Our algorithm relies on a weaker feedback and reward, as opposed to the direct back-propagated gradients used in Soft Prompt Tuning, AutoPrompt, and PEZ.</p><p>However, PIN underperforms relative to Soft Prompt Tuning on Yahoo dataset.</p><p>The text style transfer task aims to rephrase an input text to match a desired style.</p><p>We focus on unsupervised text style transfer task, where there are no input-output pair examples for training.</p><p>Our experiments, conducted with various sizes of task models under the same policy LM, consistently demonstrate the superiority of our algorithm.</p><p>There is no guarantee that the fluency will be improved by optimizing the reward, and it may even be possible to achieve higher rewards at the expense of fluency score.</p><p>The task of textual inversion entails identifying the caption that describe target images using VLMs, such as CLIP.</p><p>We use a range of image datasets for textual inversion, including MS COCO, LAION, and Lexica.art.</p><p>PIN is most effective in discovering high-quality prompts with fewer interactions, particularly compared to RLPrompt.</p><p>The sparse Tsallis entropy regularization employed by PIN enabled to handle a larger token search space.</p><p>PIN and PIN-no-fluency do not suffer from the aforementioned problem related to replay buffers.</p><p>The prompts learned by PIN are more interpretable to humans and more accurately capture the content of the target images.</p><p>Our empirical investigations across various tasks indicate that maintaining the number of tokens that are not to be ignored within the range of 10000 to 20000 yields the most favorable results.</p><p>PIN can discover prompts that are more interpretable compared to baselines.</p><p>The algorithm exhibits a relatively higher time consumption for training when compared to gradient-based methods.</p><p>Despite its advanced capabilities in discovering interpretable prompts, our algorithm does not guarantee the consistent discovery of grammatically perfect sentences.</p><p>We propose PIN algorithm, which uses sparse Tsallis entropy regularization to systematically exclude ignorable tokens from constraints.</p><p>Prompts learned by PIN exhibit better performance in various tasks.</p>",
        "abstract_gpt": "<p>Large-scale pretrained language and vision-language models have demonstrated strong adaptability to downstream tasks through prompt tuning, yet existing methods face key limitations. Soft prompt tuning learns continuous embeddings but produces opaque, non-transferable prompts, while hard prompt tuning offers interpretability and transferability but requires expensive discrete optimization. To address this challenge, we introduce PIN (Prompts made INterpretable), a reinforcement learning–based framework that incorporates sparse Tsallis entropy regularization to ignore unlikely tokens and focus on a reduced, informative action space. By filtering ignorable tokens and encouraging sparse policies, PIN mitigates exploration bias toward low-probability tokens and generates interpretable, human-readable prompts. We evaluate PIN across diverse tasks, including few-shot text classification, unsupervised text style transfer, and textual inversion with vision-language models. Experiments on datasets such as Yahoo, MS COCO, LAION, and Lexica.art show that PIN achieves competitive or superior performance compared to RLPrompt, requiring fewer interactions and producing more interpretable prompts. While computationally more demanding than gradient-based methods, PIN offers a principled and effective approach for hard prompt discovery.</p>",
        "abstract_human": "<p>With the advent of foundation models, prompt tuning has positioned itself as an important technique for directing model behaviors and eliciting desired responses. Prompt tuning regards selecting appropriate keywords included into the input, thereby adapting to the downstream task without adjusting or fine-tuning the model parameters. There is a wide range of work in prompt tuning, from approaches that directly harness the backpropagated gradient signals from the model, to those employing black-box optimization such as reinforcement learning (RL) methods. Our primary focus is on RLPrompt, which aims to find optimal prompt tokens leveraging soft Q-learning. While the results show promise, we have observed that the prompts frequently appear unnatural, which impedes their interpretability. We address this limitation by using sparse Tsallis entropy regularization, a principled approach to filtering out unlikely tokens from consideration. We extensively evaluate our approach across various tasks, including few-shot text classification, unsupervised text style transfer, and textual inversion from images. The results indicate a notable improvement over baselines, highlighting the efficacy of our approach in addressing the challenges of prompt tuning. Moreover, we show that the prompts discovered using our method are more natural and interpretable compared to those from other baselines (Deng et al., 2022).</p>"
    },
    {
        "index": "39",
        "excerpt": "<p>We are in an era of long-running cyber attack campaigns involving “Advanced and Persistent Threats” (APTs).</p><p>Carried out by sophisticated actors that prioritize stealth over other goals, these campaigns often remain undetected for many months.</p><p>During this period, attackers remain hidden in a victim’s network, while moving across hosts, installing malware, and gathering data.</p><p>Because of the stealthy nature of APTs, the primary recourse against them is after-the-fact detection, followed by forensic analysis to understand their full impact.</p><p>Such an analysis requires logs that faithfully capture all important system activity across the hosts in an enterprise.</p><p>Hence it is necessary to collect system-wide logs that cover all applications.</p><p>Most recent research works on APT investigation rely on system audit logs that operate roughly at the level of system calls.</p><p>Coarse-grained provenance tracking enabled by these logs ensures the completeness of forensic analysis, i.e., the analysis won’t miss any of the effects of an attack.</p><p>For server systems that tend to be based on Linux, the Linux auditing daemon auditd is an obvious choice for audit data collection.</p><p>Unfortunately, auditd incurs very high overheads for system-call granularity data collection, slowing down programs by 5× or more.</p><p>However, these systems require OS kernel modifications, making them a challenge from a deployment perspective.</p><p>In addition to these portability and deployability concerns, we show that existing audit data collection systems suffer from serious data loss and performance problems.</p><p>Under moderate to heavy loads, most existing systems drop a significant fraction of the events.</p><p>Even when operating at loads they can cope with, today’s log collection systems incur high overheads, slowing down workloads by 2× to 8×.</p><p>We show that existing systems can buffer hundreds of thousands of log records in memory before the data is output.</p><p>A successful attack can wipe these buffers, thus removing all attack evidence from the logs.</p><p>Existing systems produce logs ranging from several GBs to hundreds of GBs per host per day.</p><p>In this paper, we present a new audit log collection approach that overcomes these challenges.</p><p>It has been implemented into a light-weight and easily deployable system called eAudit.</p><p>eAudit is based on the Extended Berkeley Packet Filter (eBPF) framework built into recent Linux versions.</p><p>Consequently, eAudit can be readily deployed on these kernels without loading kernel modules, or changing the kernel code.</p><p>eAudit works “as is” on most recent Linux distributions.</p><p>We show that all these systems: incur high performance overheads, slowing down systems by 2x to 8x; can drop a large fraction of events; and store events in memory for substantial periods, making it easier for attackers to wipe out suspicious activities before they are logged permanently.</p><p>In Sec. 3, we describe eAudit design, focusing specifically on features for avoiding data loss, reducing runtime overhead, and minimizing opportunities for log tampering.</p><p>We present: a compact data encoding scheme that results in log files that are 10× smaller than those of other systems; a two-level buffer design that reduces contention and avoids data loss; a simple analytical model that underpins an optimal balancing of system throughput and latency; and a granular and tunable event prioritization scheme that further reduces log tampering opportunities.</p><p>eAudit avoids data loss even on peak loads that cause the best previous systems to lose over 90% of the data.</p><p>Our two-level buffer design and parameter tuning optimizations are very effective, reducing overheads by an average of 18.4× across our benchmarks.</p><p>With the benchmarks and metrics used in our motivational study, eAudit’s overhead is just 4.5%.</p><p>Our design and optimizations reduce the log tamper window by about 100× over previous systems.</p><p>Our event prioritization scheme shrinks this window by another 100× for the most important system calls.</p><p>The main goals of our design are: Reduce data volume so as to speed up every component involved in the data pipeline; Eliminate data loss even at peak system loads; Reduce overhead so as to minimize degradation of peak workloads that can be sustained; and Reduce latency of data capture so that records are logged to safe storage quickly, minimizing the opportunities for an adversary to tamper with these records.</p><p>The primary goal of this paper is to develop techniques for efficient provenance collection — techniques that do not degrade the workloads that can be sustained on a target system.</p><p>Our last goal is to degrade the ability of attackers to hide their activities through log tampering.</p><p>The eBPF framework enables small code snippets, called ebpf probes, to be safely deployed at well-defined hooks within the Linux kernel.</p><p>Each probe is a function with the signature specified in the sysfs pseudo file system at /sys/kernel/debug/tracing/events/syscalls/⟨scevent⟩/format.</p><p>System call information, including argument and return values, is first serialized and stored in per-CPU buffers that we call as message caches.</p><p>A side benefit of compact encoding is that we can record the entry and exit events separately, without being overly concerned about log size.</p><p>Due to this encoding, our system call records are 17 bytes on average (postmark benchmark), as compared with 175 bytes for sysdig and 850 bytes for auditd.</p><p>We therefore developed a two level buffering scheme to reduce the number of accesses to the ring buffer.</p><p>Our optimized two-level buffer design incurs tq once every p system calls, and ts every p · w system calls.</p><p>The overhead per system calls to drop to 0.8/100 + 2.9/800 ≈ 0.01µs, which means eAudit can potentially keep up with syscall rates of tens of millions per second.</p><p>The techniques described in the previous two sections helps to reduce the log tampering window significantly from several tens of thousands of records for previous systems (Fig. 4) to the range of hundreds.</p><p>We classify system calls into several categories, and associate a weight with each category.</p><p>Our design improves over theirs in two important ways.</p><p>Our evaluation shows that it comes close, achieving a window of just a few system calls even on very intensive workloads.</p><p>None of the benchmarks resulted in any data loss in the case of eAudit.</p><p>Across these benchmarks, our optimized design decreases the agent overhead by 18.4×.</p><p>The median among these averages is 48 nanoseconds.</p><p>Note that eAudit incurs between 4% and 5% overhead on these benchmarks, while all other systems have overheads that are much higher.</p><p>The average overhead across all the points in Fig. 21 is about 18%.</p><p>We found that it significantly underestimates eAudit’s log tamper window.</p><p>eAudit’s tamper window is several hundreds, as opposed to 10K to 100K records for sysdig.</p><p>eAudit achieves a 100-fold reduction, which significantly raises the bar for log tampering attacks.</p><p>In terms of log volume, eAudit produces 10×–13× smaller logs than sysdig, and 5.8× smaller even after compression.</p><p>eAudit scales to processors with a dozen cores, and can sustain peak workloads on such machines.</p><p>The practical significance of this ability for APT detection has not been established.</p><p>The research presented in this paper identifies and analyzes critical bottlenecks in existing audit collection systems, including high performance overheads, the dropping of a large fraction of events under sustained workloads, and large windows for log tampering.</p><p>We presented several new techniques to overcome these challenges, including a compact data encoding technique that significantly cuts down data volumes; a two-level buffering scheme that minimizes contention and avoids data loss even on intense multi-core workloads; an analytical model for optimally tuning latency and throughput; and an event prioritization scheme that reduces opportunities for log tampering.</p>",
        "abstract_gpt": "<p>Advanced Persistent Threats (APTs) pose a major challenge to enterprise security due to their stealthy, long-running nature, making forensic log analysis crucial for impact assessment. Existing audit log collection systems, such as Linux auditd, operate at system-call granularity but suffer from severe limitations, including high performance overhead, data loss under moderate workloads, and vulnerability to log tampering. This paper presents eAudit, a lightweight, deployable audit log collection system built on the eBPF framework in modern Linux kernels. eAudit introduces several key innovations: a compact data encoding scheme producing logs up to 13× smaller than existing systems, a two-level buffering design that eliminates event loss even under peak loads, an analytical model for balancing latency and throughput, and a tunable event prioritization scheme to minimize opportunities for tampering. Evaluation across benchmarks shows that eAudit reduces overhead by an average of 18.4×, achieving only 4–5% runtime cost, while shrinking the log tampering window by two orders of magnitude compared to prior systems. These results demonstrate that eAudit enables efficient, lossless, and tamper-resistant provenance collection, substantially improving the viability of system-wide logging for APT detection and forensic analysis.</p>",
        "abstract_human": "<p>Today’s advanced cyber attack campaigns can often bypass all existing protections. The primary defense against them is after-the-fact detection, followed by a forensic analysis to understand their impact. Such an analysis requires audit logs (also called provenance logs) that faithfully capture all activities and data flows on each host. While the Linux auditing daemon (auditd) and sysdig are the most popular tools for audit data collection, a number of other systems, authored by researchers and practitioners, are also available. Through a motivating experimental study, we show that these systems impose high overheads, slowing workloads by 2×to 8×; lose a majority of events under sustained workloads; and are vulnerable to log tampering that erases log entries before they are committed to persistent storage. We present a new approach that overcomes these challenges. By relying on the extended Berkeley Packet Filter (eBPF) framework built into recent Linux versions, we avoid changes to the kernel code, and hence our data collector works out of the box on most Linux distributions. We present new design, tuning and optimization techniques that enables our system to sustain workloads that are an order of magnitude more intense than those causing major data loss with existing systems. Moreover, our system incurs only a fraction of the overhead of previous systems, while considerably reducing data volumes, and shrinking the log tampering window by∼100×.</p>"
    }
]