[
    {
        "index": "22",
        "excerpt": "<p>Documents with rich layouts, including invoices, receipts, contracts, orders, and forms, constitute a significant portion of enterprise corpora.</p><p>The automatic interpretation and analysis of these documents offer considerable advantages, which has spurred the development of AI-driven solutions.</p><p>These visually rich documents feature complex layouts, bespoke type-setting, and often exhibit variations in templates, formats and quality.</p><p>Although Document AI (DocAI) has made tremendous progress in various tasks including extraction, classification and question answering, there remains a significant performance gap in real-world applications.</p><p>In particular, accuracy, reliability, contextual understanding and generalization to previously unseen domains continues to be a challenge.</p><p>Document intelligence is inherently a multi-modal problem with both the text content and visual layout cues being critical to understanding the documents.</p><p>It requires solutions distinct from conventional large language models such as GPT-3.5, Llama, Falcon or PaLM that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents.</p><p>Numerous vision-language frameworks that can process documents as images and capture the interactions between textual and visual modalities are available.</p><p>However, these frameworks necessitate the use of complex vision backbone architectures to encode image information, and they often make use of spatial information as an auxiliary contextual signal.</p><p>In this paper we present DocLLM, a light-weight extension to standard LLMs that excels in several visually rich form understanding tasks.</p><p>Unlike traditional LLMs, it models both spatial layouts and text semantics, and therefore is intrinsically multi-modal.</p><p>The spatial layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component.</p><p>Consequently, our solution preserves the causal decoder architecture, introduces only a marginal increase in the model size, and has reduced processing times, as it does not rely on a complex vision encoder.</p><p>We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering.</p><p>Existing efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings or summing the two.</p><p>In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner.</p><p>We extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships.</p><p>This is motivated by the observation that there is often a correlation between the content, position and size of the fields in a form.</p><p>Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.</p><p>When working with such documents, employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive.</p><p>In particular, the preceding tokens may not always be relevant due to the diverse arrangements of text, which can be positioned horizontally, vertically, or even in a staggered manner.</p><p>To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens.</p><p>Due to these modifications, the model is better equipped to address misaligned text, contextual completions, intricate layouts, and mixed data types.</p><p>We adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets.</p><p>These tasks encompass key information extraction, natural language inference, visual question-answering and document classification.</p><p>We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 15% to 61% for the Llama2-7B model in four out of five previously unseen datasets.</p><p>DocLLM is constructed upon the foundation of an auto-regressive transformer language model following a causal decoder structure.</p><p>It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network.</p><p>DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained using OCR.</p><p>We treat the spatial information about the text tokens as a distinct modality.</p><p>We use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner.</p><p>Instead of the traditional left-to-right next token prediction during self-supervised training, we employ a text infilling objective that better leverages contextual information.</p><p>The new attention mechanism is calculated by combining text-to-text, text-to-spatial, spatial-to-text, and spatial-to-spatial components.</p><p>The disentangled representation of these modalities in the attention scores enables selective focus when appropriate, thereby providing an optimal balance between model size and effectiveness.</p><p>DocLLM is first pre-trained in a self-supervised fashion on a large number of unlabeled documents.</p><p>Visual documents are often sparse and irregular, featuring isolated and disconnected text fragments.</p><p>It is preferable to consider coarse segments of related tokens during pre-training rather than focusing on individual tokens.</p><p>Furthermore, learning to infill text, where the prediction is conditioned on both prefix and suffix tokens rather than only preceding tokens, can be beneficial.</p><p>The infilling objectives enable contextually relevant completions, provide robustness to OCR noise or misaligned tokens, and can better handle relationships between various document fields.</p><p>We follow an autoregressive block infilling objective, where text blocks are randomly masked, and the masked blocks are shuffled and reconstructed in a sequential left-to-right fashion.</p><p>Following recent work in the field of VRDU and prior work in NLP, we instruction-tune DocLLM on a variety of instructions derived from DocAI datasets using various templates.</p><p>We employ a total of 16 datasets with their corresponding OCRs, spanning four DocAI tasks: visual question answering (VQA), natural language inference (NLI), key information extraction (KIE), and document classification (CLS).</p><p>DocLLM-7B excels in 12 out of 16 datasets, inclusively compared to ZS results of GPT4 and Llama2, and SDDS results of mPLUG-DocOwl and UReader.</p><p>DocLLM demonstrates superior performance in layout-intensive tasks such as KIE and CLS.</p><p>DocLLM also outperforms mPLUG-DocOwl on DocVQA and both mPLUG-DocOwl and UReader on KLC, despite both baselines having been instruction-tuned on these datasets.</p><p>The vanilla text-only self-attention mechanism yields the lowest NTP accuracy, underlining the importance of incorporating spatial features for understanding documents with rich layouts.</p><p>Autoregressive block infilling exhibits the best performance.</p>",
        "abstract_gpt": "<p>Visually rich documents such as invoices, forms, and contracts pose significant challenges for existing language models due to their complex layouts and spatially dispersed content. While vision-language models address these challenges by incorporating visual encoders, they often rely on heavyweight architectures. This paper introduces DocLLM, a lightweight, multi-modal extension to standard language models that integrates spatial layout information without requiring vision encoders. By treating bounding box coordinates of OCR-extracted text as a separate modality, DocLLM disentangles spatial and textual interactions via an extended self-attention mechanism. It replaces traditional next-token prediction with a novel block infilling objective conditioned on both preceding and succeeding tokens, better capturing context in irregular layouts. Pre-trained on unlabeled documents and fine-tuned using instruction data from 16 datasets spanning four Document AI tasks (VQA, NLI, KIE, and CLS), DocLLM demonstrates strong generalization. It achieves performance gains of 15-61% on unseen datasets and outperforms larger vision-language models like mPLUG-DocOwl and UReader, particularly in layout-sensitive tasks. By preserving a causal decoder architecture with minimal overhead, DocLLM offers a scalable and effective solution for document understanding without sacrificing speed or simplicity.</p>",
        "abstract_human": "<p>Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.</p>"
    },
    {
        "index": "23",
        "excerpt": "<p>The non-collusion assumption among computing parties is prevalent across privacy-preserving computations.</p><p>It allows us to solve some important problems, with no known solutions without it.</p><p>It also reduces overheads and allows us to bypass computational hardness results in other problems such as private information retrieval (PIR).</p><p>While most of these distributed systems do allow collusion among pre-defined subsets of parties, all bets are off once the adversary can lure a superset of parties.</p><p>Indeed, using the non-collusion assumption remains highly susceptible to undetectable collusion among computing parties.</p><p>As these distributed systems get considered for applications such as privacy-preserving machine learning, privacy-preserving database/directory searches, and blockchain privacy, the non-collusion assumption is considered an Achilles' heel for large adoption.</p><p>In the context of database querying service, a PIR protocol allows a client to query a database without revealing which piece of information is of interest.</p><p>PIR can be constructed from a single server or multiple servers.</p><p>By involving more non-colluding servers (k >= 2), the communication and computation complexity can be substantially reduced, which serves as a motivation for multi-server PIR schemes.</p><p>However, the non-collusion assumption promoting efficiency improvements calls for practical justification.</p><p>In this paper, we find that by relaxing the non-collusion assumption to the rationality assumption on servers, deterring such privacy-targeted collusion becomes possible.</p><p>The core question is then how to render collusion undesired for rational participants.</p><p>We design and implement a collusion mitigation mechanism on a public bulletin board with payment execution functions, considering only rational and malicious parties with no honest non-colluding servers.</p><p>We devise a mechanism for relaxing the non-collusion assumption in multi-server PIR systems to rationality assumptions on servers.</p><p>We present a practical collusion deterrence mechanism for multi-server PIR so that one can enjoy the perfect efficiency of 1-private PIR.</p><p>This is achieved in harsh settings.</p><p>We employ many servers so that a single server does not bear too much information worth leaking before it is discouraged to collude.</p><p>We examine how servers exit the PIR service and adopt a privacy protection self-insurance to protect user secrets for an extended period after the queries.</p><p>We study how servers form strong coalitions, whose members have complete trust for each other and are immune to the provided incentives, from a cooperative game theory perspective.</p><p>We implement and deploy the mechanism as a smart contract on Ethereum blockchain and apply optimizations to economize the gas costs.</p><p>The solution only adds to the underlying PIR scheme one signed commitment message for each query and response.</p><p>For practicality, we observe that in many settings where the number of available computing parties can be much higher than the required set, an effective collusion mitigation mechanism with practical parameters, i.e., small fee and penalty, can be constructed.</p><p>As a representative example, the proposed mechanism is useful for blockchain access privacy problem.</p><p>We assume all servers are either rational, i.e., taking actions that maximize utilities, or malicious, i.e., behaving arbitrarily.</p><p>There are no honest non-colluding servers.</p><p>We handicap ourselves by giving the adversary advantages in collusion.</p><p>Servers may hold arbitrary knowledge about a user’s index of interest.</p><p>Servers can communicate with each other over any unobserved communication channels.</p><p>We let there exist a secure collusion protocol, e.g., a secure MPC protocol, that can be utilized by colluding parties to compute the result collectively.</p><p>We describe the solution given rational servers.</p><p>We make use of a public bulletin board with payment execution functionality to instantiate the mechanism as a coordinator contract.</p><p>Users know servers’ identities, and they can directly communicate.</p><p>To mitigate collusion, we have participants post commitments of messages on BB for potential evidence verification.</p><p>Since we allow servers to hold arbitrary private knowledge, random companion queries are sent to help distinguish between servers learning user secrets via collusion and via private knowledge.</p><p>Colluding servers can collectively execute any collusion protocol to learn some nontrivial function of the explicit bits of the index or entry.</p><p>A collusion reporter provides the following as evidence: the circuit for computing function, her inputs to the function, and the output from computing it.</p><p>The first correct collusion reporting server for a PIR run is selected as the winner.</p><p>The payment rules are as follows: (1) fine servers submitting false reports; (2) distribute rewards to the winner; (3) penalize each server being successfully accused; (4) charge service fee for each query and pay a queried server that is not successfully accused.</p><p>To realize the incentive scheme, in the beginning, servers make deposits greater than the penalty and fines to CC.</p><p>When T is infinite or finite but unknown, backward induction no longer applies and collusion can become an equilibrium.</p><p>We apply discount rate on future returns because rational agents prefer acquiring the same returns earlier than later.</p><p>In the l-party infinitely repeated collusion game in (l, k, 1)-PIR, the strategy profile of all l servers playing no collusion or gibberish collusion in rounds one and two of each run are the only subgame perfect equilibria when the reward is large enough and the sum of service fee and penalty is large enough.</p><p>To ensure affordable service fees, we define practical service fees mathematically.</p><p>There always exist parameter assignments for inequalities ensuring that servers and users do not falsely accuse, and that collusion is not profitable.</p><p>We now consider a fraction of the l servers to be malicious.</p><p>Malicious servers behave arbitrarily and ignore incentives.</p><p>They bring about two changes to the mechanism.</p><p>First, the incentive mechanism achieves the non-collusion outcome only probabilistically.</p><p>Second, the collusion evidence verification routines can be exploited by malicious servers to function as an expensive oracle for revealing user secrets.</p><p>We adopt zero-knowledge accusation verification.</p><p>We analyze how servers’ exiting strategies and strong coalition formation affect the proposed mechanism and its parameterization.</p><p>We propose a Self-Insurance design tailored to our needs and detain service fees as the self-insurance pool for a certain privacy protection period.</p><p>We extend the analysis to consider strong coalitions formed by servers.</p><p>Servers inside the coalition act in a highly coordinated way.</p><p>Information entering into the group is shared among members.</p><p>Members do not report collusion inside.</p><p>A system designer first evaluates the participating servers and determines a recommended k.</p><p>One can compute the parameters for the collusion game according to inequalities.</p><p>We treat Ethereum as a public bulletin board and conveniently implement the coordinator contract as a smart contract on Ethereum.</p><p>The contract maintains the complete life cycle of PIR service and resolves collusion accusations.</p><p>We preserve the communication complexity of the underlying multi-server PIR.</p><p>Computing SHA-3 hash costs 30 base gas and 6 gas every 256 bits.</p><p>SealPIR has communication complexity and incurs heavy computation.</p><p>The two multi-server PIR schemes are more efficient.</p><p>Our goal is then to preserve the equilibrium in finite games in infinite runs, even when players are highly patient.</p><p>Following the intuition of a secret-shared bit guessing game, we design and implement a mechanism that discourages participants without network advantages from collusion to recover user secrets in multi-server PIR systems.</p><p>The design itself rejects manipulation from users and servers with private knowledge.</p>",
        "abstract_gpt": "<p>The non-collusion assumption is foundational in privacy-preserving computations such as multi-server Private Information Retrieval (PIR), but its practicality is hindered by the risk of undetectable collusion. This work proposes a shift from the traditional non-collusion model to a rationality-based assumption, wherein servers act to maximize their utility. The authors design and implement a collusion deterrence mechanism using a public bulletin board with payment execution, deployed as a smart contract on Ethereum. This mechanism employs economic incentives, random companion queries, and zero-knowledge accusation verification to discourage rational servers from colluding, even in the absence of any honest parties. The approach remains robust under adversarial conditions, including the presence of malicious servers and strong coalitions, while maintaining the communication efficiency of underlying PIR schemes. Experimental and theoretical analyses confirm that, under appropriate parameter settings, collusion becomes unprofitable, enabling practical, privacy-preserving PIR services without relying on strict non-collusion assumptions.</p>",
        "abstract_human": "<p>A long line of research on secure computation has confirmed that anything that can be computed, can be computed securely using a set of non-colluding parties. Indeed, this non-collusion assumption makes a number of problems solvable, as well as reduces overheads and bypasses computational hardness results, and it is pervasive across different privacy-enhancing technologies. However, it remains highly susceptible to covert, undetectable collusion among computing parties. This work stems from an observation that if the number of available computing parties is much higher than the number of parties required to perform a secure computation task, collusion attempts in privacy-preserving computations could be deterred.</p><p>We focus on the prominent privacy-preserving computation task of multi-server 1-private information retrieval (PIR) that inherently assumes no pair-wise collusion. For PIR application scenarios, such as those for blockchain light clients, where the available servers can be plentiful, a single server's deviating action is not tremendously beneficial to itself. We can make deviations undesired via small amounts of rewards and penalties, thus significantly raising the bar for collusion resistance. We design and implement a collusion mitigation mechanism on a public bulletin board with payment execution functions, considering only rational and malicious parties with no honest non-colluding servers. Privacy protection is offered for an extended period after the query executions.</p>"
    },
    {
        "index": "24",
        "excerpt": "<p>Managed data transfer is an enabler, an unsung hero of large-scale, globally-distributed systems.</p><p>The goal of data transfer management systems is to transfer when it is optimal to do so — in contrast to a last-minute transfer at the moment data needs to be consumed.</p><p>In this paper we describe Effingo, a data transfer management system we designed and built at Google.</p><p>Effingo has requirements and features uncommon in reported large-scale data transfer systems.</p><p>Effingo optimizes for smooth bandwidth usage and thus network cost.</p><p>At the same time, Effingo can operate in a deadline-aware, yet cost-efficient way.</p><p>Effingo operates on a higher layer, providing SLOs on a transfer of multiple files, including to/from multiple destinations/sources.</p><p>Effingo uses decentralized heuristics that scale well while maintaining cluster-level failure domains.</p><p>Effingo is used in production for years and widely adopted at Google, transferring daily over an exabyte in billions of files.</p><p>This scale is orders of magnitude larger than anything previously reported.</p><p>This magnitude of deployment enables economies of scale: development of specific, niche optimizations.</p><p>Moreover, operating at this scale makes many reliability investments justified to keep pace with the growth of adoption and meet the service SLO.</p><p>Effingo continuously (1) optimizes the copy tree to reduce the usage of expensive WAN links (e.g., subsea cables); (2) adjusts the transfer speed to react to changing effective bandwidth while avoiding spikes in usage according to the user latency hint; (3) maintains fairness between competing, same-priority transfers; (4) alerts about bottlenecks caused by insufficient quotas on various resources (WAN bandwidth, disk IOPS and compute).</p><p>We identify the throughput-oriented data transfer management problem in the context of a large-scale, heterogeneous organization dealing with many multi-petabyte transfers over a globally-distributed, multi-layer infrastructure.</p><p>We describe how to efficiently and at scale respond to these requirements.</p><p>Effingo is a standard service providing fairness, detecting bottlenecks, optimizing the copy tree and reducing bandwidth bursts.</p><p>We measure a production system operating at a scale of orders of magnitude larger than previously reported.</p><p>Effingo schedules transfers over space and time to minimize cost while ensuring fairness (between other Effingo transfers).</p><p>Effingo uses the store-and-forward approach originally proposed by NetStitcher, but limits it only to the locations permitted by the user, and integrates it with throughput- and fairness isolation.</p><p>Effingo parallelizes the file transfers as much as possible, given other constraints.</p><p>Effingo is optimized for a highly scalable, cluster-level filesystem Colossus.</p><p>Effingo is used by human users, single-tenant services, and multi-tenant services.</p><p>Effingo quota is measured in ingress and egress bandwidth from a particular cluster.</p><p>Effingo was designed with the following requirements: Client isolation, Isolated failure domains, Data residency, Data integrity, Resiliency, Cost/Performance tradeoff.</p><p>To isolate failure domains, Effingo stack is deployed in each cluster.</p><p>Effingo stores all transfers over a 14-day window in the copy tracker (CT) database.</p><p>For each set of files being copied from the same OSF, Effingo builds a Minimum Spanning Tree (MST) over the graph of planned transfers for this file, using values from the cost matrix as edge weights.</p><p>Effingo allows users to smooth the throughput by specifying an (optional) discrete latency hint: the maximum expected copy time.</p><p>Effingo adaptively controls the throughput of each copy operation.</p><p>Effingo uses a layered system, consisting of quotas, queues and priorities to fulfill these requirements.</p><p>Effingo’s enhanced debugging is a significant reason, besides efficiency, that teams switch to Effingo from their own solutions.</p><p>Effingo is a widely-deployed, throughput-oriented file copy system, integrated with resource management and authorization systems.</p><p>Two key design decisions are (1) per-cluster deployments, limiting failure domains to individual clusters; (2) separation from BwE, the bandwidth management layer, resulting in a modular design that reduces dependencies.</p><p>The lack of a central coordinator does not impede achieving efficiency and fairness.</p>",
        "abstract_gpt": "<p>Efficient data transfer is critical to the operation of globally distributed systems. This paper presents Effingo, a large-scale data transfer management system developed and deployed at Google. Unlike traditional systems, Effingo emphasizes proactive transfers that optimize for smooth bandwidth usage and network cost while meeting transfer deadlines. Effingo supports service-level objectives (SLOs) over collections of files and destinations, leveraging decentralized heuristics that scale without central coordination. It employs adaptive techniques to optimize copy paths, manage fairness across competing transfers, and respond to resource bottlenecks. Operating at a scale of over an exabyte per day across billions of files, Effingo enables niche optimizations and justified reliability investments. Its architecture—featuring per-cluster deployments, layered quotas, and failure domain isolation—supports a wide variety of users and services. The system’s design integrates cost-aware scheduling, throughput control, and fairness isolation, while maintaining modularity from lower-level bandwidth managers. Through extensive production deployment, Effingo demonstrates how large-scale, deadline-aware, and cost-efficient data transfers can be managed with minimal dependencies and high resiliency.</p>",
        "abstract_human": "<p>WAN bandwidth is never too broad — and the speed of light stubbornly constant.</p><p>These two fundamental constraints force globally-distributed systems to carefully replicate data close to where they are processed or served.</p><p>A large organization owning such systems adds dimensions of complexity with ever-changing network topologies, strict requirements on failure domains, multiple competing transfers, and layers of software and hardware with multiple kinds of quotas.</p><p>We present Effingo, a throughput-oriented, massively-parallel data copy service we built at Google.</p><p>For its users, Effingo delivers high-throughput transfers with an scp-like interface.</p><p>For Google, Effingo optimizes the network cost with a small footprint on datacenters.</p><p>We experimentally show how Effingo achieves fairness and efficiency through copy tree optimization and dynamic adaptation to changing network conditions.</p><p>On a typical day, Effingo transfers over an exabyte of data between dozens of clusters spread across continents and serves more than 10,000 users.</p>"
    },
    {
        "index": "25",
        "excerpt": "<p>Vector databases have become powerful tools for managing and retrieving embedding vectors, which effectively capture the semantic meaning of unstructured data.</p><p>By executing vector similarity queries, such as k-Nearest Neighbor (kNN) search, these databases can quickly retrieve relevant information based on the similarity of the embedding vectors.</p><p>Recently, vector databases have emerged as “retrieval plugins” seamlessly integrated into the generation process of Large Language Models (LLMs).</p><p>By harnessing their retrieval capabilities, we can provide relevant external data to enhance the accuracy of LLMs like ChatGPT when responding to queries that extend beyond the training data.</p><p>Vector databases have primarily been designed to handle data that do not contain private information.</p><p>In private domains, data is often highly sensitive and difficult to integrate into a single database due to regulations like GDPR.</p><p>Deploying a vector database in such application scenarios becomes even more challenging.</p><p>In the realm of medical informatics, LLMs are revolutionizing medical question answering systems.</p><p>These records contain highly sensitive patient information and are often distributed across multiple hospitals.</p><p>Due to the privacy regulations like GDPR, each hospital must independently manage its own database, often in the form of a vector database.</p><p>It is imperative to provide joint and secure query processing services across these hospitals’ local vector databases.</p><p>Data federation systems have been proposed to securely process queries across multiple data owners.</p><p>Existing work is usually designed to answer exact queries on low-dimensional data, like relational data and spatial data.</p><p>Due to the high-dimensional nature of vector data, existing solutions can be inefficient in processing federated vector similarity queries across local vector databases.</p><p>Each data owner could potentially act as a semi-honest attacker, aiming to infer sensitive information during the specified query processing protocol.</p><p>The query requester can only access information pertaining to the retrieved data, remaining unaware of the other data.</p><p>Such queries need to be processed rapidly to prevent reduction in the efficiency (e.g., inference latency of LLMs).</p><p>The query answer should be accurate and reliable enough to prevent misguidance, especially in model reasoning.</p><p>It is very challenging to achieve a balanced trade-off between query efficiency and result accuracy.</p><p>We have proposed a secure and efficient framework that processes federated vector similarity queries across multiple vector databases.</p><p>This framework leverages both local plaintext queries and collaborative secure computations.</p><p>Local plaintext queries are executed in vector databases to obtain partial results by using high-dimensional vector data indexes.</p><p>Collaborative secure computations, such as secure aggregations of these partial results, are implemented using secure multi-party computation techniques.</p><p>We have developed a user-friendly web client that enables DBAs to easily configure and monitor the vector data federation system.</p><p>We have created easy-to-integrate APIs in Python to further facilitate the incorporation of our solution into existing data retrieval and analytics workflow.</p><p>We have built a prototype system named FedSQ.</p><p>By using the open-source framework LangChain, FedSQ is integrated into a chat-based LLM and demonstrated to be useful in the application scenario of medical question answering.</p><p>FedSQ consists of three important layers, namely vector data federation, secure multi-party computation, and query coordinator layers.</p><p>Data owners can execute local vector similarity queries in vector databases.</p><p>These local vector databases provide convenient and autonomous data management services for data owners.</p><p>The contribution estimator quantifies the proportion of partial answers provided by each data owner.</p><p>The result aggregator assembles the partial answers from all data owners.</p><p>These two components are implemented via secure multi-party computation.</p><p>The query interface enables users to directly interact with the system and submit their queries.</p><p>The query parser performs syntax analysis on users’ query requests and extracts query conditions.</p><p>The query plan executor generates query plans and distributes them to the vector data federation layer.</p><p>The workflow starts with a federated vector similarity query and ends with the query answer.</p><p>Each data owner now receives the plaintext vector similarity queries.</p><p>Local answers are obtained by executing these queries within their vector databases, accelerated by vector data index.</p><p>The contribution estimator determines the proportion of partial results in each local answer.</p><p>The result aggregator collects the estimated quantity from local answers via secure operations.</p><p>The final answer will be returned to the user by the result aggregator component.</p><p>Each data owner facilitates local vector similarity queries via efficient index like HNSW.</p><p>The maximum number of connections per node is set to 16 in HNSW, and 200 candidate neighbors are saved per node.</p><p>The number of candidate neighbors accessed during the search phase of HNSW is dynamically adjusted.</p><p>The secure multi-party computation layer is integrated into a Django backend.</p><p>The CrypTen library is employed to implement secure operation protocols.</p><p>The Protobuf library enables efficient communication among the data owners.</p><p>The estimator begins by uniformly sampling from all data owners to construct profiles.</p><p>Specific non-uniform sampling techniques are employed to seek the complete query result.</p><p>Each data owner will hold his local kNN answers.</p><p>The estimator can deduce which vector database potentially holds more relevant data.</p><p>An iterative process is employed to seek more samples from this database.</p><p>A secure set union operator collects the vectors and their raw data as the final answer.</p><p>A user-friendly front-end query interface is developed utilizing the Vue.js framework.</p><p>We also integrate the Celery task queue for efficient asynchronous execution of query tasks.</p><p>FedSQ offers a unified interface for query users.</p><p>Query users can obtain essential information about the vector data federation.</p><p>Users can view both the current and historical query results.</p><p>Users can download the retrieved vector embeddings and the corresponding raw data.</p><p>The data schema displays the available attributes of the vector data federation.</p><p>DBAs can manage the schema through interface controls.</p><p>The DBAs of this vector data federation system can remove or add data owners flexibly.</p><p>FedSQ provides a graphical module to show the current status of the vector data federation.</p><p>The interface allows users to input queries, adjust LLM parameters, and obtain results.</p><p>When users ask questions without FedSQ, the LLM relies solely on its internal knowledge.</p><p>By using LangChain to integrate FedSQ, the LLM collaborates with external vector databases.</p><p>The model can access fresh data, ensuring that the outputs are based on the latest and most relevant information.</p>",
        "abstract_gpt": "<p>Vector databases have recently become essential for enhancing Large Language Models (LLMs) by enabling vector similarity search over embedding representations of unstructured data. However, their application in privacy-sensitive domains, such as medical informatics, is hindered by regulatory constraints and the decentralized nature of data storage across institutions. To address this challenge, the authors propose FedSQ, a secure and efficient framework for federated vector similarity queries across distributed vector databases. FedSQ combines local plaintext queries with collaborative secure multi-party computation to ensure privacy protection and query efficiency. It supports local querying using efficient indexing methods like HNSW and aggregates results securely using CrypTen and Protobuf. The system includes a user-friendly web interface built with Vue.js, Python APIs for integration, and a Django backend. Integrated into LLM workflows via LangChain, FedSQ improves the accuracy and recency of model responses by retrieving fresh, relevant external data while maintaining compliance with privacy regulations. This work demonstrates the viability of secure, federated vector search in sensitive domains, balancing utility and privacy.</p>",
        "abstract_human": "<p>Vector databases have emerged as crucial tools for managing and retrieving representation embeddings of unstructured data. Given the explosive growth of data, vector data is often distributed and stored across multiple organizations. However, privacy concerns and regulations like GDPR present new challenges in collaborative and secure queries, also known as federated queries, over those vector data distributed across various data owners. Although existing research has attempted to enable such query services for low-dimensional data, such as relational and spatial data, these solutions can be inefficient in answering vector similarity queries involving high-dimensional data. Therefore, we are motivated to develop a new prototype system called FedSQ that (1) ensures privacy protection across data owners and (2) balances query efficiency and result accuracy when processing federated vector similarity queries. To achieve these goals, FedSQ utilizes advanced secure multi-party computation techniques to prevent information leakage during query processing and incorporates indexing and sampling based optimizations to strike a proper performance balance.</p>"
    },
    {
        "index": "26",
        "excerpt": "<p>Memory safety bugs continue to be a major source of security vulnerabilities, despite much research on software bug-finding and mitigation approaches.</p><p>The CHERI project, developed by the University of Cambridge and SRI International since 2010, offers a promising hardware-based approach.</p><p>CHERI extends conventional hardware Instruction-Set Architectures (ISAs) to enable support for fine-grained memory protection and for scalable software compartmentalisation, with hardware-supported capabilities.</p><p>The ISA design ensures that capabilities cannot be forged, i.e., that normal code execution can shrink capabilities but never grow them.</p><p>The CHERI architectural mechanisms can be used by language implementations and systems software in various ways to provide improved security.</p><p>A 2019 analysis suggested that 30–70% of the vulnerabilities reported to the Microsoft Security Response Center (MSRC) would have been deterministically mitigated by CHERI memory-safety.</p><p>All this raises the question that we address in this paper: what is CHERI C, exactly?</p><p>We make the following contributions: discussion of the design issues that arise in the design of CHERI C and its semantics, including the subtle interactions between capabilities, undefined behaviour, and pointer provenance.</p><p>An executable mechanised semantics of CHERI C, reifying the above as an extension of the Cerberus ISO C semantics.</p><p>The CHERI C memory object model is mechanised within Coq, with the extracted code used in the executable semantics.</p><p>A prose definition of CHERI C.</p><p>Validation and experimental comparison.</p><p>CHERI-enabled architectures add new hardware support for capabilities in registers and in memory.</p><p>A capability includes an address together with bounds, permissions, and other metadata.</p><p>The one-bit tag provides integrity protection: it is preserved only by legitimate operations on capabilities and cleared by any others.</p><p>A capability can only be used as such, e.g., for a dereference, if its tag is set.</p><p>The permission bits control whether a capability can be used for loading or storing data or fetching instructions.</p><p>ISO C relies crucially on the notion of undefined behaviour (UB), to make it possible to define the semantics of a memory-unsafe language.</p><p>Any program for which there exists an abstract machine execution which has undefined behaviour is deemed to have undefined behaviour as a whole.</p><p>In CHERI C, source-language pointers are represented with hardware capabilities.</p><p>The compiler introduces code to construct a capability with correctly narrowed bounds derived from the stack-pointer capability.</p><p>The design of CHERI C has to reconcile three major and at times conflicting objectives: porting existing codebases, maintaining performance, and deterministically mitigating memory-safety errors.</p><p>CHERI C needs a well-defined and comprehensible semantics for all involved.</p><p>We give what we call the positive semantics, to clearly define what programmers can rely on, and what they are obliged to ensure, for well-defined CHERI C code.</p><p>CHERI C clearly deterministically mitigates many otherwise-exploitable security flaws, but undefined behaviour and compiler optimisations make it unclear what precise security properties it provides in general.</p><p>We choose to follow the stricter ISO rule also for CHERI C, even though that leaves code that exploits the architectural guarantee as UB.</p><p>We allow (u)intptr_t arithmetic within some region of representability, but keep defined behaviour and the integer (address-part) value of the result defined if one goes outside.</p><p>We permit casts to pointer types of (u)intptr_t capabilities with this bit set, and loads and stores of them, but make it UB to access memory via them.</p><p>Our current proposed solution is for the abstract CHERI C machine to record any non-capability write to a capability using ghost state.</p><p>Pointer values are capabilities, and tag, bounds, and permission checks are performed when they are used to access memory.</p><p>Pointer equality in CHERI C compares just the address fields, not the full capability metadata.</p><p>For binary arithmetic operations on two values of capability-carrying types, the resulting capabilities are derived from their left arguments.</p><p>Based on experience in porting code, the current default behaviour of CHERI C is to not enforce subobject bounds.</p><p>Objects created at const-qualified types are expected to be immutable, so in CHERI C those casts are no-ops on the underlying capability.</p><p>The C semantics needs a common abstraction of hardware capabilities.</p><p>We restrict the abstract scope of compression to four capability fields: address, flags, and upper and lower bounds.</p><p>The capability checks at runtime could not subsume provenance checks at compile time.</p><p>The two are complementary.</p><p>We codified CHERI C semantics as an extension of Cerberus, a well-validated semantic model for ISO C.</p><p>We defined the CHERI C memory object model in Coq and extracted it to OCaml to integrate into Cerberus.</p><p>CHERI C adds the following new undefined behaviours: UB_CHERI_InvalidCap (dereference with tag cleared), UB_CHERI_UndefinedTag (tag marked unspecified), UB_CHERI_InsufficientPermissions (lacking permission bit), UB_CHERI_BoundsViolation (out-of-bounds access), and UB012_lvalue_read_trap_representation (decode failure).</p><p>We validate our semantics by discussion with CHERI C implementers and porters of large codebases.</p><p>We validate experimentally using a test suite of 94 tests across semantic categories.</p><p>We compiled and ran all our tests using three CHERI C implementations and compared the results.</p><p>Cerberus passes all our tests with the results we expect, modulo one known bug.</p><p>The Clang/LLVM compiler is mostly compliant with our CHERI C semantics definition.</p><p>CHERI GCC is a relatively new arrival; our test suite identified five issues, one of which was confirmed as a bug.</p><p>CHERIoT targets embedded systems and could not be directly tested, but CHERI C semantics are applicable to it.</p><p>Our mechanised semantics for CHERI C should provide clarity of what is (and is not) guaranteed by the language.</p><p>The semantics would provide a solid basis for program analysis or model-checking of CHERI C.</p>",
        "abstract_gpt": "<p>CHERI (Capability Hardware Enhanced RISC Instructions) is a hardware-software co-design developed to address persistent memory safety vulnerabilities in C-based systems. By extending conventional ISAs with unforgeable capabilities, CHERI enables fine-grained memory protection and scalable compartmentalization. This paper investigates the formal semantics of CHERI C, the dialect of C used to program CHERI-enabled systems. The authors identify key design tensions between supporting legacy codebases, preserving performance, and enforcing deterministic mitigation of memory safety errors. They present a rigorous, mechanised semantics for CHERI C as an extension of the Cerberus ISO C model, including a Coq-verified memory object model and an executable OCaml semantics. The semantics captures nuanced interactions involving capabilities, undefined behavior, and pointer provenance. Validation includes collaboration with CHERI implementers, experimental testing on 94 semantic scenarios across multiple CHERI C compilers, and identification of conformance gaps. The resulting semantics defines new forms of undefined behavior unique to CHERI C and clarifies guarantees offered by capability-based memory safety. This work provides a foundational tool for reasoning about, verifying, and analyzing CHERI C programs, contributing to more secure systems software.</p>",
        "abstract_human": "<p>Memory safety issues are a persistent source of security vulnerabilities, with conventional architectures and the C codebase chronically prone to exploitable errors. The CHERI research project has shown how one can provide radically improved security for that existing codebase with minimal modification, using unforgeable hardware capabilities in place of machine-word pointers in CHERI dialects of C, implemented as adaptions of Clang/LLVM and GCC. CHERI was first prototyped as extensions of MIPS and RISC-V; it is currently being evaluated by Arm and others with the Arm Morello experimental architecture, processor, and platform, to explore its potential for mass-market adoption, and by Microsoft in their CHERIoT design for embedded cores.</p><p>There is thus considerable practical experience with CHERI C implementation and use, but exactly what CHERI C’s semantics is (or should be) remains an open question. In this paper, we present the first attempt to rigorously and comprehensively define CHERI C semantics, discuss key semantics design questions relating to capabilities, provenance, and undefined behaviour, and clarify them with semantics in multiple complementary forms: in prose, as an executable semantics adapting the Cerberus C semantics, and mechanised in Coq.</p><p>This establishes a solid foundation for CHERI C, for those porting code to it, for compiler implementers, and for future semantics and verification.</p>"
    },
    {
        "index": "27",
        "excerpt": "<p>Various software artifacts need to be carefully developed in order to produce a software system.</p><p>In software projects that involve personnel from different disciplines, the breadth of software artifacts can be vast.</p><p>Multidisciplinary teams require a multidisciplinary dependency graph.</p><p>While dependency graphs have been explored in the general development context, the multidisciplinary context introduces challenges in the extraction and analysis of dependency graphs that need to be addressed.</p><p>In this paper, we show how such a multidisciplinary dependency graph can be extracted from a AAA video game project and study the properties of that graph.</p><p>Non-code artifacts account for 97.2% of nodes, and edges connected to non-code artifacts account for 95.7% of edges.</p><p>Cross-boundary changes occur at similar rates as changes that do not cross boundaries, but their impact is higher.</p><p>On average, cross-boundary changes impact 212,104 nodes, with a median impact of 120,368 nodes.</p><p>Cross-boundary changes introduce build breakages (51% of the time) and defects (67% of the time) more frequently than changes that do not cross boundaries (44% and 37% of the time, respectively).</p><p>43%, 68%, and 26% of the tagged cross-boundary changes are associated with gameplay functionality, feature additions, and file additions, respectively.</p><p>36% and 68% of the tagged changes that do not cross boundaries are associated with tools and their configuration, as well as bug fixes.</p><p>74% of the contributors of cross-boundary changes are members of the same team.</p><p>The studied project is the most recent installment of a popular video game in a franchise that was released years ago by Ubisoft.</p><p>In the studied project, the vast majority of nodes (97.2%) and edges (95.7%) will be missed if non-code artifacts are excluded from the dependency graph.</p><p>44% of the code changes have a large cross-disciplinary impact that affects 212,104 nodes on average.</p><p>Cross-boundary changes tend to break builds (51% of the time) and induce defects (67% of the time) with significantly greater rates than changes that do not cross boundaries (44% and 37% of the time, respectively).</p><p>Cross-boundary changes are more often associated with GamePlay, Feature, and UI tags, as well as file additions and updates, whereas code changes that do not cross boundaries are more often associated with Tools and bugfix tag.</p><p>Non-code changes are rarely tagged.</p><p>Cross-boundary changes are from the Programming group only, while non-code changes have various group origins.</p><p>Contributors from one team commit most of the cross-boundary changes.</p><p>Dependency graphs should be expanded to include the multidisciplinary context.</p><p>Cross-boundary changes should receive additional scrutiny during code review.</p><p>We observe that changes having an impact that crosses disciplinary boundaries are a regular occurrence and tend to be associated with a greater risk of build breakage and defect introduction than changes that do not cross boundaries.</p><p>We show that cross-boundary changes are more commonly associated with gameplay functionality, feature additions, and UI-related development than changes that do not cross boundaries.</p><p>Changes that do not cross boundaries are more commonly associated with tool implementation and bug fixes than cross-boundary changes.</p><p>The vast majority of cross-boundary changes are contributed by one team rather than dispersed across the organization.</p>",
        "abstract_gpt": "<p>In multidisciplinary software projects, such as AAA video game development, traditional code-centric dependency graphs fail to capture the full scope of software artifacts. This paper presents a multidisciplinary dependency graph extracted from a large-scale Ubisoft game project, revealing that non-code artifacts constitute 97.2% of nodes and 95.7% of edges. The study analyzes the impact of cross-boundary changes—modifications affecting multiple disciplines—finding they are common and introduce significantly higher risks of build breakages (51%) and defects (67%) compared to intra-boundary changes (44% and 37%, respectively). Cross-boundary changes typically relate to gameplay, feature additions, and UI development, and are predominantly contributed by a single programming team. In contrast, non-cross-boundary changes often involve tool configurations and bug fixes. These findings highlight the importance of incorporating non-code elements into dependency analysis and suggest that cross-boundary changes warrant increased scrutiny during code review to mitigate risk in multidisciplinary software systems.</p>",
        "abstract_human": "<p>To produce a video game, engineers and artists must iterate on the same project simultaneously. In such projects, a change to the work products of any of the teams can impact the work of other teams. As a result, any analytics tasks should consider intra- and inter-dependencies within and between artifacts produced by different teams. For instance, the focus of quality assurance teams on changes that are local to a team differs from one that impacts others. To extract and analyze such cross-disciplinary dependencies, we propose the multidisciplinary dependency graph. We instantiate our idea by developing tools that extract dependencies and construct the graph at Ubisoft—a multinational video game organization with more than 18,000 employees.</p><p>Our analysis of a recently launched video game project reveals that code files only make up 2.8% of the dependency graph, and code-to-code dependencies only make up 4.3% of all dependencies. We also observe that 44% of the studied source code changes impact the artifacts that are developed by other teams, highlighting the importance of analyzing inter-artifact dependencies. A comparative analysis of cross-boundary changes with changes that do not cross boundaries indicates that cross-boundary changes are: (1) impacting a median of 120,368 files; (2) with a 51% probability of causing build failures; and (3) a 67% likelihood of introducing defects. All three measurements are larger than changes that do not cross boundaries to statistically significant degrees.</p><p>We also find that cross-boundary changes are: (4) more commonly associated with gameplay functionality and feature additions that directly impact the game experience than changes that do not cross boundaries, and (5) disproportionately produced by the same team (74% of the contributors are associated with that team).</p>"
    },
    {
        "index": "28",
        "excerpt": "<p>Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset without additional interactions with the environment.</p><p>This characteristic makes it particularly promising for critical applications such as healthcare decision-making, human-AI coordination and autonomous driving.</p><p>Given that the offline data is limited, fine-tuning the policy through interactions with the environment is still necessary to achieve favorable performance.</p><p>Consequently, offline-to-online (O2O) RL tends to achieve faster performance improvements based on better initializations.</p><p>To effectively fine-tune offline policies, O2O methods are typically designed based on specific offline RL algorithms.</p><p>Unfortunately, these methods often suffer from inefficient performance improvement due to restricted action exploration caused by policy constraints.</p><p>However, these methods often face high computational costs due to the need to train multiple Q-networks.</p><p>To establish a general O2O framework, it is essential to address the core issues associated with transitioning from offline to online environments.</p><p>We identify two mismatches in general O2O RL: evaluation mismatches and improvement mismatches.</p><p>Evaluation mismatches primarily occur in value regularization methods.</p><p>Improvement mismatches, on the other hand, are prevalent in policy constraint methods.</p><p>We bridge these two types of mismatches within a unified framework for general RL-based offline algorithms.</p><p>To address the evaluation mismatch in value regularization methods, we propose re-evaluating the offline policy in an optimistic manner using an off-policy evaluation method.</p><p>To handle the improvement mismatch in the re-evaluated critics and policy constraint methods, we introduce value alignment to calibrate the critic so that it aligns with the probabilities predicted by the policy.</p><p>Finally, we propose a constrained fine-tuning framework to guide the policy update by adding a regularization term, with the target of mitigating the negative impact of data shift.</p><p>Extensive experimental results on multiple benchmark environments validate that the proposed methods can achieve better or comparable performance when compared to state-of-the-art methods.</p><p>We show that resolving these two types of mismatches is essential for achieving general O2O RL.</p><p>Policy re-evaluation aims to achieve optimistic Q-value estimates, preventing instability in Q-value estimation.</p><p>Value alignment calibrates the critic to align with the policy, ensuring consistency between action probabilities and their corresponding Q-values.</p><p>We introduce a constrained fine-tuning framework that incorporates a regularization term into the policy objective, combating the inevitable distribution shift and ensuring stable and optimal performance when fine-tuning the policy in online environments.</p><p>Most offline RL methods suffer from one or both of these issues, which underscores the importance of addressing these mismatches to achieve stable and effective online fine-tuning.</p><p>Evaluation mismatch often occurs in the value regularization methods.</p><p>This shift frequently results in a sharp increase in Q-values at the beginning of online fine-tuning, which can hinder stable performance improvements.</p><p>Improvement mismatch is commonly found in policy constraint methods.</p><p>This divergence often misguides the update of policy at the beginning stage of online fine-tuning, resulting in unfavourable performance.</p><p>Offline actor and critic trained by behavior-regularized MDP for initialization in online fine-tuning introduces both types of mismatches, resulting in unstable and inefficient updates.</p><p>We develop a policy re-evaluation technique to optimistically re-evaluate the well-trained offline policy using an off-policy evaluation method.</p><p>We propose value alignment, which aims to align the critic’s estimates with the policy’s action probabilities, effectively tackling the improvement mismatch in both types of methods.</p><p>We develop a constrained fine-tuning framework.</p><p>The goal is to enable the critic to have optimistic estimates of Q-values that more closely approximate the true values.</p><p>With a large training step K, the error will be bounded by an acceptable value.</p><p>This implies that, given sufficient data, one can achieve a critic with optimistic property and minor extrapolation error through policy re-evaluation.</p><p>The misalignment means that the action with the highest Q-value does not necessarily have the highest probability, often leading to misleading updates of the policy.</p><p>The performance of re-evaluated critics sharply declines at the initial stage and does not recover in the subsequent training; while our aligned critic achieves stable and favorable performance.</p><p>These results indicate that the misalignment between the re-evaluated critic and the offline policy can make it difficult for the policy to optimize in a correct direction.</p><p>Given that the well-trained offline policy is reliable, the desirable critic should not only have optimistic Q-value estimates but also maintain alignment with the offline policy.</p><p>We derive Proposition 4.3 to show that the aligned state values can be maintained within an appropriate range, meaning that the estimates remain optimistic while avoiding overestimation.</p><p>This flexibility allows us to implement the transition to SAC from different offline algorithms.</p><p>This enables us to assume that normalized Q-values around the optimal action follow a Gaussian distribution.</p><p>This ensures that the output action of the policy maintains a smoothing and optimistic property of Q-values.</p><p>To mitigate this problem, we propose an auxiliary advantage function to correct erroneous updates.</p><p>The auxiliary advantage enables the policy to update in a reliable region.</p><p>To deal with the inevitable distribution shift between the offline dataset and the online environment, we develop a constrained fine-tuning framework.</p><p>We impose a constraint term on the policy objective to ensure that it updates within the credible region of the reference policy.</p><p>We provide a theoretical guarantee for the proposed CFT framework.</p><p>Experiments validate the effectiveness of the proposed method on D4RL MuJoCo and AntMaze tasks.</p><p>Our method can converge more stably and rapidly than other methods and achieve the optimal performance in most cases.</p><p>Although PPO is an on-policy method with low efficiency, it shows significant superiority on sparse reward tasks.</p><p>One of the advantages of our method is that it imposes no requirements on offline algorithms.</p><p>Our methods achieve the strong transferability from various offline methods.</p><p>To address these two mismatches in O2O RL, we proposed optimistic critic reconstruction to re-evaluate an optimistic critic and align it with the offline actor before online fine-tuning.</p><p>We introduce constrained fine-tuning to constrain the divergence of current policy and the best foregoing policy to maintain the stability of online fine-tuning.</p><p>These two components form a versatile O2O framework, allowing the transition from any offline algorithms to three state-of-the-art online algorithms.</p><p>Experiments show our framework can converge to optimal performance without affecting the aligned critic at the beginning of online fine-tuning and achieve strong empirical performance.</p>",
        "abstract_gpt": "<p>Offline-to-online (O2O) reinforcement learning aims to fine-tune a policy learned from fixed offline data through limited online interactions. While this approach offers promise for high-stakes domains like healthcare and autonomous driving, existing methods often suffer from inefficiencies due to evaluation and improvement mismatches between the offline and online settings. This paper introduces a general O2O framework that addresses these mismatches through three key components: optimistic policy re-evaluation, value alignment between the critic and policy, and a constrained fine-tuning (CFT) framework to mitigate distribution shift. The optimistic re-evaluation produces reliable Q-value estimates, while value alignment ensures consistency between action probabilities and Q-values. The CFT approach incorporates a regularization term into the policy objective to limit updates to credible regions. Extensive experiments on benchmark environments demonstrate that the proposed method achieves more stable and efficient performance than prior approaches and offers strong transferability across various offline algorithms. The results confirm the effectiveness of addressing mismatches to enable general and robust O2O RL.</p>",
        "abstract_human": "<p>Offline-to-online (O2O) reinforcement learning (RL) provides an effective means of leveraging an offline pre-trained policy as initialization to improve performance rapidly with limited online interactions. Recent studies often design fine-tuning strategies for a specific offline RL method and cannot perform general O2O learning from any offline method. To deal with this problem, we disclose that there are evaluation and improvement mismatches between the offline dataset and the online environment, which hinders the direct application of pre-trained policies to online fine-tuning. In this paper, we propose to handle these two mismatches simultaneously, which aims to achieve general O2O learning from any offline method to any online method. Before online fine-tuning, we re-evaluate the pessimistic critic trained on the offline dataset in an optimistic way and then calibrate the misaligned critic with the reliable offline actor to avoid erroneous update. After obtaining an optimistic and and aligned critic, we perform constrained fine-tuning to combat distribution shift during online learning. We show empirically that the proposed method can achieve stable and efficient performance improvement on multiple simulated tasks when compared to the state-of-the-art methods. The implementation is available at https://github.com/QinwenLuo/OCR-CFT.</p>"
    },
    {
        "index": "29",
        "excerpt": "<p>Single-image super-resolution (SISR) involves generating high-resolution (HR) images from low-resolution (LR) counterparts, a process crucial in various applications including video surveillance, medical diagnosis, and photography.</p><p>SISR is challenging due to the diverse real-world degradation patterns and the inherent ill-posed nature of the task, where different HR images can correspond to the same LR image.</p><p>SISR methods are generally categorized into regression-based and generation-based approaches.</p><p>Regression-based methods focus on minimizing pixel-level discrepancies, i.e., distortion, between SR predictions and HR references.</p><p>However, this approach often fails to capture the perceptual quality of images.</p><p>To address this, generation-based methods employ deep generative models, including autoregressive models, variational autoencoders (VAEs), normalizing flows (NFs), and generative adversarial networks (GANs), aiming to improve the perceptual aspects of SR images.</p><p>Recently, Diffusion Probabilistic Models (DPMs), a novel class of generative models, have attracted increased interest for their impressive generative abilities, especially in the SISR task.</p><p>Nonetheless, DPM-based methods face challenges due to their dependence on a long sampling chain, which can lead to error accumulation and reduce training and sampling efficiency.</p><p>A further issue is the discrepancy between training and sampling: training typically involves denoising noisy images conditioned on ground truth samples, whereas testing (or sampling) conditions on previously self-generated results.</p><p>To bridge the gap between training and sampling in diffusion models, we introduce DREAM, an end-to-end training framework denoting Diffusion Rectification and Estimation-Adaptive Models.</p><p>DREAM consists of two key elements: diffusion rectification and estimation adaptation.</p><p>Diffusion rectification extends traditional diffusion training with an extra forward pass, enabling the model to utilize its own predictions.</p><p>However, solely relying on this self-alignment can compromise perceptual quality for the sake of reducing distortion.</p><p>To counter this, our estimation adaptation strategy balances standard diffusion and diffusion rectification by adaptively incorporating ground-truth information.</p><p>The DREAM framework excels in its simplicity, easily integrating into existing diffusion-based models with only three lines of code and requiring no alterations to the network architecture or sampling process.</p><p>When applied to the SR task, DREAM has notably improved generation quality across various diffusion-based SR methods and datasets.</p><p>DREAM accelerates training convergence by 2 to 3 times and improves sampling efficiency, requiring 10 to 20 times fewer steps for comparable or superior results.</p><p>It also demonstrates enhanced out-of-distribution (OOD) SR results compared to baseline methods.</p><p>We introduce DREAM, a simple yet effective framework to alleviate the training-sampling discrepancy in standard diffusion models, requiring minimal code modifications.</p><p>We demonstrate the application of DREAM to various diffusion-based SR methods, resulting in significant improvements in distortion and perception metrics.</p><p>The proposed DREAM also notably speeds up training convergence, enhances sampling efficiency, and delivers superior out-of-distribution (OOD) results.</p><p>Our approach, distinct from previous unconditional methods, addresses discrepancies based on predictions relative to the conditional input data, ensuring a tailored and accurate solution for complex visual prediction tasks like SISR.</p><p>Our method also draws inspiration from step-unrolling techniques in depth estimation and text generation, leveraging the model’s own predictions for error estimation.</p><p>However, we uniquely integrate self-estimation with adaptive incorporation of ground-truth data.</p><p>This integration, guided by the pattern of estimation errors, effectively balances perceptual quality and distortion, enhancing generated image qualities.</p><p>Training diffusion models for SR presents a critical challenge, stemming from a discrepancy between the training and inference phases, which we term as training-sampling discrepancy.</p><p>During the training phase, the model operates on actual data, wherein the noisy image at diffusion step is derived from the ground-truth HR image.</p><p>However, during the inference phase, the ground truth is unavailable.</p><p>The model now operates on predicted data, where the noisy image is obtained from the preceding sampling step.</p><p>Due to the estimation error, the noisy image constructed in these two processes usually differs, giving rise to the training-sampling discrepancy.</p><p>This underscores the efficacy of our approach in bridging the training-sampling discrepancy and thereby facilitating more accurate predictions.</p><p>The goal of diffusion rectification is to modify the behavior of the diffusion training to account for the training-sampling discrepancy.</p><p>We extend the diffusion training framework to align more closely with the sampling process, enabling the model to utilize its own output for prediction.</p><p>This DRM approach strives not only to eliminate the sampled noise but also to address the error term arising from the discrepancy between the imperfect estimation and the ground-truth.</p><p>While DRM incorporates additional rectification supervision to account for the sampling process, its naive application to the SR task might not deliver satisfactory results.</p><p>A distortion-perception tradeoff is observed in the generated SR images.</p><p>Despite achieving a state-of-the-art PSNR (less distortion), the images produced by DRM tend to be smoother and lack fine details, reflecting a high FID score (poor perception).</p><p>To address the issue, and inspired by the powerful generative capability of the standard diffusion model, we propose an estimation adaptation strategy.</p><p>This aims to harness both the superior quality of standard diffusion and the reduced distortion offered by diffusion rectification.</p><p>We adaptively inject ground-truth information by blending it with the self-estimation.</p><p>DREAM reaches convergence at around 100k to 150k iterations, a significant improvement over the standard diffusion-based SR3’s 400k iterations.</p><p>DREAM not only converges faster but also surpasses SR3’s final results before its own convergence.</p><p>DREAM achieves a PSNR of 28.07 and FID of 14.72 at just 470k iterations, while SR3 with standard diffusion reaches PSNR 27.02 and FID 16.72 after 980k iterations, indicating a 2× speedup in training.</p><p>DREAM achieves improved distortion metrics and comparable perceptual quality with only 100 steps, marking a 20× speedup in sampling.</p><p>DREAM training approach significantly enhances model robustness, producing more realistic and clearer images across different scales.</p><p>DREAM captures finer details such as the beard of cats, the structural integrity of a tower, and the intricate wrinkles on a bed.</p><p>DREAM training framework consistently improves baseline model across diverse datasets and scales.</p><p>This paper introduces DREAM, a novel training framework designed to address the training-sampling discrepancy in conditional diffusion models with minimal code modifications.</p><p>DREAM comprises two key components: diffusion rectification and estimation adaptation.</p><p>Extensive experiments demonstrate that DREAM enhances distortion and perception metrics across various diffusion-based SR baselines.</p><p>It also speeds up training, improves sampling efficiency, and achieves robust OOD performance across diverse datasets and scales.</p>",
        "abstract_gpt": "<p>Single-image super-resolution (SISR) is a challenging task due to real-world degradation variability and the ill-posed nature of generating high-resolution images from low-resolution inputs. While diffusion probabilistic models (DPMs) have shown strong generative capabilities in this domain, they suffer from training-sampling discrepancies and inefficiencies in convergence and sampling. This paper introduces DREAM (Diffusion Rectification and Estimation-Adaptive Models), a simple yet effective training framework that addresses these challenges through two key components: diffusion rectification and estimation adaptation. Diffusion rectification aligns training more closely with sampling by incorporating the model’s own predictions, while estimation adaptation blends ground-truth information with self-estimation to balance perceptual quality and distortion. DREAM integrates easily into existing diffusion-based SISR models without altering architecture or sampling procedures. Extensive experiments across various baselines and datasets show that DREAM accelerates convergence by 2–3×, reduces sampling steps by up to 20×, and improves both distortion and perceptual metrics, including in out-of-distribution scenarios. These results demonstrate DREAM’s effectiveness in bridging the training-inference gap and enhancing robustness in high-quality image generation.</p>",
        "abstract_human": "<p>We present DREAM, a novel training framework representing Diffusion Rectification and Estimation-Adaptive Models, requiring minimal code changes (just three lines) yet significantly enhancing the alignment of training with sampling in diffusion models. DREAM features two components: diffusion rectification, which adjusts training to reflect the sampling process, and estimation adaptation, which balances perception against distortion. When applied to image super-resolution (SR), DREAM adeptly navigates the tradeoff between minimizing distortion and preserving high image quality. Experiments demonstrate DREAM's superiority over standard diffusion-based SR methods, showing a 2 to 3× faster training convergence and a 10 to 20× reduction in sampling steps to achieve comparable results. We hope DREAM will inspire a rethinking of diffusion model training paradigms. Our source code is available at link.</p>"
    },
    {
        "index": "30",
        "excerpt": "<p>Large Language Models (LLMs) have evolved to become impressively powerful in recent developments, with Generative Pretrained Transformer (GPT) models showing increasingly effective capabilities.</p><p>The generative nature of these models has led to their widespread adoption in numerous application fields.</p><p>Despite their advanced capabilities, LLMs are capable of generating incorrect results, an issue that is particularly problematic in applications where precision and dependability are critical, like the biomedical and healthcare fields.</p><p>While existing methods show varying degrees of improvement on different tasks, in general there lacks a systematic way to efficiently quantify the likelihood of errors in LLM outputs.</p><p>In many LLM applications, including disease prediction, medical diagnosis, and question answering (QA), a concrete and precise answer is desired.</p><p>For these mission critical tasks, a quantitative error estimation or confidence level for the response is equally important as giving a correct answer itself.</p><p>However, the text-in text-out nature of the generative language models makes it challenging to estimate the error probability of the answer quantitatively.</p><p>Although some LLMs provide internal probability scores for the generated tokens, they are poorly calibrated to the true error rate, particularly after applying reinforcement learning with human feedback.</p><p>Our goal in this paper is to address this issue by establishing a systematic way to quantitatively estimate the likelihood of error in LLM answer.</p><p>We approach this through training an estimator model h via multi-objective optimization, leveraging extensive research in Pareto optimization.</p><p>Given the optimized model h and any LLM response, we can then directly estimate the LLM response error rate which we refer to as the Pareto optimal learning assessed risk (POLAR) score.</p><p>We introduce a novel approach that trains a Pareto-optimal probabilistic model h to simultaneously optimize on LLM and align with the external information sources.</p><p>We propose a novel framework using Pareto optimization aligning to the LLM and multiple external information sources.</p><p>The POLAR score from our framework is shown experimentally to be effective in estimating LLM error rate.</p><p>We demonstrate that POLAR scores can be leveraged to boost an LLM’s performance by easily combining with other popular strategies such as self-verification and retrieval augmented generation.</p><p>In this work we present a framework to systematically estimate error of an LLM output by simultaneously aligning to multiple information sources while circumventing the weighting dilemma through Pareto optimization.</p><p>Our error estimation framework for LLM responses is a two-step process.</p><p>In the first step, we iterate through a corpus of input instances to collect the corresponding LLM responses, while dynamically retrieving heuristic answers from other information sources.</p><p>In this process, a probabilistic function h is learned that fits the multiple sources in a Pareto optimal manner.</p><p>In the second step, the optimized h model is used to estimate the error rate of the LLM response on any new input instance, which we refer to as the POLAR score.</p><p>After the error estimation step, we also provide an optional third step that strategically re-prompt the LLM based on the POLAR score, and leverage the information retrieved from the information sources in an RAG manner.</p><p>The primary challenge here is to design a framework to resolve conflicts among different sources and with the LLM.</p><p>In order for h to align with the multiple information sources, mathematically, we want to solve the following multi-objective problem.</p><p>As the objectives may conflict, we seek an h∗ that is Pareto optimal, following multi-objective learning theory and Pareto optimization.</p><p>The Pareto optimization framework effectively manages dependencies between information sources.</p><p>However, finding Pareto optimal solutions remains challenging in the multi-objective optimization literature.</p><p>We propose finding an optimal solution h∗ by solving a scalarized version of the multi-objective problem, which can be done via standard stochastic gradient descent algorithms such as Adam.</p><p>Once a optimal solution h∗ is found, we can estimate the error rate for any new input, and compute a risk score referred to as the Pareto optimal learning assessed risk (POLAR) score.</p><p>Identifying LLM responses with a higher risk of error presents an opportunity to efficiently correct the errors and improve the final accuracy.</p><p>We provide an optional Step 3, which easily connects the POLAR score to other prompting strategies to correct the error automatically.</p><p>We propose two dynamic prompting strategies to illustrate correcting LLM errors using the POLAR score.</p><p>This setting allows information sources to serve as additional input to the LLM to enhance its answer.</p><p>Dynamic self-verification: if the POLAR score is above a threshold, simply ask the LLM to self-verify its previous answer.</p><p>POLAR-assisted RAG: if the POLAR score is high, retrieve information from all triggered sources and feed it back to the LLM to generate a revised answer.</p><p>We leverage the publicly available datasets collected by Zhang et al. and evaluate on four different NLP tasks: CDR, ChemProt, SemEval, and SMS.</p><p>We do not use any of the ground truth in the training sets, and only use the ground truth labels on test set for evaluation of the LLM error rate and error correction performance.</p><p>To maximize LLM capabilities, we carefully design prompts for each problem, clarifying the problem setting, knowledge background, input/output structure, and instructions for stating “unsure”.</p><p>The information sources in these datasets were created by human experts that utilize knowledge bases, textual patterns, and a combination of the two.</p><p>We choose the quadratic aggregator for G and the BERT model for h.</p><p>For the biomedical CDR and ChemProt, we choose BiomedBERT for h.</p><p>The POLAR score reliably estimates the true probability of LLM error rates.</p><p>The POLAR scores are highly correlated with the true error rate.</p><p>Responses with the highest POLAR scores are most prone to errors, with the top scores indicating nearly a 100 percent error rate.</p><p>The proposed POLAR score consistently outperforms other methods.</p><p>Snorkel, WeaSEL, and LLM distilled model can achieve top or close-to-top performance in some cases under specific metric, but lack the consistency to deliver stable calibration for different LLMs on different tasks.</p><p>The POLAR-assisted dynamic prompting increases the GPT-4 performance.</p><p>GPT-4 with POLAR-assisted RAG outperforms state-of-the-art supervised task-specific model.</p><p>Removing all external sources from the model leads to inconsistent error estimation capabilities.</p><p>External information sources are essential to prevent h from overfitting to LLM responses, especially for the biomedical domain tasks.</p><p>The nonlinear quadratic loss aggregator, when combined with BERT finetuning, delivers superior error estimation performance.</p><p>The Chebyshev aggregator consistently underperforms across various scenarios.</p><p>We presented a novel framework for LLM error estimation using Pareto optimal learning.</p><p>The error estimator learned in our framework aligns with the LLM and other information sources Pareto optimally.</p><p>We showed experimentally that the proposed POLAR score is well calibrated with the LLM error rate evaluated on ground truth, ensuring reliable error estimation.</p><p>We proposed two POLAR-assisted dynamic prompting strategies, and showed that POLAR-assisted RAG enhances GPT-4’s performance, surpassing state-of-the-art task-specific model.</p><p>This development marks a substantial advancement in the application of LLM, providing an effective method to both estimate and reduce LLM errors.</p>",
        "abstract_gpt": "<p>Large Language Models (LLMs) have demonstrated impressive generative capabilities across various domains, yet they remain prone to generating erroneous outputs—particularly concerning in high-stakes fields like biomedicine. This paper addresses the critical need for systematic and reliable error estimation in LLM responses. The authors introduce POLAR (Pareto Optimal Learning Assessed Risk), a novel framework that leverages multi-objective Pareto optimization to align an error estimator model with both the LLM and external information sources. The approach involves training a probabilistic function in two stages: collecting LLM responses alongside heuristic source-derived answers, and optimizing the model to estimate the error probability for new inputs. Experimental evaluations across four biomedical and NLP datasets demonstrate that the POLAR score is highly correlated with true error rates and consistently outperforms existing methods in calibration stability. Furthermore, the framework supports dynamic prompting strategies—self-verification and retrieval-augmented generation—which significantly enhance GPT-4 performance, surpassing state-of-the-art supervised baselines. This work offers a scalable and effective solution for error detection and correction in LLMs, with strong implications for improving model reliability in critical applications.</p>",
        "abstract_human": "<p>Large Language Models (LLMs) have shown impressive abilities in many applications. When a concrete and precise answer is desired, it is important to have a quantitative estimation of the potential error rate. However, this can be challenging due to the text-in-text-out nature of generative models. We present a method based on Pareto optimization that generates a risk score to estimate the probability of error in an LLM response by integrating multiple sources of information. We prove theoretically that the error estimator optimized in our framework aligns with the LLM and the information sources in a Pareto optimal manner. Experimental results show that the risk scores estimated by our method are well correlated with the true LLM error rate, thus facilitating error correction. By dynamically combining with prompting strategies such as self-verification and information retrieval, we demonstrate the proposed method can be utilized to increase the performance of an LLM, surpassing state-of-the-art task specific models.</p>"
    }
]