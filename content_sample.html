<p><span style="font-weight: 400;">**Relevant Content for Abstract Preparation**</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">### Community Preference Optimization (COMPO)</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">Language models often aggregate diverse and conflicting human feedback, leading to outputs optimized for a hypothetical "average" user. COMPO addresses this by personalizing preference optimization for language models (LMs) using contextual information about the preference provider, focusing on group-level preferences. This is inspired by recommender systems, aiming to generate outputs tailored to specific communities. To this end, the COMPRED dataset was introduced, which comprises community-level preferences extracted from Reddit, enabling personalization without individual privacy risks.</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">### Dataset: COMPRED</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">COMPRED includes over 1 million preference pairs across 187 subreddits, grouped into five domains:</span></p>
<ol>
<li><span style="font-weight: 400;"> **Science**: Capturing divergences arising from topic specialization (e.g., /r/science vs. /r/StringTheory).</span></li>
<li><span style="font-weight: 400;"> **Finance**: Highlighting distinctions between subreddits focused on varying income brackets and investing goals.</span></li>
<li><span style="font-weight: 400;"> **History**: Addressing different norms for responses (e.g., /r/AskHistorians&rsquo; academic approach vs. /r/History).</span></li>
<li><span style="font-weight: 400;"> **Politics**: Divided by leanings, issues, or regional focus (e.g., /r/Conservative vs. /r/askliberal).</span></li>
<li><span style="font-weight: 400;"> **Gender/Sexuality**: Exploring intersections with other demographics and topics such as parenting and fashion.</span></li>
</ol>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">The dataset uses Reddit&rsquo;s post-comment structure, relying on upvotes as a proxy for community preferences. Preference pairs are created by comparing responses to identical prompts posted at different times, factoring in upvotes and post timings to avoid biases.</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">### Motivational Experiment</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">To demonstrate the value of incorporating subreddit context, reward models were trained with and without community identifiers. Models trained with subreddit context achieved higher preference prediction accuracy, particularly in datasets like politics and gender, where community-specific preferences are pronounced. In contrast, conditioning on the wrong subreddit significantly reduced performance.</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">### Main Experiments: COMPO</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">Using Llama-2 7B as the base model, COMPO was evaluated against baselines to assess the impact of contextualizing LMs with community identifiers:</span></p>
<ol>
<li><span style="font-weight: 400;"> **SFT-NC**: Finetuning with no subreddit context.</span></li>
<li><span style="font-weight: 400;"> **DPO**: Direct preference optimization without subreddit context.</span></li>
<li><span style="font-weight: 400;"> **SFT-C**: Finetuning with subreddit context but no preference tuning.</span></li>
</ol>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">COMPO consistently outperformed these baselines in both automated and human evaluations, with notable improvements in politically and culturally sensitive domains. Performance degraded when random subreddit identifiers were introduced, underscoring the importance of accurate contextualization.</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">### Human Evaluation</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">Human annotators evaluated the relevance of responses from COMPO and baseline models across eight subreddits. COMPO was preferred in 46.5% of cases, significantly outperforming baselines like DPO (37.1%). Annotators noted COMPO&rsquo;s ability to generate responses aligned with subreddit-specific norms and values, especially in politically diverse and gender-related contexts.</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">### Analysis</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">Further analysis revealed that subreddit context was most beneficial when subreddit predictability (via input text) was low, indicating the added value of explicit contextualization. High predictability reduced the advantage of community-specific tuning, suggesting opportunities for efficient resource allocation by routing predictable examples to generalist models.</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">### Conclusions</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;">COMPO demonstrates the potential of personalized preference optimization for LMs by incorporating community-level context. The approach improves the relevance of model outputs to specific communities and introduces the COMPRED dataset for future research in personalized LMs. This work also highlights directions for addressing challenges like intra-community diversity, cold-start problems, and the risks of reinforcing echo chambers.</span></p>
<p><br /><br /></p>