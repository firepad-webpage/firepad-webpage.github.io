<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
<p>Pathological examination through whole slide images (WSIs) is the gold standard for cancer diagnosis, but their immense size and the scarcity of pixel-level annotations pose challenges for training robust models. Multiple instance learning (MIL) methods have achieved success in WSI classification by aggregating patch-level features, yet they rely heavily on large labeled datasets and lack incorporation of pathological prior knowledge, limiting generalization. Recent advances in vision-language models (VLMs) offer new opportunities, but transferring them effectively to pathology remains difficult. To address these issues, we propose ViLa-MIL, a dual-scale vision-language MIL framework for WSI classification. ViLa-MIL employs a frozen large language model to generate dual-scale visual descriptive text prompts, capturing both global tumor structures and local fine-grained details, consistent with pathologistsâ€™ diagnostic practice. A prototype-guided patch decoder and a context-guided text decoder further enhance feature alignment and aggregation. Extensive evaluation on three cancer subtyping datasets demonstrates that ViLa-MIL consistently outperforms state-of-the-art baselines, achieving superior robustness, generalization, and cross-domain adaptability in few-shot settings.</p>
</body>
</html>
