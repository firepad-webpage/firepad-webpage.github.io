<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
<p>Offline-to-online (O2O) reinforcement learning aims to fine-tune a policy learned from fixed offline data through limited online interactions. While this approach offers promise for high-stakes domains like healthcare and autonomous driving, existing methods often suffer from inefficiencies due to evaluation and improvement mismatches between the offline and online settings. This paper introduces a general O2O framework that addresses these mismatches through three key components: optimistic policy re-evaluation, value alignment between the critic and policy, and a constrained fine-tuning (CFT) framework to mitigate distribution shift. The optimistic re-evaluation produces reliable Q-value estimates, while value alignment ensures consistency between action probabilities and Q-values. The CFT approach incorporates a regularization term into the policy objective to limit updates to credible regions. Extensive experiments on benchmark environments demonstrate that the proposed method achieves more stable and efficient performance than prior approaches and offers strong transferability across various offline algorithms. The results confirm the effectiveness of addressing mismatches to enable general and robust O2O RL.</p>
</body>
</html>
