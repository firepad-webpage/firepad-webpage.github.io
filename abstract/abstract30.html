<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
<p>Large Language Models (LLMs) have demonstrated impressive generative capabilities across various domains, yet they remain prone to generating erroneous outputs—particularly concerning in high-stakes fields like biomedicine. This paper addresses the critical need for systematic and reliable error estimation in LLM responses. The authors introduce POLAR (Pareto Optimal Learning Assessed Risk), a novel framework that leverages multi-objective Pareto optimization to align an error estimator model with both the LLM and external information sources. The approach involves training a probabilistic function in two stages: collecting LLM responses alongside heuristic source-derived answers, and optimizing the model to estimate the error probability for new inputs. Experimental evaluations across four biomedical and NLP datasets demonstrate that the POLAR score is highly correlated with true error rates and consistently outperforms existing methods in calibration stability. Furthermore, the framework supports dynamic prompting strategies—self-verification and retrieval-augmented generation—which significantly enhance GPT-4 performance, surpassing state-of-the-art supervised baselines. This work offers a scalable and effective solution for error detection and correction in LLMs, with strong implications for improving model reliability in critical applications.</p>
</body>
</html>
