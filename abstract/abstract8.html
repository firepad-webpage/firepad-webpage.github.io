<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
    <p>Tool learning has emerged as a promising approach to enhance the applicability of large language models (LLMs) in real-world tasks. However, the introduction of external tools into LLM workflows raises new safety concerns that remain underexplored. In this work, we propose ToolSword, a comprehensive framework designed to systematically uncover and evaluate safety issues associated with tool learning in LLMs. ToolSword examines safety challenges across three key stages of tool learning—input, execution, and output—through six scenarios, including malicious queries, jailbreak attacks, noise misdirection, risky cues, harmful feedback, and error conflicts. Utilizing ToolSword, we conduct an in-depth analysis of 11 open-source and closed-source LLMs with strong tool learning capabilities. Our findings reveal that LLMs frequently encounter safety failures at all stages, such as mishandling harmful queries, selecting unsafe tools, and generating unsafe outputs. Notably, even advanced models like GPT-4 are susceptible to these vulnerabilities. Furthermore, while LLMs perform comparably to humans in safety-neutral environments, they struggle with safety-critical tasks. Our study highlights the urgent need to enhance safety alignment mechanisms for LLMs in tool learning contexts.</p>
</body>
</html>
