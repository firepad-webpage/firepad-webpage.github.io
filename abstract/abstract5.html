<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
    <p>LLM-based code agents have advanced AI-assisted software development by combining large language models with external tools for dynamic code execution and system interaction. However, these agents pose significant safety risks, as they may generate or execute harmful code with severe security implications. Existing research primarily focuses on static code generation, leaving a gap in the comprehensive evaluation of code agents' dynamic behavior. To address this, we introduce RedCode, a benchmark designed to assess the safety of code agents across both code generation and execution. RedCode comprises 4,050 test cases spanning 25 safety scenarios across 8 domains for code execution (RedCode-Exec) and 160 prompts targeting malware generation (RedCode-Gen). We evaluate 19 LLM-based code agents, revealing a high attack success rate and low rejection rate, especially for natural language inputs. OpenCodeInterpreter demonstrates stronger safety constraints compared to CodeAct and ReAct. However, agents leveraging stronger base models like GPT-4 exhibit improved rejection rates but also generate more potent malware. These findings highlight significant safety concerns and the need for improved safeguards in LLM-based code agents, as they remain vulnerable to executing harmful actions in real-world environments.</p>
</body>
</html>
