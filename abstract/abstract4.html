<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
    <p>Deep neural networks are known to be highly vulnerable to adversarial attacks, where small, deliberate perturbations in input data can cause significant errors in classification. A key contributor to this vulnerability is the non-Lipschitz behavior of neural networks, which enables small input changes to yield large feature space shifts. In this work, we introduce a novel random feature subspace threat model to abstract this phenomenon and investigate strategies to enhance robustness. We demonstrate that classifiers lacking the ability to abstain are inherently susceptible to adversarial manipulations, regardless of the distribution of natural data or the classifier's decision boundary. However, by allowing classifiers to abstain on uncertain inputs, we show it is possible to defend against such adversarial threats under reasonable assumptions on data distribution. Specifically, we prove that a nearest-neighbor classifier with a tunable abstention threshold can achieve a favorable trade-off between robust precision and recall. Our theoretical findings are complemented by experimental results demonstrating the efficacy of our approach on representations learned through supervised and self-supervised contrastive learning. Ultimately, this work underscores the critical role of abstention in adversarial robustness and provides formal guarantees and practical algorithms for data-driven abstention in nearest-neighbor classifiers.</p></body>
</html>
