<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
<p>Large-scale pretrained language and vision-language models have demonstrated strong adaptability to downstream tasks through prompt tuning, yet existing methods face key limitations. Soft prompt tuning learns continuous embeddings but produces opaque, non-transferable prompts, while hard prompt tuning offers interpretability and transferability but requires expensive discrete optimization. To address this challenge, we introduce PIN (Prompts made INterpretable), a reinforcement learningâ€“based framework that incorporates sparse Tsallis entropy regularization to ignore unlikely tokens and focus on a reduced, informative action space. By filtering ignorable tokens and encouraging sparse policies, PIN mitigates exploration bias toward low-probability tokens and generates interpretable, human-readable prompts. We evaluate PIN across diverse tasks, including few-shot text classification, unsupervised text style transfer, and textual inversion with vision-language models. Experiments on datasets such as Yahoo, MS COCO, LAION, and Lexica.art show that PIN achieves competitive or superior performance compared to RLPrompt, requiring fewer interactions and producing more interpretable prompts. While computationally more demanding than gradient-based methods, PIN offers a principled and effective approach for hard prompt discovery.</p>
</body>
</html>
